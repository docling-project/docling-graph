defaults:
  processing_mode: one-to-one
  backend: llm
  inference: local
  export_format: csv
docling:
  pipeline: ocr
  export:
    docling_json: true
    markdown: true
    per_page_markdown: false
models:
  vlm:
    local:
      default_model: numind/NuExtract-2.0-2B
      provider: docling
  llm:
    local:
      default_model: llama-3.1-8b
      provider: vllm
    remote:
      default_model: mistral-small-latest
      provider: mistral
    providers:
      ollama:
        default_model: llama3:8b-instruct
      vllm:
        default_model: llama-3.1-8b
      mistral:
        default_model: mistral-small-latest
      openai:
        default_model: gpt-4-turbo
      gemini:
        default_model: gemini-2.5-flash
output:
  default_directory: outputs
  create_visualizations: true
  create_markdown: true
