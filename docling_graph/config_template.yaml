# Docling-Graph Configuration File
# This file defines default settings and model configurations

# Default settings (can be overridden via CLI flags)
defaults:
  processing_mode: many-to-one  # one-to-one | many-to-one
  extraction_contract: direct   # direct | staged
  backend: llm                  # llm | vlm
  inference: local              # local | remote
  export_format: csv            # csv | cypher
  # Max tokens per chunk when chunking is used (null = use default 512)
  chunk_max_tokens: null
  # Staged (3-pass) tuning: ID pass → fill (bottom-up) → merge. Preset + optional overrides.
  staged_tuning_preset: standard  # standard | advanced (advanced = larger ID-pass shards, larger fill batches)
  staged_pass_retries: null       # retries per pass when LLM returns invalid JSON; null = use preset
  staged_workers: null           # parallel workers for fill pass; null = use preset
  staged_nodes_fill_cap: null    # max nodes per LLM call in fill pass; null = use preset
  staged_id_shard_size: null     # paths per ID-pass call; 0 = single call; null = use preset
  staged_id_identity_only: true  # discover only identity-bearing paths during ID pass
  staged_id_compact_prompt: true # compact ID prompt to reduce token usage
  staged_id_auto_shard_threshold: 10
  staged_id_shard_min_size: 2
  staged_quality_require_root: true
  staged_quality_min_instances: 1
  staged_quality_max_parent_lookup_miss: 0
  staged_id_max_tokens: null
  staged_fill_max_tokens: null

# Docling pipeline configuration
docling:
  # Choose pipeline type for document conversion
  # - "ocr": Classic OCR pipeline (most accurate for standard documents)
  # - "vision": Vision-Language Model pipeline based on Granite-Docling (best for complex layouts)
  pipeline: ocr  # ocr | vision

  # Export options for Docling document processing
  export:
    # Export the full Docling document structure as JSON
    # Contains document metadata, layout information, tables, etc.
    docling_json: true

    # Export the document as markdown (full document)
    # Best for human-readable output and documentation
    markdown: true

    # Export per-page markdown files
    # Useful for page-by-page analysis or processing
    # Creates a subdirectory with one .md file per page
    per_page_markdown: false

# Model selections by backend and inference location
models:
  vlm:
    local:
      model: "numind/NuExtract-2.0-2B"
      provider: "docling"

  llm:
    local:
      model: "ibm-granite/granite-4.0-1b"
      provider: "vllm"

    remote:
      model: "mistral-small-latest"
      provider: "mistral"

# Optional: runtime defaults for LLMs (can be overridden by CLI flags)
llm_overrides:
  generation:
    temperature: 0.1
    max_tokens: null
    top_p: null
  reliability:
    timeout_s: null
    max_retries: null
  connection:
    # Custom endpoint (on-prem): use fixed env vars CUSTOM_LLM_BASE_URL and CUSTOM_LLM_API_KEY
    base_url: null
    api_key: null

# Output settings
output:
  # Directory for all outputs (graphs, JSON, reports, visualizations)
  # Note: the CLI flag --output-dir overrides this at runtime
  directory: "outputs"

  # Optional toggles for downstream tools (not required by pipeline)
  # Set to false if you want to skip generating visualization or markdown reports
  create_visualizations: true
  create_markdown: true
