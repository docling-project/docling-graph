# Docling-Graph Configuration File
# This file defines default settings and model configurations

# Default settings (can be overridden via CLI flags)
defaults:
  processing_mode: many-to-one  # one-to-one | many-to-one
  extraction_contract: direct   # direct | staged | delta
  backend: llm                  # llm | vlm
  inference: local              # local | remote
  export_format: csv            # csv | cypher
  structured_output: true       # true | false (schema-enforced output via LiteLLM response_format)
  structured_sparse_check: true # true | false (auto-fallback when structured result looks sparse)
  # Max tokens per chunk when chunking is used (null = use default 512)
  chunk_max_tokens: null
  # Max total input tokens sent per LLM call in delta extraction
  llm_batch_token_size: 1024
  # Staged (3-pass) tuning: ID pass → fill (bottom-up) → merge. Preset + optional overrides.
  staged_tuning_preset: standard  # standard | advanced (advanced = larger ID-pass shards, larger fill batches)
  staged_pass_retries: null       # retries per pass when LLM returns invalid JSON; null = use preset
  parallel_workers: null         # parallel workers for staged/delta extraction; null = use preset for staged
  delta_normalizer_validate_paths: true
  delta_normalizer_canonicalize_ids: true
  delta_normalizer_strip_nested_properties: true
  delta_normalizer_attach_provenance: true
  delta_resolvers_enabled: true
  delta_resolvers_mode: semantic # off | fuzzy | semantic | chain
  delta_resolver_fuzzy_threshold: 0.8
  delta_resolver_semantic_threshold: 0.8
  delta_resolver_properties: null
  delta_resolver_paths: null
  quality_max_unknown_path_drops: -1  # -1 disables this gate
  quality_max_id_mismatch: -1         # -1 disables this gate
  quality_max_nested_property_drops: -1  # -1 disables this gate
  delta_quality_require_root: true
  delta_quality_min_instances: 20  # below this, delta fails gate and direct extraction is used
  delta_quality_max_parent_lookup_miss: 4
  delta_quality_adaptive_parent_lookup: true
  delta_quality_require_relationships: false
  delta_quality_require_structural_attachments: false
  delta_quality_min_non_empty_properties: -1
  delta_quality_min_root_non_empty_fields: -1
  delta_quality_min_non_empty_by_path: null
  delta_quality_max_orphan_ratio: -1.0
  delta_quality_max_canonical_duplicates: -1
  delta_batch_split_max_retries: 1
  staged_nodes_fill_cap: null    # max nodes per LLM call in fill pass; null = use preset
  staged_id_shard_size: null     # paths per ID-pass call; 0 = single call; null = use preset
  staged_id_identity_only: true  # discover only identity-bearing paths during ID pass
  staged_id_compact_prompt: true # compact ID prompt to reduce token usage
  staged_id_auto_shard_threshold: 10
  staged_id_shard_min_size: 2
  staged_quality_require_root: true
  staged_quality_min_instances: 1
  staged_quality_max_parent_lookup_miss: 0
  staged_id_max_tokens: null
  staged_fill_max_tokens: null

# Docling pipeline configuration
docling:
  # Choose pipeline type for document conversion
  # - "ocr": Classic OCR pipeline (most accurate for standard documents)
  # - "vision": Vision-Language Model pipeline based on Granite-Docling (best for complex layouts)
  pipeline: ocr  # ocr | vision

  # Export options for Docling document processing
  export:
    # Export the full Docling document structure as JSON
    # Contains document metadata, layout information, tables, etc.
    docling_json: true

    # Export the document as markdown (full document)
    # Best for human-readable output and documentation
    markdown: true

    # Export per-page markdown files
    # Useful for page-by-page analysis or processing
    # Creates a subdirectory with one .md file per page
    per_page_markdown: false

# Model selections by backend and inference location
models:
  vlm:
    local:
      model: "numind/NuExtract-2.0-2B"
      provider: "docling"

  llm:
    local:
      model: "ibm-granite/granite-4.0-1b"
      provider: "vllm"

    remote:
      model: "mistral-small-latest"
      provider: "mistral"

# Optional: runtime defaults for LLMs (can be overridden by CLI flags)
llm_overrides:
  generation:
    temperature: 0.1
    max_tokens: null
    top_p: null
  reliability:
    timeout_s: null
    max_retries: null
  connection:
    # Custom endpoint (on-prem): use fixed env vars CUSTOM_LLM_BASE_URL and CUSTOM_LLM_API_KEY
    base_url: null
    api_key: null

# Output settings
output:
  # Directory for all outputs (graphs, JSON, reports, visualizations)
  # Note: the CLI flag --output-dir overrides this at runtime
  directory: "outputs"

  # Optional toggles for downstream tools (not required by pipeline)
  # Set to false if you want to skip generating visualization or markdown reports
  create_visualizations: true
  create_markdown: true
