"""
Example 02: Quickstart - LLM Extraction from PDF

Description:
    Basic LLM extraction from a multi-page rheology research PDF using a remote API.
    Demonstrates the standard workflow for text-heavy documents with automatic chunking.

Use Cases:
    - Rheology researchs and academic documents
    - Technical reports and whitepapers
    - Multi-page business documents
    - Any text-heavy PDF content

Prerequisites:
    - Installation: uv sync --extra remote
    - Environment: export MISTRAL_API_KEY="your-api-key"
    - Data: Sample rheology research included in repository

Key Concepts:
    - LLM Backend: Processes text extracted from PDFs
    - Many-to-One Mode: All pages merged into single output
    - Chunking: Automatically splits large documents for LLM context limits
    - Remote Inference: Uses Mistral API for extraction
    - Programmatic Merge: Combines chunk results without additional LLM call

Expected Output:
    - nodes.csv: Extracted research data (authors, experiments, results)
    - edges.csv: Relationships between research entities
    - graph.html: Interactive knowledge graph visualization
    - document.md: Markdown version of the PDF
    - report.md: Extraction statistics and summary

Related Examples:
    - Example 01: VLM extraction from images
    - Example 07: Local LLM inference
    - Example 08: Advanced chunking strategies
    - Documentation: https://ibm.github.io/docling-graph/usage/examples/research-paper/
"""

import sys
from pathlib import Path

from rich import print as rich_print
from rich.console import Console
from rich.panel import Panel

# Setup project path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

try:
    from examples.templates.rheology_research import Research

    from docling_graph import PipelineConfig
except ImportError:
    rich_print("[red]Error:[/red] Could not import required modules.")
    rich_print("Please run this script from the project root directory.")
    sys.exit(1)

# Configuration
SOURCE_FILE = "docs/examples/data/research_paper/rheology.pdf"
TEMPLATE_CLASS = Research
OUTPUT_DIR = "outputs/02_quickstart_llm_pdf"

console = Console()


def main() -> None:
    """Execute LLM extraction from rheology research PDF."""
    console.print(
        Panel.fit(
            "[bold blue]Example 02: Quickstart - LLM from PDF[/bold blue]\n"
            "[dim]Extract structured data from a rheology research using Large Language Model[/dim]",
            border_style="blue",
        )
    )

    console.print("\n[yellow]üìã Configuration:[/yellow]")
    console.print(f"  ‚Ä¢ Source: [cyan]{SOURCE_FILE}[/cyan]")
    console.print(f"  ‚Ä¢ Template: [cyan]{TEMPLATE_CLASS.__name__}[/cyan]")
    console.print("  ‚Ä¢ Backend: [cyan]LLM (Large Language Model)[/cyan]")
    console.print("  ‚Ä¢ Provider: [cyan]Mistral AI[/cyan]")
    console.print("  ‚Ä¢ Mode: [cyan]many-to-one[/cyan]")

    console.print("\n[yellow]‚ö†Ô∏è  Prerequisites:[/yellow]")
    console.print("  ‚Ä¢ Mistral API key must be set: [cyan]export MISTRAL_API_KEY='...'[/cyan]")
    console.print("  ‚Ä¢ Install remote extras: [cyan]uv sync --extra remote[/cyan]")

    try:
        # Configure the pipeline
        config = PipelineConfig(
            source=SOURCE_FILE,
            template=TEMPLATE_CLASS,
            output_dir=OUTPUT_DIR,
            # LLM backend for text-based extraction
            backend="llm",
            # Remote inference using API
            inference="remote",
            # Use Mistral AI provider
            provider_override="mistral",
            # Use a capable model for complex extraction
            model_override="mistral-large-latest",
            # Many-to-one: merge all pages into single result
            processing_mode="many-to-one",
            # Enable chunking for large documents
            use_chunking=True,
            # Use programmatic merge (faster, no extra API call)
            llm_consolidation=False,
        )

        # Execute the pipeline
        console.print("\n[yellow]‚öôÔ∏è  Processing (this may take 1-2 minutes)...[/yellow]")
        console.print("  ‚Ä¢ Converting PDF to markdown")
        console.print("  ‚Ä¢ Chunking document for LLM context")
        console.print("  ‚Ä¢ Extracting data from each chunk")
        console.print("  ‚Ä¢ Merging results programmatically")
        console.print("  ‚Ä¢ Building knowledge graph")

        config.run()

        # Success message
        console.print("\n[green]‚úì Success![/green]")
        console.print(f"\n[bold]Output Location:[/bold] [cyan]{OUTPUT_DIR}[/cyan]")

        console.print("\n[bold]üìä Next Steps:[/bold]")
        console.print(
            f"  1. View interactive graph: [cyan]uv run docling-graph inspect {OUTPUT_DIR}[/cyan]"
        )
        console.print(
            f"  2. Check extracted data: [cyan]cat {OUTPUT_DIR}/docling_graph/nodes.csv[/cyan]"
        )
        console.print(f"  3. View markdown: [cyan]cat {OUTPUT_DIR}/docling/document.md[/cyan]")
        console.print(f"  4. Read summary: [cyan]cat {OUTPUT_DIR}/docling_graph/report.md[/cyan]")

        console.print("\n[bold]üí° What Happened:[/bold]")
        console.print("  ‚Ä¢ PDF converted to markdown using Docling")
        console.print("  ‚Ä¢ Document split into chunks respecting context limits")
        console.print("  ‚Ä¢ Each chunk processed by Mistral LLM")
        console.print("  ‚Ä¢ Results merged programmatically (no LLM consolidation)")
        console.print("  ‚Ä¢ Knowledge graph built from extracted entities")

        console.print("\n[bold]üéØ Key Differences from Example 01:[/bold]")
        console.print("  ‚Ä¢ LLM vs VLM: Text-based vs vision-based extraction")
        console.print("  ‚Ä¢ Remote vs Local: API call vs local model")
        console.print("  ‚Ä¢ Many-to-one vs One-to-one: Merged vs separate outputs")
        console.print("  ‚Ä¢ Chunking: Enabled for large documents")

    except FileNotFoundError:
        console.print(f"\n[red]‚úó Error:[/red] Source file not found: {SOURCE_FILE}")
        console.print("\n[yellow]Troubleshooting:[/yellow]")
        console.print("  ‚Ä¢ Ensure you're running from the project root directory")
        console.print("  ‚Ä¢ Check that the sample data exists in docs/examples/data/")
        sys.exit(1)

    except Exception as e:
        error_msg = str(e).lower()
        console.print(f"\n[red]‚úó Error:[/red] {e}")
        console.print("\n[yellow]Troubleshooting:[/yellow]")

        if "api" in error_msg or "key" in error_msg or "auth" in error_msg:
            console.print(
                "  ‚Ä¢ Set your Mistral API key: [cyan]export MISTRAL_API_KEY='your-key'[/cyan]"
            )
            console.print("  ‚Ä¢ Get a key at: https://console.mistral.ai/")
            console.print("  ‚Ä¢ Or use local inference: see Example 07")
        else:
            console.print("  ‚Ä¢ Ensure dependencies installed: [cyan]uv sync --extra remote[/cyan]")
            console.print("  ‚Ä¢ Check your internet connection")
            console.print("  ‚Ä¢ Verify the template class is correctly defined")
            console.print("  ‚Ä¢ Try with a smaller document first")

        sys.exit(1)


if __name__ == "__main__":
    main()
