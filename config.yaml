defaults:
  processing_mode: many-to-one
  backend_type: vlm
  inference: local
  export_format: csv
docling:
  pipeline: vision
models:
  vlm:
    local:
      default_model: numind/NuExtract-2.0-8B
      provider: docling
  llm:
    local:
      default_model: llama3:8b-instruct
      provider: ollama
    remote:
      default_model: mistral-small-latest
      provider: mistral
    providers:
      mistral:
        default_model: mistral-small-latest
      openai:
        default_model: gpt-4-turbo
      gemini:
        default_model: gemini-2.5-flash
output:
  default_directory: outputs
  create_visualizations: true
  create_markdown: true
