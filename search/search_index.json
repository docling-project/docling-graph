{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Docling Graph","text":"<p>Docling Graph converts documents into validated Pydantic objects and then into a directed knowledge graph, with exports to CSV or Cypher and both static and interactive visualizations.</p>"},{"location":"#overview","title":"Overview","text":"<p>This transformation of unstructured documents into validated knowledge graphs with precise semantic relationships is essential for complex domains like chemistry, finance, and physics where AI systems must understand exact entity connections (e.g., chemical compounds and their reactions, financial instruments and their dependencies, physical properties and their measurements) rather than approximate text vectors, enabling explainable reasoning over technical document collections.</p> <p>The toolkit supports two extraction families: local VLM via Docling and LLM-based extraction via local (vLLM, Ollama) or API providers (Mistral, OpenAI, Gemini, IBM WatsonX), all orchestrated by a flexible, config-driven pipeline.</p>"},{"location":"#key-capabilities","title":"Key Capabilities","text":""},{"location":"#extraction","title":"\ud83e\udde0 Extraction","text":"<ul> <li>Local VLM - Docling's information extraction pipeline (ideal for small documents with key-value focus)</li> <li>LLM - Local via vLLM/Ollama or remote via Mistral/OpenAI/Gemini/IBM WatsonX API</li> <li>Hybrid Chunking - Leveraging Docling's segmentation with semantic LLM chunking</li> <li>Flexible Strategies - Page-wise or whole-document conversion</li> </ul>"},{"location":"#graph-construction","title":"\ud83d\udd28 Graph Construction","text":"<ul> <li>Markdown to Graph - Convert validated Pydantic instances to NetworkX DiGraph</li> <li>Smart Merge - Combine multi-page documents into unified processing</li> <li>Type Safety - Enhanced with Pydantic validation and configuration</li> </ul>"},{"location":"#export","title":"\ud83d\udce6 Export","text":"<ul> <li>Docling Document - JSON format with full document structure</li> <li>Markdown - Full document and per-page options</li> <li>CSV - Compatible with Neo4j admin import</li> <li>Cypher - Script generation for bulk ingestion</li> <li>JSON - General-purpose graph data</li> </ul>"},{"location":"#visualization","title":"\ud83d\udcca Visualization","text":"<ul> <li>Interactive HTML - Full-page browser view with node/edge exploration</li> <li>Markdown Reports - Detailed graph nodes content and edges</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install\npip install docling-graph\n\n# Or with all features\npip install docling-graph[all]\n</code></pre> <pre><code>from docling_graph import run_pipeline, PipelineConfig\nfrom your_templates import YourTemplate\n\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    output_dir=\"outputs\"\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed installation instructions</li> <li>Quick Start - Get up and running quickly</li> <li>Pydantic Templates - Create extraction templates</li> <li>Examples - Explore example use cases</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub: IBM/docling-graph</li> <li>Issues: Report bugs or request features</li> <li>Contributing: See our Contributing Guide</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p> <p>IBM \u2764\ufe0f Open Source AI</p> <p>Docling Graph has been brought to you by IBM Research.</p>"},{"location":"api/config/","title":"Configuration API","text":"<p>Configuration classes for the Docling Graph pipeline.</p>"},{"location":"api/config/#pipelineconfig","title":"PipelineConfig","text":""},{"location":"api/config/#docling_graph.config.PipelineConfig","title":"<code>docling_graph.config.PipelineConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type-safe configuration for the docling-graph pipeline. This is the SINGLE SOURCE OF TRUTH for all defaults. All other modules should reference these defaults via PipelineConfig, not duplicate them.</p> Source code in <code>docling_graph/config.py</code> <pre><code>class PipelineConfig(BaseModel):\n    \"\"\"\n    Type-safe configuration for the docling-graph pipeline.\n    This is the SINGLE SOURCE OF TRUTH for all defaults.\n    All other modules should reference these defaults via PipelineConfig, not duplicate them.\n    \"\"\"\n\n    # Optional fields (empty by default, filled in at runtime)\n    source: Union[str, Path] = Field(default=\"\", description=\"Path to the source document\")\n    template: Union[str, type[BaseModel]] = Field(\n        default=\"\", description=\"Pydantic template class or dotted path string\"\n    )\n\n    # Core processing settings (with defaults)\n    backend: Literal[\"llm\", \"vlm\"] = Field(default=\"llm\")\n    inference: Literal[\"local\", \"remote\"] = Field(default=\"local\")\n    processing_mode: Literal[\"one-to-one\", \"many-to-one\"] = Field(default=\"many-to-one\")\n\n    # Docling settings (with defaults)\n    docling_config: Literal[\"ocr\", \"vision\"] = Field(default=\"ocr\")\n\n    # Model overrides\n    model_override: str | None = None\n    provider_override: str | None = None\n\n    # Models configuration (flat only, with defaults)\n    models: ModelsConfig = Field(default_factory=ModelsConfig)\n\n    # Extract settings (with defaults)\n    use_chunking: bool = True\n    llm_consolidation: bool = False\n    max_batch_size: int = 1\n\n    # Export settings (with defaults)\n    export_format: Literal[\"csv\", \"cypher\"] = Field(default=\"csv\")\n    export_docling: bool = Field(default=True)\n    export_docling_json: bool = Field(default=True)\n    export_markdown: bool = Field(default=True)\n    export_per_page_markdown: bool = Field(default=False)\n\n    # Graph settings (with defaults)\n    reverse_edges: bool = Field(default=False)\n\n    # Output settings (with defaults)\n    output_dir: Union[str, Path] = Field(default=\"outputs\")\n\n    @field_validator(\"source\", \"output_dir\")\n    @classmethod\n    def _path_to_str(cls, v: Union[str, Path]) -&gt; str:\n        return str(v)\n\n    @model_validator(mode=\"after\")\n    def _validate_vlm_constraints(self) -&gt; Self:\n        if self.backend == \"vlm\" and self.inference == \"remote\":\n            raise ValueError(\n                \"VLM backend currently only supports local inference. Use inference='local' or backend='llm'.\"\n            )\n        return self\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert config to dictionary format expected by run_pipeline.\"\"\"\n        return {\n            \"source\": self.source,\n            \"template\": self.template,\n            \"backend\": self.backend,\n            \"inference\": self.inference,\n            \"processing_mode\": self.processing_mode,\n            \"docling_config\": self.docling_config,\n            \"use_chunking\": self.use_chunking,\n            \"llm_consolidation\": self.llm_consolidation,\n            \"model_override\": self.model_override,\n            \"provider_override\": self.provider_override,\n            \"export_format\": self.export_format,\n            \"export_docling\": self.export_docling,\n            \"export_docling_json\": self.export_docling_json,\n            \"export_markdown\": self.export_markdown,\n            \"export_per_page_markdown\": self.export_per_page_markdown,\n            \"reverse_edges\": self.reverse_edges,\n            \"output_dir\": self.output_dir,\n            \"models\": self.models.model_dump(),\n        }\n\n    def run(self) -&gt; None:\n        \"\"\"Convenience method to run the pipeline with this configuration.\"\"\"\n        from docling_graph.pipeline import run_pipeline\n\n        run_pipeline(self.to_dict())\n\n    @classmethod\n    def generate_yaml_dict(cls) -&gt; Dict[str, Any]:\n        \"\"\"\n        Generate a YAML-compatible config dict with all defaults.\n        This is used by init.py to create config_template.yaml and config.yaml\n        without hardcoding defaults in multiple places.\n        \"\"\"\n        default_config = cls()\n        return {\n            \"defaults\": {\n                \"backend\": default_config.backend,\n                \"inference\": default_config.inference,\n                \"processing_mode\": default_config.processing_mode,\n                \"export_format\": default_config.export_format,\n            },\n            \"docling\": {\n                \"pipeline\": default_config.docling_config,\n                \"export\": {\n                    \"docling_json\": default_config.export_docling_json,\n                    \"markdown\": default_config.export_markdown,\n                    \"per_page_markdown\": default_config.export_per_page_markdown,\n                },\n            },\n            \"models\": default_config.models.model_dump(),\n            \"output\": {\n                \"directory\": str(default_config.output_dir),\n            },\n        }\n</code></pre>"},{"location":"api/config/#docling_graph.config.PipelineConfig.generate_yaml_dict","title":"<code>generate_yaml_dict()</code>  <code>classmethod</code>","text":"<p>Generate a YAML-compatible config dict with all defaults. This is used by init.py to create config_template.yaml and config.yaml without hardcoding defaults in multiple places.</p> Source code in <code>docling_graph/config.py</code> <pre><code>@classmethod\ndef generate_yaml_dict(cls) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a YAML-compatible config dict with all defaults.\n    This is used by init.py to create config_template.yaml and config.yaml\n    without hardcoding defaults in multiple places.\n    \"\"\"\n    default_config = cls()\n    return {\n        \"defaults\": {\n            \"backend\": default_config.backend,\n            \"inference\": default_config.inference,\n            \"processing_mode\": default_config.processing_mode,\n            \"export_format\": default_config.export_format,\n        },\n        \"docling\": {\n            \"pipeline\": default_config.docling_config,\n            \"export\": {\n                \"docling_json\": default_config.export_docling_json,\n                \"markdown\": default_config.export_markdown,\n                \"per_page_markdown\": default_config.export_per_page_markdown,\n            },\n        },\n        \"models\": default_config.models.model_dump(),\n        \"output\": {\n            \"directory\": str(default_config.output_dir),\n        },\n    }\n</code></pre>"},{"location":"api/config/#docling_graph.config.PipelineConfig.run","title":"<code>run()</code>","text":"<p>Convenience method to run the pipeline with this configuration.</p> Source code in <code>docling_graph/config.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Convenience method to run the pipeline with this configuration.\"\"\"\n    from docling_graph.pipeline import run_pipeline\n\n    run_pipeline(self.to_dict())\n</code></pre>"},{"location":"api/config/#docling_graph.config.PipelineConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert config to dictionary format expected by run_pipeline.</p> Source code in <code>docling_graph/config.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert config to dictionary format expected by run_pipeline.\"\"\"\n    return {\n        \"source\": self.source,\n        \"template\": self.template,\n        \"backend\": self.backend,\n        \"inference\": self.inference,\n        \"processing_mode\": self.processing_mode,\n        \"docling_config\": self.docling_config,\n        \"use_chunking\": self.use_chunking,\n        \"llm_consolidation\": self.llm_consolidation,\n        \"model_override\": self.model_override,\n        \"provider_override\": self.provider_override,\n        \"export_format\": self.export_format,\n        \"export_docling\": self.export_docling,\n        \"export_docling_json\": self.export_docling_json,\n        \"export_markdown\": self.export_markdown,\n        \"export_per_page_markdown\": self.export_per_page_markdown,\n        \"reverse_edges\": self.reverse_edges,\n        \"output_dir\": self.output_dir,\n        \"models\": self.models.model_dump(),\n    }\n</code></pre>"},{"location":"api/config/#modelconfig","title":"ModelConfig","text":""},{"location":"api/config/#docling_graph.llm_clients.config.ModelConfig","title":"<code>docling_graph.llm_clients.config.ModelConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a single LLM model.</p> Source code in <code>docling_graph/llm_clients/config.py</code> <pre><code>@dataclass\nclass ModelConfig:\n    \"\"\"Configuration for a single LLM model.\"\"\"\n\n    model_id: str\n    context_limit: int\n    max_new_tokens: int = 4096  # Maximum tokens the model can generate in response\n    description: str = \"\"\n    notes: str = \"\"\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"ModelConfig({self.model_id}, context={self.context_limit}, \"\n            f\"max_new_tokens={self.max_new_tokens})\"\n        )\n</code></pre>"},{"location":"api/config/#providerconfig","title":"ProviderConfig","text":""},{"location":"api/config/#docling_graph.llm_clients.config.ProviderConfig","title":"<code>docling_graph.llm_clients.config.ProviderConfig</code>  <code>dataclass</code>","text":"<p>Configuration for an LLM provider.</p> Source code in <code>docling_graph/llm_clients/config.py</code> <pre><code>@dataclass\nclass ProviderConfig:\n    \"\"\"Configuration for an LLM provider.\"\"\"\n\n    provider_id: str\n    models: Dict[str, ModelConfig]\n    tokenizer: str\n    content_ratio: float = 0.8  # Ratio of context available for content vs prompt/response\n\n    def get_model(self, model_name: str) -&gt; ModelConfig | None:\n        \"\"\"Get a specific model from this provider.\"\"\"\n        return self.models.get(model_name)\n\n    def list_models(self) -&gt; list[str]:\n        \"\"\"List all available models for this provider.\"\"\"\n        return list(self.models.keys())\n\n    def get_recommended_chunk_size(self, model_name: str, schema_size: int = 0) -&gt; int:\n        \"\"\"\n        Get recommended chunk size for a model in this provider.\n\n        Uses dynamic adjustment based on schema complexity to prevent output overflow.\n\n        Args:\n            model_name: Name of the model\n            schema_size: Size of the Pydantic schema JSON (optional, for dynamic adjustment)\n\n        Returns:\n            Recommended max_tokens for DocumentChunker\n        \"\"\"\n        model = self.get_model(model_name)\n        if not model:\n            return 5120  # Default fallback\n\n        # Estimate output density from schema complexity\n        # Larger schemas = more extracted data = larger JSON output\n        if schema_size &gt; 10000:\n            output_ratio = 0.8  # Complex schema: 80% of input becomes output\n        elif schema_size &gt; 5000:\n            output_ratio = 0.5  # Medium schema: 50% expansion\n        elif schema_size &gt; 0:\n            output_ratio = 0.3  # Simple schema: 30% expansion\n        else:\n            # No schema info: use conservative default\n            output_ratio = 0.4\n\n        system_prompt_tokens = 500\n        safety_buffer = 0.8  # 20% safety margin\n\n        # Strategy 1: Output-constrained sizing\n        # Chunk size limited by max_new_tokens to prevent output overflow\n        max_safe_chunk = int(model.max_new_tokens / output_ratio * safety_buffer)\n\n        # Strategy 2: Context-constrained sizing\n        # Also respect total context window\n        max_by_context = int((model.context_limit - system_prompt_tokens) * 0.7)\n\n        # Use the SMALLER of the two (most conservative approach)\n        chunk_size = min(max_safe_chunk, max_by_context)\n\n        return max(1024, chunk_size)  # Minimum 1024 tokens\n</code></pre>"},{"location":"api/config/#docling_graph.llm_clients.config.ProviderConfig.get_model","title":"<code>get_model(model_name)</code>","text":"<p>Get a specific model from this provider.</p> Source code in <code>docling_graph/llm_clients/config.py</code> <pre><code>def get_model(self, model_name: str) -&gt; ModelConfig | None:\n    \"\"\"Get a specific model from this provider.\"\"\"\n    return self.models.get(model_name)\n</code></pre>"},{"location":"api/config/#docling_graph.llm_clients.config.ProviderConfig.get_recommended_chunk_size","title":"<code>get_recommended_chunk_size(model_name, schema_size=0)</code>","text":"<p>Get recommended chunk size for a model in this provider.</p> <p>Uses dynamic adjustment based on schema complexity to prevent output overflow.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model</p> required <code>schema_size</code> <code>int</code> <p>Size of the Pydantic schema JSON (optional, for dynamic adjustment)</p> <code>0</code> <p>Returns:</p> Type Description <code>int</code> <p>Recommended max_tokens for DocumentChunker</p> Source code in <code>docling_graph/llm_clients/config.py</code> <pre><code>def get_recommended_chunk_size(self, model_name: str, schema_size: int = 0) -&gt; int:\n    \"\"\"\n    Get recommended chunk size for a model in this provider.\n\n    Uses dynamic adjustment based on schema complexity to prevent output overflow.\n\n    Args:\n        model_name: Name of the model\n        schema_size: Size of the Pydantic schema JSON (optional, for dynamic adjustment)\n\n    Returns:\n        Recommended max_tokens for DocumentChunker\n    \"\"\"\n    model = self.get_model(model_name)\n    if not model:\n        return 5120  # Default fallback\n\n    # Estimate output density from schema complexity\n    # Larger schemas = more extracted data = larger JSON output\n    if schema_size &gt; 10000:\n        output_ratio = 0.8  # Complex schema: 80% of input becomes output\n    elif schema_size &gt; 5000:\n        output_ratio = 0.5  # Medium schema: 50% expansion\n    elif schema_size &gt; 0:\n        output_ratio = 0.3  # Simple schema: 30% expansion\n    else:\n        # No schema info: use conservative default\n        output_ratio = 0.4\n\n    system_prompt_tokens = 500\n    safety_buffer = 0.8  # 20% safety margin\n\n    # Strategy 1: Output-constrained sizing\n    # Chunk size limited by max_new_tokens to prevent output overflow\n    max_safe_chunk = int(model.max_new_tokens / output_ratio * safety_buffer)\n\n    # Strategy 2: Context-constrained sizing\n    # Also respect total context window\n    max_by_context = int((model.context_limit - system_prompt_tokens) * 0.7)\n\n    # Use the SMALLER of the two (most conservative approach)\n    chunk_size = min(max_safe_chunk, max_by_context)\n\n    return max(1024, chunk_size)  # Minimum 1024 tokens\n</code></pre>"},{"location":"api/config/#docling_graph.llm_clients.config.ProviderConfig.list_models","title":"<code>list_models()</code>","text":"<p>List all available models for this provider.</p> Source code in <code>docling_graph/llm_clients/config.py</code> <pre><code>def list_models(self) -&gt; list[str]:\n    \"\"\"List all available models for this provider.\"\"\"\n    return list(self.models.keys())\n</code></pre>"},{"location":"api/config/#configuration-functions","title":"Configuration Functions","text":""},{"location":"api/config/#get_provider_config","title":"get_provider_config","text":""},{"location":"api/config/#docling_graph.llm_clients.config.get_provider_config","title":"<code>docling_graph.llm_clients.config.get_provider_config(provider)</code>","text":"<p>Get provider configuration by name.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider ID (e.g., \"mistral\", \"openai\", \"gemini\")</p> required <p>Returns:</p> Type Description <code>ProviderConfig | None</code> <p>ProviderConfig or None if not found</p> Source code in <code>docling_graph/llm_clients/config.py</code> <pre><code>def get_provider_config(provider: str) -&gt; ProviderConfig | None:\n    \"\"\"\n    Get provider configuration by name.\n\n    Args:\n        provider: Provider ID (e.g., \"mistral\", \"openai\", \"gemini\")\n\n    Returns:\n        ProviderConfig or None if not found\n    \"\"\"\n    return PROVIDERS.get(provider.lower())\n</code></pre>"},{"location":"api/config/#get_model_config","title":"get_model_config","text":""},{"location":"api/config/#docling_graph.llm_clients.config.get_model_config","title":"<code>docling_graph.llm_clients.config.get_model_config(provider, model_name)</code>","text":"<p>Get model configuration by provider and model name.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider ID (e.g., \"mistral\", \"openai\")</p> required <code>model_name</code> <code>str</code> <p>Model name (e.g., \"mistral-large-latest\")</p> required <p>Returns:</p> Type Description <code>ModelConfig | None</code> <p>ModelConfig or None if not found</p> Source code in <code>docling_graph/llm_clients/config.py</code> <pre><code>def get_model_config(provider: str, model_name: str) -&gt; ModelConfig | None:\n    \"\"\"\n    Get model configuration by provider and model name.\n\n    Args:\n        provider: Provider ID (e.g., \"mistral\", \"openai\")\n        model_name: Model name (e.g., \"mistral-large-latest\")\n\n    Returns:\n        ModelConfig or None if not found\n    \"\"\"\n    provider_config = get_provider_config(provider)\n    if not provider_config:\n        return None\n    return provider_config.get_model(model_name)\n</code></pre>"},{"location":"api/config/#get_context_limit","title":"get_context_limit","text":""},{"location":"api/config/#docling_graph.llm_clients.config.get_context_limit","title":"<code>docling_graph.llm_clients.config.get_context_limit(provider, model)</code>","text":"<p>Get context window size for a model.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider ID</p> required <code>model</code> <code>str</code> <p>Model name</p> required <p>Returns:</p> Type Description <code>int</code> <p>Context limit in tokens (defaults to 8000 if not found)</p> Source code in <code>docling_graph/llm_clients/config.py</code> <pre><code>def get_context_limit(provider: str, model: str) -&gt; int:\n    \"\"\"\n    Get context window size for a model.\n\n    Args:\n        provider: Provider ID\n        model: Model name\n\n    Returns:\n        Context limit in tokens (defaults to 8000 if not found)\n    \"\"\"\n    config = get_model_config(provider, model)\n    if config:\n        return config.context_limit\n    return 8000  # Safe default\n</code></pre>"},{"location":"api/config/#get_recommended_chunk_size","title":"get_recommended_chunk_size","text":""},{"location":"api/config/#docling_graph.llm_clients.config.get_recommended_chunk_size","title":"<code>docling_graph.llm_clients.config.get_recommended_chunk_size(provider, model, schema_size=0)</code>","text":"<p>Get recommended chunk size for chunker based on model's context window.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider ID</p> required <code>model</code> <code>str</code> <p>Model name</p> required <code>schema_size</code> <code>int</code> <p>Size of the Pydantic schema JSON (optional, for dynamic adjustment)</p> <code>0</code> <p>Returns:</p> Type Description <code>int</code> <p>Recommended max_tokens for DocumentChunker</p> Source code in <code>docling_graph/llm_clients/config.py</code> <pre><code>def get_recommended_chunk_size(provider: str, model: str, schema_size: int = 0) -&gt; int:\n    \"\"\"\n    Get recommended chunk size for chunker based on model's context window.\n\n    Args:\n        provider: Provider ID\n        model: Model name\n        schema_size: Size of the Pydantic schema JSON (optional, for dynamic adjustment)\n\n    Returns:\n        Recommended max_tokens for DocumentChunker\n    \"\"\"\n    provider_config = get_provider_config(provider)\n    if provider_config:\n        return provider_config.get_recommended_chunk_size(model, schema_size)\n    return 5120  # Default fallback\n</code></pre>"},{"location":"api/config/#see-also","title":"See Also","text":"<ul> <li>Pipeline API - Main pipeline interface</li> <li>Configuration Guide - Detailed configuration guide</li> <li>LLM Clients - LLM client implementations</li> </ul>"},{"location":"api/converters/","title":"Converters API","text":"<p>Graph conversion utilities for transforming Pydantic models into NetworkX graphs.</p>"},{"location":"api/converters/#graphconverter","title":"GraphConverter","text":""},{"location":"api/converters/#docling_graph.core.converters.graph_converter.GraphConverter","title":"<code>docling_graph.core.converters.graph_converter.GraphConverter</code>","text":"<p>Converts Pydantic models to NetworkX graphs with enhanced features.</p> <p>This converter supports: - Deterministic node ID generation via NodeIDRegistry - Automatic graph cleanup (phantom nodes, duplicates) - Stable node IDs across batch extractions - Bidirectional edges - Full validation</p> <p>This converter is stateless and thread-safe. All conversion state is managed through method parameters rather than instance variables.</p> Source code in <code>docling_graph/core/converters/graph_converter.py</code> <pre><code>class GraphConverter:\n    \"\"\"Converts Pydantic models to NetworkX graphs with enhanced features.\n\n    This converter supports:\n    - Deterministic node ID generation via NodeIDRegistry\n    - Automatic graph cleanup (phantom nodes, duplicates)\n    - Stable node IDs across batch extractions\n    - Bidirectional edges\n    - Full validation\n\n    This converter is stateless and thread-safe. All conversion state is managed\n    through method parameters rather than instance variables.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: GraphConfig | None = None,\n        add_reverse_edges: bool = False,\n        validate_graph: bool = True,\n        registry: NodeIDRegistry | None = None,\n        auto_cleanup: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the graph converter.\n\n        Args:\n            config: Graph configuration (optional)\n            add_reverse_edges: Create bidirectional edges (default: False)\n            validate_graph: Validate graph structure (default: True)\n            registry: NodeIDRegistry for deterministic node IDs across batches.\n                If None, creates a new registry per conversion (works for single-batch).\n                Pass a shared registry for cross-batch consistency.\n            auto_cleanup: Automatically cleanup graph after conversion,\n                removing phantom nodes, duplicates, orphaned edges (default: True)\n        \"\"\"\n        self.config = config or GraphConfig()\n        self.add_reverse_edges = add_reverse_edges or self.config.add_reverse_edges\n        self.validate_graph = validate_graph or self.config.validate_graph\n\n        # Initialize registry (use provided or create new)\n        self.registry = registry or NodeIDRegistry()\n\n        # Initialize cleaner for automatic cleanup\n        self.auto_cleanup = auto_cleanup\n        self.cleaner = GraphCleaner(verbose=True) if auto_cleanup else None\n\n    def pydantic_list_to_graph(\n        self,\n        model_instances: List[BaseModel],\n    ) -&gt; tuple[nx.DiGraph, GraphMetadata]:\n        \"\"\"\n        Convert list of Pydantic models to a NetworkX graph.\n\n        Process:\n        1. Pre-register all models for deterministic node IDs\n        2. Create nodes from models\n        3. Create edges between entities\n        4. Apply automatic cleanup (if enabled)\n        5. Validate graph structure\n        6. Calculate statistics\n\n        Args:\n            model_instances: List of Pydantic model instances to convert\n\n        Returns:\n            Tuple of (graph, metadata)\n        \"\"\"\n        if not model_instances:\n            raise ValueError(\"Cannot create graph from empty model list\")\n\n        # Pre-register all models to ensure consistent node IDs across batches\n        rich_print(\n            \"[blue][GraphConverter][/blue] Pre-registering models for deterministic node IDs...\"\n        )\n        self.registry.register_batch(model_instances)\n\n        # Create fresh graph for this conversion\n        graph = nx.DiGraph()\n        visited_ids: Set[str] = set()\n\n        # First pass: create nodes\n        for model in model_instances:\n            self._create_nodes_pass(model, graph, visited_ids)\n\n        # Second pass: create edges\n        edges_to_add: List[Edge] = []\n        for model in model_instances:\n            edges = self._create_edges_pass(model, visited_ids)\n            edges_to_add.extend(edges)\n\n        # Add edges to graph\n        edge_list = [(e.source, e.target, {\"label\": e.label, **e.properties}) for e in edges_to_add]\n\n        if self.add_reverse_edges:\n            reverse_edge_list = [\n                (\n                    e.target,\n                    e.source,\n                    {\"label\": f\"reverse_{e.label}\", **e.properties},\n                )\n                for e in edges_to_add\n            ]\n            edge_list.extend(reverse_edge_list)\n\n        graph.add_edges_from(edge_list)\n\n        # Auto-cleanup if enabled\n        if self.auto_cleanup and self.cleaner:\n            rich_print(\"[blue][GraphConverter][/blue] Running automatic graph cleanup...\")\n            graph = self.cleaner.clean_graph(graph)\n\n        # Validate\n        if self.validate_graph:\n            try:\n                validate_graph_structure(graph, raise_on_error=True)\n                rich_print(\"[green][GraphConverter][/green] Graph structure validated successfully\")\n            except ValueError as e:\n                rich_print(f\"[red][GraphConverter][/red] Validation failed: {e}\")\n                raise\n\n        # Calculate statistics\n        registry_stats = self.registry.get_stats()\n        rich_print(\n            f\"[blue][GraphConverter][/blue] Final graph: \"\n            f\"[cyan]{graph.number_of_nodes()}[/cyan] nodes, \"\n            f\"[yellow]{graph.number_of_edges()}[/yellow] edges\\n\"\n            f\"  Registry: {registry_stats['total_entities']} entities across \"\n            f\"{len(registry_stats['classes'])} classes\"\n        )\n\n        metadata = calculate_graph_stats(graph, len(model_instances))\n        return graph, metadata\n\n    def _create_nodes_pass(\n        self,\n        model: BaseModel,\n        graph: nx.DiGraph,\n        visited_ids: Set[str],\n    ) -&gt; None:\n        \"\"\"Recursively create nodes from model and nested entities.\"\"\"\n        # Check if this model should be an entity (respect is_entity=False)\n        model_config = model.model_config\n        is_entity = (\n            model_config.get(\"is_entity\", True)\n            if hasattr(model_config, \"get\")\n            else getattr(model_config, \"is_entity\", True)\n        )\n\n        if not is_entity:\n            # Skip node creation for non-entities (they will be embedded in parent nodes)\n            return\n\n        # Get node ID from registry\n        node_id = self._get_node_id(model)\n\n        if node_id in visited_ids:\n            return\n\n        visited_ids.add(node_id)\n\n        # Prepare node attributes\n        node_attrs: dict[str, Any] = {\n            \"id\": node_id,\n            \"label\": model.__class__.__name__,\n            \"type\": \"entity\",\n            \"__class__\": model.__class__.__name__,\n        }\n\n        # Add all fields from model\n        for field_name, field_value in model:\n            if isinstance(field_value, BaseModel):\n                node_attrs[field_name] = None  # Reference, not value\n                self._create_nodes_pass(field_value, graph, visited_ids)\n            elif isinstance(field_value, list) and field_value:\n                if isinstance(field_value[0], BaseModel):\n                    # List of nested entities\n                    node_attrs[field_name] = None\n                    for item in field_value:\n                        self._create_nodes_pass(item, graph, visited_ids)\n                else:\n                    # List of primitives\n                    node_attrs[field_name] = field_value\n            else:\n                node_attrs[field_name] = field_value\n\n        graph.add_node(node_id, **node_attrs)\n\n    def _create_edges_pass(\n        self,\n        model: BaseModel,\n        visited_ids: Set[str],\n    ) -&gt; List[Edge]:\n        \"\"\"Recursively create edges from model relationships.\"\"\"\n        edges: List[Edge] = []\n        source_id = self._get_node_id(model)\n\n        # Process all fields\n        for field_name, field_value in model:\n            # Check for explicit edge label in field metadata\n            edge_label = self._get_edge_label(model, field_name)\n\n            if isinstance(field_value, BaseModel):\n                target_id = self._get_node_id(field_value)\n                edges.append(\n                    Edge(\n                        source=source_id,\n                        target=target_id,\n                        label=edge_label or field_name,\n                        properties={},\n                    )\n                )\n\n                # Recursively process nested model\n                edges.extend(self._create_edges_pass(field_value, visited_ids))\n\n            elif isinstance(field_value, list) and field_value:\n                if isinstance(field_value[0], BaseModel):\n                    for item in field_value:\n                        target_id = self._get_node_id(item)\n                        edges.append(\n                            Edge(\n                                source=source_id,\n                                target=target_id,\n                                label=edge_label or field_name,\n                                properties={},\n                            )\n                        )\n\n                        # Recursively process nested model\n                        edges.extend(self._create_edges_pass(item, visited_ids))\n\n        return edges\n\n    def _get_node_id(self, model: BaseModel) -&gt; str:\n        \"\"\"Get deterministic node ID from registry.\"\"\"\n        return self.registry.get_node_id(model)\n\n    def _get_edge_label(self, model: BaseModel, field_name: str) -&gt; str | None:\n        \"\"\"\n        Extract edge label from field metadata if available.\n\n        Looks for json_schema_extra['edge_label'] in field info.\n        \"\"\"\n        field_info = model.model_fields.get(field_name)\n        if field_info and isinstance(field_info.json_schema_extra, Mapping):\n            value = field_info.json_schema_extra.get(\"edge_label\")\n            if isinstance(value, str):\n                return value\n        return None\n\n    def set_registry(self, registry: NodeIDRegistry) -&gt; None:\n        \"\"\"Update the registry (for sharing across multiple conversions).\"\"\"\n        self.registry = registry\n        rich_print(\n            f\"[blue][GraphConverter][/blue] Registry updated with \"\n            f\"{registry.get_stats()['total_entities']} entities\"\n        )\n</code></pre>"},{"location":"api/converters/#docling_graph.core.converters.graph_converter.GraphConverter.__init__","title":"<code>__init__(config=None, add_reverse_edges=False, validate_graph=True, registry=None, auto_cleanup=True)</code>","text":"<p>Initialize the graph converter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GraphConfig | None</code> <p>Graph configuration (optional)</p> <code>None</code> <code>add_reverse_edges</code> <code>bool</code> <p>Create bidirectional edges (default: False)</p> <code>False</code> <code>validate_graph</code> <code>bool</code> <p>Validate graph structure (default: True)</p> <code>True</code> <code>registry</code> <code>NodeIDRegistry | None</code> <p>NodeIDRegistry for deterministic node IDs across batches. If None, creates a new registry per conversion (works for single-batch). Pass a shared registry for cross-batch consistency.</p> <code>None</code> <code>auto_cleanup</code> <code>bool</code> <p>Automatically cleanup graph after conversion, removing phantom nodes, duplicates, orphaned edges (default: True)</p> <code>True</code> Source code in <code>docling_graph/core/converters/graph_converter.py</code> <pre><code>def __init__(\n    self,\n    config: GraphConfig | None = None,\n    add_reverse_edges: bool = False,\n    validate_graph: bool = True,\n    registry: NodeIDRegistry | None = None,\n    auto_cleanup: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize the graph converter.\n\n    Args:\n        config: Graph configuration (optional)\n        add_reverse_edges: Create bidirectional edges (default: False)\n        validate_graph: Validate graph structure (default: True)\n        registry: NodeIDRegistry for deterministic node IDs across batches.\n            If None, creates a new registry per conversion (works for single-batch).\n            Pass a shared registry for cross-batch consistency.\n        auto_cleanup: Automatically cleanup graph after conversion,\n            removing phantom nodes, duplicates, orphaned edges (default: True)\n    \"\"\"\n    self.config = config or GraphConfig()\n    self.add_reverse_edges = add_reverse_edges or self.config.add_reverse_edges\n    self.validate_graph = validate_graph or self.config.validate_graph\n\n    # Initialize registry (use provided or create new)\n    self.registry = registry or NodeIDRegistry()\n\n    # Initialize cleaner for automatic cleanup\n    self.auto_cleanup = auto_cleanup\n    self.cleaner = GraphCleaner(verbose=True) if auto_cleanup else None\n</code></pre>"},{"location":"api/converters/#docling_graph.core.converters.graph_converter.GraphConverter.pydantic_list_to_graph","title":"<code>pydantic_list_to_graph(model_instances)</code>","text":"<p>Convert list of Pydantic models to a NetworkX graph.</p> <p>Process: 1. Pre-register all models for deterministic node IDs 2. Create nodes from models 3. Create edges between entities 4. Apply automatic cleanup (if enabled) 5. Validate graph structure 6. Calculate statistics</p> <p>Parameters:</p> Name Type Description Default <code>model_instances</code> <code>List[BaseModel]</code> <p>List of Pydantic model instances to convert</p> required <p>Returns:</p> Type Description <code>tuple[DiGraph, GraphMetadata]</code> <p>Tuple of (graph, metadata)</p> Source code in <code>docling_graph/core/converters/graph_converter.py</code> <pre><code>def pydantic_list_to_graph(\n    self,\n    model_instances: List[BaseModel],\n) -&gt; tuple[nx.DiGraph, GraphMetadata]:\n    \"\"\"\n    Convert list of Pydantic models to a NetworkX graph.\n\n    Process:\n    1. Pre-register all models for deterministic node IDs\n    2. Create nodes from models\n    3. Create edges between entities\n    4. Apply automatic cleanup (if enabled)\n    5. Validate graph structure\n    6. Calculate statistics\n\n    Args:\n        model_instances: List of Pydantic model instances to convert\n\n    Returns:\n        Tuple of (graph, metadata)\n    \"\"\"\n    if not model_instances:\n        raise ValueError(\"Cannot create graph from empty model list\")\n\n    # Pre-register all models to ensure consistent node IDs across batches\n    rich_print(\n        \"[blue][GraphConverter][/blue] Pre-registering models for deterministic node IDs...\"\n    )\n    self.registry.register_batch(model_instances)\n\n    # Create fresh graph for this conversion\n    graph = nx.DiGraph()\n    visited_ids: Set[str] = set()\n\n    # First pass: create nodes\n    for model in model_instances:\n        self._create_nodes_pass(model, graph, visited_ids)\n\n    # Second pass: create edges\n    edges_to_add: List[Edge] = []\n    for model in model_instances:\n        edges = self._create_edges_pass(model, visited_ids)\n        edges_to_add.extend(edges)\n\n    # Add edges to graph\n    edge_list = [(e.source, e.target, {\"label\": e.label, **e.properties}) for e in edges_to_add]\n\n    if self.add_reverse_edges:\n        reverse_edge_list = [\n            (\n                e.target,\n                e.source,\n                {\"label\": f\"reverse_{e.label}\", **e.properties},\n            )\n            for e in edges_to_add\n        ]\n        edge_list.extend(reverse_edge_list)\n\n    graph.add_edges_from(edge_list)\n\n    # Auto-cleanup if enabled\n    if self.auto_cleanup and self.cleaner:\n        rich_print(\"[blue][GraphConverter][/blue] Running automatic graph cleanup...\")\n        graph = self.cleaner.clean_graph(graph)\n\n    # Validate\n    if self.validate_graph:\n        try:\n            validate_graph_structure(graph, raise_on_error=True)\n            rich_print(\"[green][GraphConverter][/green] Graph structure validated successfully\")\n        except ValueError as e:\n            rich_print(f\"[red][GraphConverter][/red] Validation failed: {e}\")\n            raise\n\n    # Calculate statistics\n    registry_stats = self.registry.get_stats()\n    rich_print(\n        f\"[blue][GraphConverter][/blue] Final graph: \"\n        f\"[cyan]{graph.number_of_nodes()}[/cyan] nodes, \"\n        f\"[yellow]{graph.number_of_edges()}[/yellow] edges\\n\"\n        f\"  Registry: {registry_stats['total_entities']} entities across \"\n        f\"{len(registry_stats['classes'])} classes\"\n    )\n\n    metadata = calculate_graph_stats(graph, len(model_instances))\n    return graph, metadata\n</code></pre>"},{"location":"api/converters/#docling_graph.core.converters.graph_converter.GraphConverter.set_registry","title":"<code>set_registry(registry)</code>","text":"<p>Update the registry (for sharing across multiple conversions).</p> Source code in <code>docling_graph/core/converters/graph_converter.py</code> <pre><code>def set_registry(self, registry: NodeIDRegistry) -&gt; None:\n    \"\"\"Update the registry (for sharing across multiple conversions).\"\"\"\n    self.registry = registry\n    rich_print(\n        f\"[blue][GraphConverter][/blue] Registry updated with \"\n        f\"{registry.get_stats()['total_entities']} entities\"\n    )\n</code></pre>"},{"location":"api/converters/#graphconfig","title":"GraphConfig","text":""},{"location":"api/converters/#docling_graph.core.converters.config.GraphConfig","title":"<code>docling_graph.core.converters.config.GraphConfig</code>  <code>dataclass</code>","text":"<p>Internal Constants.</p> Source code in <code>docling_graph/core/converters/config.py</code> <pre><code>@dataclass(frozen=True)\nclass GraphConfig:\n    \"\"\"Internal Constants.\"\"\"\n\n    # Node ID generation\n    NODE_ID_HASH_LENGTH: Final[int] = 12\n\n    # Serialization\n    MAX_STRING_LENGTH: Final[int] = 1000\n    TRUNCATE_SUFFIX: Final[str] = \"...\"\n\n    \"\"\"Configuration Options.\"\"\"\n\n    # Edge options\n    add_reverse_edges: bool = False\n\n    # Trigger graph validation\n    validate_graph: bool = True\n</code></pre>"},{"location":"api/converters/#docling_graph.core.converters.config.GraphConfig.TRUNCATE_SUFFIX","title":"<code>TRUNCATE_SUFFIX = '...'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Configuration Options.</p>"},{"location":"api/converters/#exportconfig","title":"ExportConfig","text":""},{"location":"api/converters/#docling_graph.core.converters.config.ExportConfig","title":"<code>docling_graph.core.converters.config.ExportConfig</code>  <code>dataclass</code>","text":"<p>Configuration for graph export.</p> Source code in <code>docling_graph/core/converters/config.py</code> <pre><code>@dataclass(frozen=True)\nclass ExportConfig:\n    \"\"\"Configuration for graph export.\"\"\"\n\n    # CSV export\n    CSV_ENCODING: str = \"utf-8\"\n    CSV_NODE_FILENAME: str = \"nodes.csv\"\n    CSV_EDGE_FILENAME: str = \"edges.csv\"\n\n    # Cypher export\n    CYPHER_ENCODING: str = \"utf-8\"\n    CYPHER_FILENAME: str = \"graph.cypher\"\n    CYPHER_BATCH_SIZE: int = 1000\n\n    # JSON export\n    JSON_ENCODING: str = \"utf-8\"\n    JSON_INDENT: int = 2\n    JSON_FILENAME: str = \"graph.json\"\n\n    # General\n    ENSURE_ASCII: bool = False\n</code></pre>"},{"location":"api/converters/#nodeidregistry","title":"NodeIDRegistry","text":""},{"location":"api/converters/#docling_graph.core.converters.node_id_registry.NodeIDRegistry","title":"<code>docling_graph.core.converters.node_id_registry.NodeIDRegistry</code>","text":"<p>Global registry that maps entity fingerprints to stable node IDs.</p> <p>This ensures deterministic, globally-consistent node IDs across: - Multiple batch extractions - Model merging operations - Graph conversion</p> Source code in <code>docling_graph/core/converters/node_id_registry.py</code> <pre><code>class NodeIDRegistry:\n    \"\"\"\n    Global registry that maps entity fingerprints to stable node IDs.\n\n    This ensures deterministic, globally-consistent node IDs across:\n    - Multiple batch extractions\n    - Model merging operations\n    - Graph conversion\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the registry.\"\"\"\n        # Map: entity_fingerprint \u2192 node_id\n        self.fingerprint_to_id: Dict[str, str] = {}\n\n        # Map: node_id \u2192 entity_fingerprint (reverse lookup)\n        self.id_to_fingerprint: Dict[str, str] = {}\n\n        # Track seen nodes for collision detection\n        self.seen_classes: Dict[str, Set[str]] = {}\n\n    def _generate_fingerprint(self, model_instance: BaseModel) -&gt; str:\n        \"\"\"\n        Generate a fingerprint (content hash) for an entity.\n\n        This identifies WHAT the entity is (independent of when it was created).\n\n        For materials: hash of (name, category, chemical_formula)\n        For measurements: hash of (name, numeric_value, unit)\n        For processes: hash of (step_type, name, sequence_order)\n        \"\"\"\n        # Extract identity fields that uniquely identify this entity\n        model_config = model_instance.model_config\n\n        # Get graph_id_fields from config\n        id_fields: List[str] = []\n        if hasattr(model_config, \"get\"):\n            id_fields = cast(List[str], model_config.get(\"graph_id_fields\", []))\n        else:\n            id_fields = cast(List[str], getattr(model_config, \"graph_id_fields\", []))\n\n        # Build fingerprint from identity fields\n        fingerprint_data = {}\n\n        if id_fields:\n            for field in id_fields:\n                if hasattr(model_instance, field):\n                    value = getattr(model_instance, field)\n\n                    # Normalize lists to sorted tuples\n                    if isinstance(value, list):\n                        try:\n                            value = tuple(sorted(set(value)))\n                        except TypeError:\n                            value = tuple(value)\n\n                    fingerprint_data[field] = value\n        else:\n            # No identity fields specified; use all non-empty fields\n            for field_name, field_value in model_instance:\n                if field_value and not isinstance(field_value, list | dict | BaseModel):\n                    fingerprint_data[field_name] = field_value\n\n        fingerprint_data[\"__class__\"] = model_instance.__class__.__name__\n\n        # Create deterministic hash\n        fingerprint_str = json.dumps(fingerprint_data, sort_keys=True, default=str)\n        fingerprint = hashlib.blake2b(fingerprint_str.encode()).hexdigest()[:16]\n\n        return fingerprint\n\n    def get_node_id(\n        self,\n        model_instance: BaseModel,\n        auto_register: bool = True,\n    ) -&gt; str:\n        \"\"\"\n        Get or create a deterministic node ID for a model instance.\n\n        Args:\n            model_instance: The Pydantic model instance\n            auto_register: If True, automatically register unknown entities\n\n        Returns:\n            Stable node ID in format: ClassName_fingerprint\n        \"\"\"\n        fingerprint = self._generate_fingerprint(model_instance)\n        class_name = model_instance.__class__.__name__\n\n        # Check if we've seen this entity before\n        if fingerprint in self.fingerprint_to_id:\n            existing_id = self.fingerprint_to_id[fingerprint]\n            # Verify class name matches (detect collisions)\n            if not existing_id.startswith(class_name):\n                raise ValueError(\n                    f\"Node ID collision: fingerprint {fingerprint} maps to both \"\n                    f\"{existing_id} (old) and {class_name} (new)\"\n                )\n            return existing_id\n\n        # Create new node ID\n        if class_name not in self.seen_classes:\n            self.seen_classes[class_name] = set()\n\n        node_id = f\"{class_name}_{fingerprint}\"\n\n        if auto_register:\n            self.fingerprint_to_id[fingerprint] = node_id\n            self.id_to_fingerprint[node_id] = fingerprint\n            self.seen_classes[class_name].add(fingerprint)\n\n        return node_id\n\n    def register_batch(self, models: list[BaseModel]) -&gt; None:\n        \"\"\"\n        Register all models in a batch to pre-populate the registry.\n\n        Call this BEFORE converting models to graph to ensure consistent IDs.\n        \"\"\"\n        for model in models:\n            self.get_node_id(model, auto_register=True)\n\n    def get_stats(self) -&gt; dict:\n        \"\"\"Get registry statistics.\"\"\"\n        return {\n            \"total_entities\": len(self.fingerprint_to_id),\n            \"classes\": {cls: len(fps) for cls, fps in self.seen_classes.items()},\n        }\n</code></pre>"},{"location":"api/converters/#docling_graph.core.converters.node_id_registry.NodeIDRegistry.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the registry.</p> Source code in <code>docling_graph/core/converters/node_id_registry.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the registry.\"\"\"\n    # Map: entity_fingerprint \u2192 node_id\n    self.fingerprint_to_id: Dict[str, str] = {}\n\n    # Map: node_id \u2192 entity_fingerprint (reverse lookup)\n    self.id_to_fingerprint: Dict[str, str] = {}\n\n    # Track seen nodes for collision detection\n    self.seen_classes: Dict[str, Set[str]] = {}\n</code></pre>"},{"location":"api/converters/#docling_graph.core.converters.node_id_registry.NodeIDRegistry.get_node_id","title":"<code>get_node_id(model_instance, auto_register=True)</code>","text":"<p>Get or create a deterministic node ID for a model instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_instance</code> <code>BaseModel</code> <p>The Pydantic model instance</p> required <code>auto_register</code> <code>bool</code> <p>If True, automatically register unknown entities</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Stable node ID in format: ClassName_fingerprint</p> Source code in <code>docling_graph/core/converters/node_id_registry.py</code> <pre><code>def get_node_id(\n    self,\n    model_instance: BaseModel,\n    auto_register: bool = True,\n) -&gt; str:\n    \"\"\"\n    Get or create a deterministic node ID for a model instance.\n\n    Args:\n        model_instance: The Pydantic model instance\n        auto_register: If True, automatically register unknown entities\n\n    Returns:\n        Stable node ID in format: ClassName_fingerprint\n    \"\"\"\n    fingerprint = self._generate_fingerprint(model_instance)\n    class_name = model_instance.__class__.__name__\n\n    # Check if we've seen this entity before\n    if fingerprint in self.fingerprint_to_id:\n        existing_id = self.fingerprint_to_id[fingerprint]\n        # Verify class name matches (detect collisions)\n        if not existing_id.startswith(class_name):\n            raise ValueError(\n                f\"Node ID collision: fingerprint {fingerprint} maps to both \"\n                f\"{existing_id} (old) and {class_name} (new)\"\n            )\n        return existing_id\n\n    # Create new node ID\n    if class_name not in self.seen_classes:\n        self.seen_classes[class_name] = set()\n\n    node_id = f\"{class_name}_{fingerprint}\"\n\n    if auto_register:\n        self.fingerprint_to_id[fingerprint] = node_id\n        self.id_to_fingerprint[node_id] = fingerprint\n        self.seen_classes[class_name].add(fingerprint)\n\n    return node_id\n</code></pre>"},{"location":"api/converters/#docling_graph.core.converters.node_id_registry.NodeIDRegistry.get_stats","title":"<code>get_stats()</code>","text":"<p>Get registry statistics.</p> Source code in <code>docling_graph/core/converters/node_id_registry.py</code> <pre><code>def get_stats(self) -&gt; dict:\n    \"\"\"Get registry statistics.\"\"\"\n    return {\n        \"total_entities\": len(self.fingerprint_to_id),\n        \"classes\": {cls: len(fps) for cls, fps in self.seen_classes.items()},\n    }\n</code></pre>"},{"location":"api/converters/#docling_graph.core.converters.node_id_registry.NodeIDRegistry.register_batch","title":"<code>register_batch(models)</code>","text":"<p>Register all models in a batch to pre-populate the registry.</p> <p>Call this BEFORE converting models to graph to ensure consistent IDs.</p> Source code in <code>docling_graph/core/converters/node_id_registry.py</code> <pre><code>def register_batch(self, models: list[BaseModel]) -&gt; None:\n    \"\"\"\n    Register all models in a batch to pre-populate the registry.\n\n    Call this BEFORE converting models to graph to ensure consistent IDs.\n    \"\"\"\n    for model in models:\n        self.get_node_id(model, auto_register=True)\n</code></pre>"},{"location":"api/converters/#see-also","title":"See Also","text":"<ul> <li>Exporters API - Export graph data</li> <li>Pydantic Templates Guide - Create templates</li> </ul>"},{"location":"api/exporters/","title":"Exporters API","text":"<p>Export graph data to various formats.</p>"},{"location":"api/exporters/#exporterbase","title":"ExporterBase","text":""},{"location":"api/exporters/#docling_graph.core.exporters.base.GraphExporterProtocol","title":"<code>docling_graph.core.exporters.base.GraphExporterProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for graph export implementations.</p> Source code in <code>docling_graph/core/exporters/base.py</code> <pre><code>@runtime_checkable\nclass GraphExporterProtocol(Protocol):\n    \"\"\"Protocol for graph export implementations.\"\"\"\n\n    def export(self, graph: nx.DiGraph, output_path: Path) -&gt; None:\n        \"\"\"Export graph to specified format.\n\n        Args:\n            graph: NetworkX directed graph to export.\n            output_path: Path where to save the exported graph.\n        \"\"\"\n        ...\n\n    def validate_graph(self, graph: nx.DiGraph) -&gt; bool:\n        \"\"\"Validate that graph can be exported.\n\n        Args:\n            graph: NetworkX directed graph to validate.\n\n        Returns:\n            True if graph is valid for export.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.base.GraphExporterProtocol.export","title":"<code>export(graph, output_path)</code>","text":"<p>Export graph to specified format.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>NetworkX directed graph to export.</p> required <code>output_path</code> <code>Path</code> <p>Path where to save the exported graph.</p> required Source code in <code>docling_graph/core/exporters/base.py</code> <pre><code>def export(self, graph: nx.DiGraph, output_path: Path) -&gt; None:\n    \"\"\"Export graph to specified format.\n\n    Args:\n        graph: NetworkX directed graph to export.\n        output_path: Path where to save the exported graph.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.base.GraphExporterProtocol.validate_graph","title":"<code>validate_graph(graph)</code>","text":"<p>Validate that graph can be exported.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>NetworkX directed graph to validate.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if graph is valid for export.</p> Source code in <code>docling_graph/core/exporters/base.py</code> <pre><code>def validate_graph(self, graph: nx.DiGraph) -&gt; bool:\n    \"\"\"Validate that graph can be exported.\n\n    Args:\n        graph: NetworkX directed graph to validate.\n\n    Returns:\n        True if graph is valid for export.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/exporters/#jsonexporter","title":"JSONExporter","text":""},{"location":"api/exporters/#docling_graph.core.exporters.json_exporter.JSONExporter","title":"<code>docling_graph.core.exporters.json_exporter.JSONExporter</code>","text":"<p>Export graph to JSON format.</p> Source code in <code>docling_graph/core/exporters/json_exporter.py</code> <pre><code>class JSONExporter:\n    \"\"\"Export graph to JSON format.\"\"\"\n\n    def __init__(self, config: ExportConfig | None = None) -&gt; None:\n        \"\"\"Initialize JSON exporter.\n\n        Args:\n            config: Export configuration. Uses defaults if None.\n        \"\"\"\n        self.config = config or ExportConfig()\n\n    def export(self, graph: nx.DiGraph, output_path: Path) -&gt; None:\n        \"\"\"Export graph to JSON.\n\n        Args:\n            graph: NetworkX directed graph to export.\n            output_path: File path where to save JSON.\n\n        Raises:\n            ValueError: If graph is empty.\n        \"\"\"\n        if not self.validate_graph(graph):\n            raise ValueError(\"Cannot export empty graph\")\n\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        graph_dict = self._graph_to_dict(graph)\n\n        with open(output_path, \"w\", encoding=self.config.JSON_ENCODING) as f:\n            json.dump(\n                graph_dict,\n                f,\n                indent=self.config.JSON_INDENT,\n                ensure_ascii=self.config.ENSURE_ASCII,\n                default=json_serializable,\n            )\n\n    def validate_graph(self, graph: nx.DiGraph) -&gt; bool:\n        \"\"\"Validate that graph is not empty.\n\n        Args:\n            graph: NetworkX directed graph.\n\n        Returns:\n            True if graph has nodes.\n        \"\"\"\n        num_nodes = cast(int, graph.number_of_nodes())\n        return num_nodes &gt; 0\n\n    @staticmethod\n    def _graph_to_dict(graph: nx.DiGraph) -&gt; Dict[str, Any]:\n        \"\"\"Convert graph to dictionary format.\n\n        Args:\n            graph: NetworkX directed graph.\n\n        Returns:\n            Dictionary representation of graph.\n        \"\"\"\n        nodes: List[Dict[str, Any]] = []\n        for node_id, data in graph.nodes(data=True):\n            node_dict = {\"id\": node_id, **data}\n            nodes.append(node_dict)\n\n        edges: List[Dict[str, Any]] = []\n        for source, target, data in graph.edges(data=True):\n            edge_dict = {\"source\": source, \"target\": target, **data}\n            edges.append(edge_dict)\n\n        return {\n            \"nodes\": nodes,\n            \"edges\": edges,\n            \"metadata\": {\"node_count\": len(nodes), \"edge_count\": len(edges)},\n        }\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.json_exporter.JSONExporter.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize JSON exporter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ExportConfig | None</code> <p>Export configuration. Uses defaults if None.</p> <code>None</code> Source code in <code>docling_graph/core/exporters/json_exporter.py</code> <pre><code>def __init__(self, config: ExportConfig | None = None) -&gt; None:\n    \"\"\"Initialize JSON exporter.\n\n    Args:\n        config: Export configuration. Uses defaults if None.\n    \"\"\"\n    self.config = config or ExportConfig()\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.json_exporter.JSONExporter.export","title":"<code>export(graph, output_path)</code>","text":"<p>Export graph to JSON.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>NetworkX directed graph to export.</p> required <code>output_path</code> <code>Path</code> <p>File path where to save JSON.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If graph is empty.</p> Source code in <code>docling_graph/core/exporters/json_exporter.py</code> <pre><code>def export(self, graph: nx.DiGraph, output_path: Path) -&gt; None:\n    \"\"\"Export graph to JSON.\n\n    Args:\n        graph: NetworkX directed graph to export.\n        output_path: File path where to save JSON.\n\n    Raises:\n        ValueError: If graph is empty.\n    \"\"\"\n    if not self.validate_graph(graph):\n        raise ValueError(\"Cannot export empty graph\")\n\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    graph_dict = self._graph_to_dict(graph)\n\n    with open(output_path, \"w\", encoding=self.config.JSON_ENCODING) as f:\n        json.dump(\n            graph_dict,\n            f,\n            indent=self.config.JSON_INDENT,\n            ensure_ascii=self.config.ENSURE_ASCII,\n            default=json_serializable,\n        )\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.json_exporter.JSONExporter.validate_graph","title":"<code>validate_graph(graph)</code>","text":"<p>Validate that graph is not empty.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>NetworkX directed graph.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if graph has nodes.</p> Source code in <code>docling_graph/core/exporters/json_exporter.py</code> <pre><code>def validate_graph(self, graph: nx.DiGraph) -&gt; bool:\n    \"\"\"Validate that graph is not empty.\n\n    Args:\n        graph: NetworkX directed graph.\n\n    Returns:\n        True if graph has nodes.\n    \"\"\"\n    num_nodes = cast(int, graph.number_of_nodes())\n    return num_nodes &gt; 0\n</code></pre>"},{"location":"api/exporters/#csvexporter","title":"CSVExporter","text":""},{"location":"api/exporters/#docling_graph.core.exporters.csv_exporter.CSVExporter","title":"<code>docling_graph.core.exporters.csv_exporter.CSVExporter</code>","text":"<p>Export graph to CSV format compatible with Neo4j import.</p> Source code in <code>docling_graph/core/exporters/csv_exporter.py</code> <pre><code>class CSVExporter:\n    \"\"\"Export graph to CSV format compatible with Neo4j import.\"\"\"\n\n    def __init__(self, config: ExportConfig | None = None) -&gt; None:\n        \"\"\"Initialize CSV exporter.\n\n        Args:\n            config: Export configuration. Uses defaults if None.\n        \"\"\"\n        self.config = config or ExportConfig()\n\n    def export(self, graph: nx.DiGraph, output_path: Path) -&gt; None:\n        \"\"\"Export graph to CSV files (nodes and edges).\n\n        Args:\n            graph: NetworkX directed graph to export.\n            output_path: Directory path where to save CSV files.\n\n        Raises:\n            ValueError: If graph is empty.\n        \"\"\"\n        if not self.validate_graph(graph):\n            raise ValueError(\"Cannot export empty graph\")\n\n        output_path.mkdir(parents=True, exist_ok=True)\n\n        # Export nodes\n        nodes_path = output_path / self.config.CSV_NODE_FILENAME\n        self._export_nodes(graph, nodes_path)\n\n        # Export edges\n        edges_path = output_path / self.config.CSV_EDGE_FILENAME\n        self._export_edges(graph, edges_path)\n\n    def validate_graph(self, graph: nx.DiGraph) -&gt; bool:\n        \"\"\"Validate that graph is not empty.\n\n        Args:\n            graph: NetworkX directed graph.\n\n        Returns:\n            True if graph has nodes.\n        \"\"\"\n        num_nodes = cast(int, graph.number_of_nodes())\n        return num_nodes &gt; 0\n\n    def _export_nodes(self, graph: nx.DiGraph, path: Path) -&gt; None:\n        \"\"\"Export nodes to CSV.\n\n        Args:\n            graph: NetworkX directed graph.\n            path: Path to save nodes CSV.\n        \"\"\"\n        nodes_data = []\n\n        for node_id, data in graph.nodes(data=True):\n            node_dict = {\"id\": node_id, **data}\n            nodes_data.append(node_dict)\n\n        nodes_df = pd.DataFrame(nodes_data)\n        nodes_df.to_csv(path, index=False, encoding=self.config.CSV_ENCODING)\n\n    def _export_edges(self, graph: nx.DiGraph, path: Path) -&gt; None:\n        \"\"\"Export edges to CSV.\n\n        Args:\n            graph: NetworkX directed graph.\n            path: Path to save edges CSV.\n        \"\"\"\n        edges_data = []\n\n        for source, target, data in graph.edges(data=True):\n            edge_dict = {\"source\": source, \"target\": target, **data}\n            edges_data.append(edge_dict)\n\n        edges_df = pd.DataFrame(edges_data)\n        edges_df.to_csv(path, index=False, encoding=self.config.CSV_ENCODING)\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.csv_exporter.CSVExporter.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize CSV exporter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ExportConfig | None</code> <p>Export configuration. Uses defaults if None.</p> <code>None</code> Source code in <code>docling_graph/core/exporters/csv_exporter.py</code> <pre><code>def __init__(self, config: ExportConfig | None = None) -&gt; None:\n    \"\"\"Initialize CSV exporter.\n\n    Args:\n        config: Export configuration. Uses defaults if None.\n    \"\"\"\n    self.config = config or ExportConfig()\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.csv_exporter.CSVExporter.export","title":"<code>export(graph, output_path)</code>","text":"<p>Export graph to CSV files (nodes and edges).</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>NetworkX directed graph to export.</p> required <code>output_path</code> <code>Path</code> <p>Directory path where to save CSV files.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If graph is empty.</p> Source code in <code>docling_graph/core/exporters/csv_exporter.py</code> <pre><code>def export(self, graph: nx.DiGraph, output_path: Path) -&gt; None:\n    \"\"\"Export graph to CSV files (nodes and edges).\n\n    Args:\n        graph: NetworkX directed graph to export.\n        output_path: Directory path where to save CSV files.\n\n    Raises:\n        ValueError: If graph is empty.\n    \"\"\"\n    if not self.validate_graph(graph):\n        raise ValueError(\"Cannot export empty graph\")\n\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    # Export nodes\n    nodes_path = output_path / self.config.CSV_NODE_FILENAME\n    self._export_nodes(graph, nodes_path)\n\n    # Export edges\n    edges_path = output_path / self.config.CSV_EDGE_FILENAME\n    self._export_edges(graph, edges_path)\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.csv_exporter.CSVExporter.validate_graph","title":"<code>validate_graph(graph)</code>","text":"<p>Validate that graph is not empty.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>NetworkX directed graph.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if graph has nodes.</p> Source code in <code>docling_graph/core/exporters/csv_exporter.py</code> <pre><code>def validate_graph(self, graph: nx.DiGraph) -&gt; bool:\n    \"\"\"Validate that graph is not empty.\n\n    Args:\n        graph: NetworkX directed graph.\n\n    Returns:\n        True if graph has nodes.\n    \"\"\"\n    num_nodes = cast(int, graph.number_of_nodes())\n    return num_nodes &gt; 0\n</code></pre>"},{"location":"api/exporters/#cypherexporter","title":"CypherExporter","text":""},{"location":"api/exporters/#docling_graph.core.exporters.cypher_exporter.CypherExporter","title":"<code>docling_graph.core.exporters.cypher_exporter.CypherExporter</code>","text":"<p>Export graph to Cypher script for Neo4j.</p> Source code in <code>docling_graph/core/exporters/cypher_exporter.py</code> <pre><code>class CypherExporter:\n    \"\"\"Export graph to Cypher script for Neo4j.\"\"\"\n\n    def __init__(self, config: ExportConfig | None = None) -&gt; None:\n        \"\"\"Initialize Cypher exporter.\n\n        Args:\n            config: Export configuration. Uses defaults if None.\n        \"\"\"\n        self.config = config or ExportConfig()\n\n    def export(self, graph: nx.DiGraph, output_path: Path) -&gt; None:\n        \"\"\"Export graph to Cypher script.\n\n        Args:\n            graph: NetworkX directed graph to export.\n            output_path: File path where to save Cypher script.\n\n        Raises:\n            ValueError: If graph is empty.\n        \"\"\"\n        if not self.validate_graph(graph):\n            raise ValueError(\"Cannot export empty graph\")\n\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(output_path, \"w\", encoding=self.config.CYPHER_ENCODING) as f:\n            # Write header\n            f.write(\"// Cypher script generated by docling-graph\\n\")\n            f.write(\"// Import this into Neo4j\\n\\n\")\n\n            # Write node creation statements\n            f.write(\"// --- Create Nodes ---\\n\")\n            self._write_nodes(graph, f)\n\n            # Write relationship creation statements\n            f.write(\"\\n// --- Create Relationships ---\\n\")\n            self._write_relationships(graph, f)\n\n    def validate_graph(self, graph: nx.DiGraph) -&gt; bool:\n        \"\"\"Validate that graph is not empty.\n\n        Args:\n            graph: NetworkX directed graph.\n\n        Returns:\n            True if graph has nodes.\n        \"\"\"\n        # number_of_nodes() is not typed in stubs; cast to int for mypy\n        num_nodes = cast(int, graph.number_of_nodes())\n        return num_nodes &gt; 0\n\n    @staticmethod\n    def _escape_cypher_string(value: Any) -&gt; str:\n        \"\"\"Escape strings for use in Cypher queries.\n\n        Args:\n            value: Value to escape.\n\n        Returns:\n            Escaped string safe for Cypher.\n        \"\"\"\n        # Ensure we operate on a string for proper typing\n        val_str: str = value if isinstance(value, str) else str(value)\n\n        # Escape backslashes, quotes, and newlines\n        return (\n            val_str.replace(\"\\\\\", \"\\\\\\\\\")\n            .replace(\"'\", \"\\\\'\")\n            .replace('\"', '\\\\\"')\n            .replace(\"\\n\", \"\\\\n\")\n        )\n\n    @staticmethod\n    def _sanitize_identifier(identifier: str) -&gt; str:\n        \"\"\"Sanitize identifier for use in Cypher.\n\n        Args:\n            identifier: Identifier to sanitize.\n\n        Returns:\n            Sanitized identifier safe for Cypher.\n        \"\"\"\n        # Replace non-alphanumeric characters with underscore\n        sanitized = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", str(identifier))\n        # Ensure it doesn't start with a number\n        if sanitized and sanitized[0].isdigit():\n            sanitized = \"n_\" + sanitized\n        return sanitized or \"node\"\n\n    def _write_nodes(self, graph: nx.DiGraph, file: TextIO) -&gt; None:\n        \"\"\"Write node creation statements.\n\n        Args:\n            graph: NetworkX directed graph.\n            file: File object to write to.\n        \"\"\"\n        node_vars: Dict[str, str] = {}\n\n        for i, (node_id, data) in enumerate(graph.nodes(data=True)):\n            # Create sanitized variable name\n            base_var = self._sanitize_identifier(node_id)\n            node_var = f\"{base_var}_{i}\"\n            node_vars[node_id] = node_var\n\n            # Get node label\n            label = data.get(\"label\", \"Node\")\n\n            # Build properties\n            props = []\n            for key, value in data.items():\n                if key != \"label\" and value is not None:\n                    escaped_value = self._escape_cypher_string(value)\n                    props.append(f'{key}: \"{escaped_value}\"')\n\n            props_str = \", \".join(props)\n\n            # Write CREATE statement\n            if props_str:\n                file.write(f\"CREATE ({node_var}:{label} {{{props_str}}})\\n\")\n            else:\n                file.write(f\"CREATE ({node_var}:{label})\\n\")\n\n        # Store node vars for relationship creation\n        self._node_vars = node_vars\n\n    def _write_relationships(self, graph: nx.DiGraph, file: TextIO) -&gt; None:\n        \"\"\"Write relationship creation statements.\n\n        Args:\n            graph: NetworkX directed graph.\n            file: File object to write to.\n        \"\"\"\n        for source, target, data in graph.edges(data=True):\n            source_var = self._node_vars.get(source)\n            target_var = self._node_vars.get(target)\n\n            if not source_var or not target_var:\n                continue\n\n            # Get relationship type\n            rel_type = data.get(\"label\", \"RELATED_TO\").upper()\n            rel_type = self._sanitize_identifier(rel_type)\n\n            # Build properties\n            props = []\n            for key, value in data.items():\n                if key != \"label\" and value is not None:\n                    escaped_value = self._escape_cypher_string(value)\n                    props.append(f'{key}: \"{escaped_value}\"')\n\n            # Write MATCH and CREATE statements\n            file.write(f\"MATCH ({source_var}), ({target_var})\\n\")\n            if props:\n                props_str = \", \".join(props)\n                file.write(f\"CREATE ({source_var})-[:{rel_type} {{{props_str}}}]-&gt;({target_var})\\n\")\n            else:\n                file.write(f\"CREATE ({source_var})-[:{rel_type}]-&gt;({target_var})\\n\")\n            file.write(\"\\n\")\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.cypher_exporter.CypherExporter.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize Cypher exporter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ExportConfig | None</code> <p>Export configuration. Uses defaults if None.</p> <code>None</code> Source code in <code>docling_graph/core/exporters/cypher_exporter.py</code> <pre><code>def __init__(self, config: ExportConfig | None = None) -&gt; None:\n    \"\"\"Initialize Cypher exporter.\n\n    Args:\n        config: Export configuration. Uses defaults if None.\n    \"\"\"\n    self.config = config or ExportConfig()\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.cypher_exporter.CypherExporter.export","title":"<code>export(graph, output_path)</code>","text":"<p>Export graph to Cypher script.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>NetworkX directed graph to export.</p> required <code>output_path</code> <code>Path</code> <p>File path where to save Cypher script.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If graph is empty.</p> Source code in <code>docling_graph/core/exporters/cypher_exporter.py</code> <pre><code>def export(self, graph: nx.DiGraph, output_path: Path) -&gt; None:\n    \"\"\"Export graph to Cypher script.\n\n    Args:\n        graph: NetworkX directed graph to export.\n        output_path: File path where to save Cypher script.\n\n    Raises:\n        ValueError: If graph is empty.\n    \"\"\"\n    if not self.validate_graph(graph):\n        raise ValueError(\"Cannot export empty graph\")\n\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(output_path, \"w\", encoding=self.config.CYPHER_ENCODING) as f:\n        # Write header\n        f.write(\"// Cypher script generated by docling-graph\\n\")\n        f.write(\"// Import this into Neo4j\\n\\n\")\n\n        # Write node creation statements\n        f.write(\"// --- Create Nodes ---\\n\")\n        self._write_nodes(graph, f)\n\n        # Write relationship creation statements\n        f.write(\"\\n// --- Create Relationships ---\\n\")\n        self._write_relationships(graph, f)\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.cypher_exporter.CypherExporter.validate_graph","title":"<code>validate_graph(graph)</code>","text":"<p>Validate that graph is not empty.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>NetworkX directed graph.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if graph has nodes.</p> Source code in <code>docling_graph/core/exporters/cypher_exporter.py</code> <pre><code>def validate_graph(self, graph: nx.DiGraph) -&gt; bool:\n    \"\"\"Validate that graph is not empty.\n\n    Args:\n        graph: NetworkX directed graph.\n\n    Returns:\n        True if graph has nodes.\n    \"\"\"\n    # number_of_nodes() is not typed in stubs; cast to int for mypy\n    num_nodes = cast(int, graph.number_of_nodes())\n    return num_nodes &gt; 0\n</code></pre>"},{"location":"api/exporters/#doclingexporter","title":"DoclingExporter","text":""},{"location":"api/exporters/#docling_graph.core.exporters.docling_exporter.DoclingExporter","title":"<code>docling_graph.core.exporters.docling_exporter.DoclingExporter</code>","text":"<p>Export Docling documents and markdown to output directory.</p> Source code in <code>docling_graph/core/exporters/docling_exporter.py</code> <pre><code>class DoclingExporter:\n    \"\"\"Export Docling documents and markdown to output directory.\"\"\"\n\n    def __init__(self, output_dir: Path | None = None) -&gt; None:\n        \"\"\"Initialize Docling exporter.\n\n        Args:\n            output_dir: Directory where outputs will be saved.\n        \"\"\"\n        self.output_dir = output_dir or Path(\"outputs\")\n\n    def export_document(\n        self,\n        document: DoclingDocument,\n        base_name: str,\n        include_json: bool = True,\n        include_markdown: bool = True,\n        per_page: bool = False,\n    ) -&gt; dict[str, str | list[str]]:\n        \"\"\"Export Docling document and markdown.\n\n        Args:\n            document: Docling Document object.\n            base_name: Base name for output files (without extension).\n            include_json: Whether to export document as JSON.\n            include_markdown: Whether to export markdown.\n            per_page: Whether to export per-page markdown files.\n\n        Returns:\n            Dictionary with paths to created files.\n        \"\"\"\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        exported_files: dict[str, str | list[str]] = {}\n\n        # Export document as JSON\n        if include_json:\n            json_path = self.output_dir / f\"{base_name}_docling.json\"\n            self._export_document_json(document, json_path)\n            exported_files[\"document_json\"] = str(json_path)\n            rich_print(f\"[green]\u2192[/green] Saved Docling document to [green]{json_path}[/green]\")\n\n        # Export full markdown\n        if include_markdown:\n            md_path = self.output_dir / f\"{base_name}_markdown.md\"\n            full_markdown = document.export_to_markdown()\n            self._save_text(full_markdown, md_path)\n            exported_files[\"markdown\"] = str(md_path)\n            rich_print(f\"[green]\u2192[/green] Saved full markdown to [green]{md_path}[/green]\")\n\n        # Export per-page markdown\n        if per_page:\n            page_dir = self.output_dir / f\"{base_name}_pages\"\n            page_dir.mkdir(parents=True, exist_ok=True)\n\n            page_files = []\n            for page_no in sorted(document.pages.keys()):\n                page_md = document.export_to_markdown(page_no=page_no)\n                page_path = page_dir / f\"page_{page_no:03d}.md\"\n                self._save_text(page_md, page_path)\n                page_files.append(str(page_path))\n\n            exported_files[\"page_markdowns\"] = page_files\n            rich_print(\n                f\"[green]\u2192[/green] Saved {len(page_files)} page markdown files to [green]{page_dir}[/green]\"\n            )\n\n        return exported_files\n\n    def _export_document_json(self, document: DoclingDocument, output_path: Path) -&gt; None:\n        \"\"\"Export Docling document to JSON format.\n\n        Args:\n            document: Docling Document object.\n            output_path: Path where to save JSON file.\n        \"\"\"\n        # Export using Docling's native export method\n        doc_dict = document.export_to_dict()\n\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(doc_dict, f, indent=2, ensure_ascii=False, default=str)\n\n    def _save_text(self, content: str, output_path: Path) -&gt; None:\n        \"\"\"Save text content to file.\n\n        Args:\n            content: Text content to save.\n            output_path: Path where to save file.\n        \"\"\"\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(content)\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.docling_exporter.DoclingExporter.__init__","title":"<code>__init__(output_dir=None)</code>","text":"<p>Initialize Docling exporter.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path | None</code> <p>Directory where outputs will be saved.</p> <code>None</code> Source code in <code>docling_graph/core/exporters/docling_exporter.py</code> <pre><code>def __init__(self, output_dir: Path | None = None) -&gt; None:\n    \"\"\"Initialize Docling exporter.\n\n    Args:\n        output_dir: Directory where outputs will be saved.\n    \"\"\"\n    self.output_dir = output_dir or Path(\"outputs\")\n</code></pre>"},{"location":"api/exporters/#docling_graph.core.exporters.docling_exporter.DoclingExporter.export_document","title":"<code>export_document(document, base_name, include_json=True, include_markdown=True, per_page=False)</code>","text":"<p>Export Docling document and markdown.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>DoclingDocument</code> <p>Docling Document object.</p> required <code>base_name</code> <code>str</code> <p>Base name for output files (without extension).</p> required <code>include_json</code> <code>bool</code> <p>Whether to export document as JSON.</p> <code>True</code> <code>include_markdown</code> <code>bool</code> <p>Whether to export markdown.</p> <code>True</code> <code>per_page</code> <code>bool</code> <p>Whether to export per-page markdown files.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, str | list[str]]</code> <p>Dictionary with paths to created files.</p> Source code in <code>docling_graph/core/exporters/docling_exporter.py</code> <pre><code>def export_document(\n    self,\n    document: DoclingDocument,\n    base_name: str,\n    include_json: bool = True,\n    include_markdown: bool = True,\n    per_page: bool = False,\n) -&gt; dict[str, str | list[str]]:\n    \"\"\"Export Docling document and markdown.\n\n    Args:\n        document: Docling Document object.\n        base_name: Base name for output files (without extension).\n        include_json: Whether to export document as JSON.\n        include_markdown: Whether to export markdown.\n        per_page: Whether to export per-page markdown files.\n\n    Returns:\n        Dictionary with paths to created files.\n    \"\"\"\n    self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    exported_files: dict[str, str | list[str]] = {}\n\n    # Export document as JSON\n    if include_json:\n        json_path = self.output_dir / f\"{base_name}_docling.json\"\n        self._export_document_json(document, json_path)\n        exported_files[\"document_json\"] = str(json_path)\n        rich_print(f\"[green]\u2192[/green] Saved Docling document to [green]{json_path}[/green]\")\n\n    # Export full markdown\n    if include_markdown:\n        md_path = self.output_dir / f\"{base_name}_markdown.md\"\n        full_markdown = document.export_to_markdown()\n        self._save_text(full_markdown, md_path)\n        exported_files[\"markdown\"] = str(md_path)\n        rich_print(f\"[green]\u2192[/green] Saved full markdown to [green]{md_path}[/green]\")\n\n    # Export per-page markdown\n    if per_page:\n        page_dir = self.output_dir / f\"{base_name}_pages\"\n        page_dir.mkdir(parents=True, exist_ok=True)\n\n        page_files = []\n        for page_no in sorted(document.pages.keys()):\n            page_md = document.export_to_markdown(page_no=page_no)\n            page_path = page_dir / f\"page_{page_no:03d}.md\"\n            self._save_text(page_md, page_path)\n            page_files.append(str(page_path))\n\n        exported_files[\"page_markdowns\"] = page_files\n        rich_print(\n            f\"[green]\u2192[/green] Saved {len(page_files)} page markdown files to [green]{page_dir}[/green]\"\n        )\n\n    return exported_files\n</code></pre>"},{"location":"api/exporters/#see-also","title":"See Also","text":"<ul> <li>Converters API - Graph conversion</li> <li>Examples - Export examples</li> </ul>"},{"location":"api/extractors/","title":"Extractors API","text":"<p>Document extraction and processing utilities.</p>"},{"location":"api/extractors/#extractorbase","title":"ExtractorBase","text":""},{"location":"api/extractors/#docling_graph.core.extractors.extractor_base.BaseExtractor","title":"<code>docling_graph.core.extractors.extractor_base.BaseExtractor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all extraction strategies.</p> Source code in <code>docling_graph/core/extractors/extractor_base.py</code> <pre><code>class BaseExtractor(ABC):\n    \"\"\"Abstract base class for all extraction strategies.\"\"\"\n\n    @abstractmethod\n    def extract(self, source: str, template: Type[BaseModel]) -&gt; List[BaseModel]:\n        \"\"\"\n        Extracts structured data from a source document based on a Pydantic template.\n\n        Args:\n            source (str): The file path to the document.\n            template (Type[BaseModel]): The Pydantic model to extract into.\n\n        Returns:\n            List[BaseModel]: A list of Pydantic model instances.\n                - For \"One-to-One\", this list may contain N models (one per page).\n                - For \"Many-to-One\", this list will contain 1 model.\n        \"\"\"\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.extractor_base.BaseExtractor.extract","title":"<code>extract(source, template)</code>  <code>abstractmethod</code>","text":"<p>Extracts structured data from a source document based on a Pydantic template.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The file path to the document.</p> required <code>template</code> <code>Type[BaseModel]</code> <p>The Pydantic model to extract into.</p> required <p>Returns:</p> Type Description <code>List[BaseModel]</code> <p>List[BaseModel]: A list of Pydantic model instances. - For \"One-to-One\", this list may contain N models (one per page). - For \"Many-to-One\", this list will contain 1 model.</p> Source code in <code>docling_graph/core/extractors/extractor_base.py</code> <pre><code>@abstractmethod\ndef extract(self, source: str, template: Type[BaseModel]) -&gt; List[BaseModel]:\n    \"\"\"\n    Extracts structured data from a source document based on a Pydantic template.\n\n    Args:\n        source (str): The file path to the document.\n        template (Type[BaseModel]): The Pydantic model to extract into.\n\n    Returns:\n        List[BaseModel]: A list of Pydantic model instances.\n            - For \"One-to-One\", this list may contain N models (one per page).\n            - For \"Many-to-One\", this list will contain 1 model.\n    \"\"\"\n</code></pre>"},{"location":"api/extractors/#documentprocessor","title":"DocumentProcessor","text":""},{"location":"api/extractors/#docling_graph.core.extractors.document_processor.DocumentProcessor","title":"<code>docling_graph.core.extractors.document_processor.DocumentProcessor</code>","text":"<p>Handles document conversion to Markdown format and chunking.</p> Source code in <code>docling_graph/core/extractors/document_processor.py</code> <pre><code>class DocumentProcessor:\n    \"\"\"Handles document conversion to Markdown format and chunking.\"\"\"\n\n    def __init__(\n        self,\n        docling_config: str = \"ocr\",\n        chunker_config: dict | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize document processor with specified pipeline.\n\n        Args:\n            docling_config (str): Either \"vision\" or \"ocr\" by default.\n                vision: Uses VLM pipeline for complex layouts.\n                ocr: Uses classic OCR pipeline for standard documents.\n            chunker_config (dict): Configuration for DocumentChunker.\n                Example: {\n                    \"tokenizer_name\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n                    \"max_tokens\": 4096,\n                    \"merge_peers\": True\n                }\n                Or use provider shortcut:\n                {\n                    \"provider\": \"mistral\",\n                    \"merge_peers\": True\n                }\n        \"\"\"\n        self.docling_config = docling_config\n        self._last_document: DoclingDocument | None = None\n        self._last_source: str | None = None\n\n        # Initialize chunker if config provided\n        self.chunker = None\n        if chunker_config:\n            self.chunker = DocumentChunker(**chunker_config)\n            rich_print(\n                f\"[blue][DocumentProcessor][/blue] Initialized chunker with \"\n                f\"configuration: {self.chunker.get_config_summary()}\"\n            )\n\n        if docling_config == \"vision\":\n            # VLM Pipeline - Best for complex layouts and images\n            self.converter = DocumentConverter(\n                format_options={\n                    InputFormat.PDF: PdfFormatOption(\n                        pipeline_cls=VlmPipeline,\n                    ),\n                    InputFormat.IMAGE: PdfFormatOption(\n                        pipeline_cls=VlmPipeline,\n                    ),\n                }\n            )\n            rich_print(\n                \"[blue][DocumentProcessor][/blue] Initialized with [magenta]VLM pipeline[/magenta]\"\n            )\n        else:\n            # Default Pipeline - Most accurate with OCR for standard documents\n            pipeline_options = PdfPipelineOptions()\n            pipeline_options.do_ocr = True\n            pipeline_options.do_table_structure = True\n            pipeline_options.table_structure_options.do_cell_matching = True\n            pipeline_options.ocr_options.lang = [\"en\", \"fr\"]\n            pipeline_options.accelerator_options = AcceleratorOptions(\n                num_threads=4, device=AcceleratorDevice.AUTO\n            )\n\n            self.converter = DocumentConverter(\n                format_options={\n                    InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options),\n                    InputFormat.IMAGE: PdfFormatOption(pipeline_options=pipeline_options),\n                }\n            )\n            rich_print(\n                \"[blue][DocumentProcessor][/blue] Initialized with [green]Classic OCR pipeline[/green] (English, French)\"\n            )\n\n    def convert_to_docling_doc(self, source: str) -&gt; DoclingDocument:\n        \"\"\"\n        Converts a document to Docling's Document format.\n\n        Args:\n            source (str): Path to the source document.\n\n        Returns:\n            Document: Docling document object.\n        \"\"\"\n        rich_print(\n            f\"[blue][DocumentProcessor][/blue] Converting document: [yellow]{source}[/yellow]\"\n        )\n        result = self.converter.convert(source)\n\n        # Cache the document and source for potential reuse\n        self._last_document = result.document\n        self._last_source = source\n\n        rich_print(\n            f\"[blue][DocumentProcessor][/blue] Converted [cyan]{result.document.num_pages()}[/cyan] pages\"\n        )\n        return result.document\n\n    @property\n    def last_document(self) -&gt; DoclingDocument | None:\n        \"\"\"\n        Get the last converted DoclingDocument.\n\n        Returns:\n            The last converted document, or None if no document has been converted yet.\n        \"\"\"\n        return self._last_document\n\n    @property\n    def last_source(self) -&gt; str | None:\n        \"\"\"\n        Get the source path of the last converted document.\n\n        Returns:\n            The source path of the last converted document, or None.\n        \"\"\"\n        return self._last_source\n\n    @overload\n    def extract_chunks(\n        self, document: DoclingDocument, with_stats: Literal[True]\n    ) -&gt; tuple[List[str], dict]: ...\n\n    @overload\n    def extract_chunks(\n        self, document: DoclingDocument, with_stats: Literal[False] = False\n    ) -&gt; List[str]: ...\n\n    def extract_chunks(self, document: DoclingDocument, with_stats: bool = False) -&gt; Any:\n        \"\"\"\n        Extract structure-aware chunks from document using HybridChunker.\n\n        This replaces naive text splitting with semantic chunking that preserves:\n        - Tables\n        - Lists\n        - Section hierarchies\n        - Semantic boundaries\n\n        Args:\n            document: DoclingDocument from convert_to_docling_doc()\n            with_stats: If True, return (chunks, stats). If False, return just chunks.\n\n        Returns:\n            List of contextualized text chunks (or tuple with stats if with_stats=True)\n        \"\"\"\n        if not self.chunker:\n            raise ValueError(\n                \"Chunker not initialized. Pass chunker_config to __init__() to enable chunking.\"\n            )\n\n        if with_stats:\n            chunks, stats = self.chunker.chunk_document_with_stats(document)\n            rich_print(\n                f\"[blue][DocumentProcessor][/blue] Created [cyan]{stats['total_chunks']}[/cyan] chunks \"\n                f\"(avg: {stats['avg_tokens']:.0f} tokens, max: {stats['max_tokens_in_chunk']} tokens)\"\n            )\n            return chunks, stats\n        else:\n            chunks = self.chunker.chunk_document(document)\n            rich_print(\n                f\"[blue][DocumentProcessor][/blue] Created [cyan]{len(chunks)}[/cyan] \"\n                \"structure-aware chunks\"\n            )\n            return chunks\n\n    def extract_page_markdowns(self, document: DoclingDocument) -&gt; List[str]:\n        \"\"\"\n        Extracts Markdown content for each page.\n\n        Args:\n            document (Document): Docling document object.\n\n        Returns:\n            List[str]: List of Markdown strings, one per page.\n        \"\"\"\n        page_markdowns = []\n        for page_no in sorted(document.pages.keys()):\n            md = document.export_to_markdown(page_no=page_no)\n            page_markdowns.append(md)\n\n        rich_print(\n            f\"[blue][DocumentProcessor][/blue] Extracted Markdown for [cyan]{len(page_markdowns)}[/cyan] pages\"\n        )\n        return page_markdowns\n\n    def process_document(self, source: str) -&gt; List[str]:\n        \"\"\"High-level helper to get per-page markdowns from a source file.\n\n        This wraps conversion and page extraction into a single call, which\n        simplifies strategy code and matches the interface commonly mocked in tests.\n\n        Args:\n            source: Path to the source document.\n\n        Returns:\n            List of Markdown strings, one per page.\n        \"\"\"\n        rich_print(\"[blue][DocumentProcessor][/blue] Processing document into per-page markdowns\")\n        document = self.convert_to_docling_doc(source)\n        return self.extract_page_markdowns(document)\n\n    def process_document_with_chunking(self, source: str) -&gt; List[str]:\n        \"\"\"\n        Process document with structure-aware chunking instead of page-by-page.\n\n        This is the recommended approach for LLM extraction as it:\n        - Preserves tables and lists\n        - Respects semantic boundaries\n        - Optimizes for context window usage\n\n        Args:\n            source: Path to the source document.\n\n        Returns:\n            List of structure-aware text chunks\n        \"\"\"\n        rich_print(\n            \"[blue][DocumentProcessor][/blue] Processing document with structure-aware chunking\"\n        )\n        document = self.convert_to_docling_doc(source)\n        return self.extract_chunks(document)\n\n    def extract_full_markdown(self, document: DoclingDocument) -&gt; str:\n        \"\"\"\n        Extracts the full document as a single Markdown string.\n\n        Args:\n            document (Document): Docling document object.\n\n        Returns:\n            str: Complete document in Markdown format.\n        \"\"\"\n        md = document.export_to_markdown()\n        rich_print(\n            f\"[blue][DocumentProcessor][/blue] Extracted full document Markdown ([cyan]{len(md)}[/cyan] chars)\"\n        )\n        return md\n\n    def cleanup(self) -&gt; None:\n        \"\"\"Clean up document converter resources.\"\"\"\n        try:\n            # Clear cached document\n            self._last_document = None\n            self._last_source = None\n\n            if hasattr(self, \"converter\"):\n                del self.converter\n            gc.collect()\n            rich_print(\"[blue][DocumentProcessor][/blue] [green]Cleaned up resources[/green]\")\n        except Exception as e:\n            rich_print(\n                f\"[blue][DocumentProcessor][/blue] [yellow]Warning during cleanup:[/yellow] {e}\"\n            )\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_processor.DocumentProcessor.last_document","title":"<code>last_document</code>  <code>property</code>","text":"<p>Get the last converted DoclingDocument.</p> <p>Returns:</p> Type Description <code>DoclingDocument | None</code> <p>The last converted document, or None if no document has been converted yet.</p>"},{"location":"api/extractors/#docling_graph.core.extractors.document_processor.DocumentProcessor.last_source","title":"<code>last_source</code>  <code>property</code>","text":"<p>Get the source path of the last converted document.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>The source path of the last converted document, or None.</p>"},{"location":"api/extractors/#docling_graph.core.extractors.document_processor.DocumentProcessor.__init__","title":"<code>__init__(docling_config='ocr', chunker_config=None)</code>","text":"<p>Initialize document processor with specified pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>docling_config</code> <code>str</code> <p>Either \"vision\" or \"ocr\" by default. vision: Uses VLM pipeline for complex layouts. ocr: Uses classic OCR pipeline for standard documents.</p> <code>'ocr'</code> <code>chunker_config</code> <code>dict</code> <p>Configuration for DocumentChunker. Example: {     \"tokenizer_name\": \"mistralai/Mistral-7B-Instruct-v0.2\",     \"max_tokens\": 4096,     \"merge_peers\": True } Or use provider shortcut: {     \"provider\": \"mistral\",     \"merge_peers\": True }</p> <code>None</code> Source code in <code>docling_graph/core/extractors/document_processor.py</code> <pre><code>def __init__(\n    self,\n    docling_config: str = \"ocr\",\n    chunker_config: dict | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize document processor with specified pipeline.\n\n    Args:\n        docling_config (str): Either \"vision\" or \"ocr\" by default.\n            vision: Uses VLM pipeline for complex layouts.\n            ocr: Uses classic OCR pipeline for standard documents.\n        chunker_config (dict): Configuration for DocumentChunker.\n            Example: {\n                \"tokenizer_name\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n                \"max_tokens\": 4096,\n                \"merge_peers\": True\n            }\n            Or use provider shortcut:\n            {\n                \"provider\": \"mistral\",\n                \"merge_peers\": True\n            }\n    \"\"\"\n    self.docling_config = docling_config\n    self._last_document: DoclingDocument | None = None\n    self._last_source: str | None = None\n\n    # Initialize chunker if config provided\n    self.chunker = None\n    if chunker_config:\n        self.chunker = DocumentChunker(**chunker_config)\n        rich_print(\n            f\"[blue][DocumentProcessor][/blue] Initialized chunker with \"\n            f\"configuration: {self.chunker.get_config_summary()}\"\n        )\n\n    if docling_config == \"vision\":\n        # VLM Pipeline - Best for complex layouts and images\n        self.converter = DocumentConverter(\n            format_options={\n                InputFormat.PDF: PdfFormatOption(\n                    pipeline_cls=VlmPipeline,\n                ),\n                InputFormat.IMAGE: PdfFormatOption(\n                    pipeline_cls=VlmPipeline,\n                ),\n            }\n        )\n        rich_print(\n            \"[blue][DocumentProcessor][/blue] Initialized with [magenta]VLM pipeline[/magenta]\"\n        )\n    else:\n        # Default Pipeline - Most accurate with OCR for standard documents\n        pipeline_options = PdfPipelineOptions()\n        pipeline_options.do_ocr = True\n        pipeline_options.do_table_structure = True\n        pipeline_options.table_structure_options.do_cell_matching = True\n        pipeline_options.ocr_options.lang = [\"en\", \"fr\"]\n        pipeline_options.accelerator_options = AcceleratorOptions(\n            num_threads=4, device=AcceleratorDevice.AUTO\n        )\n\n        self.converter = DocumentConverter(\n            format_options={\n                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options),\n                InputFormat.IMAGE: PdfFormatOption(pipeline_options=pipeline_options),\n            }\n        )\n        rich_print(\n            \"[blue][DocumentProcessor][/blue] Initialized with [green]Classic OCR pipeline[/green] (English, French)\"\n        )\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_processor.DocumentProcessor.cleanup","title":"<code>cleanup()</code>","text":"<p>Clean up document converter resources.</p> Source code in <code>docling_graph/core/extractors/document_processor.py</code> <pre><code>def cleanup(self) -&gt; None:\n    \"\"\"Clean up document converter resources.\"\"\"\n    try:\n        # Clear cached document\n        self._last_document = None\n        self._last_source = None\n\n        if hasattr(self, \"converter\"):\n            del self.converter\n        gc.collect()\n        rich_print(\"[blue][DocumentProcessor][/blue] [green]Cleaned up resources[/green]\")\n    except Exception as e:\n        rich_print(\n            f\"[blue][DocumentProcessor][/blue] [yellow]Warning during cleanup:[/yellow] {e}\"\n        )\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_processor.DocumentProcessor.convert_to_docling_doc","title":"<code>convert_to_docling_doc(source)</code>","text":"<p>Converts a document to Docling's Document format.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Path to the source document.</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>DoclingDocument</code> <p>Docling document object.</p> Source code in <code>docling_graph/core/extractors/document_processor.py</code> <pre><code>def convert_to_docling_doc(self, source: str) -&gt; DoclingDocument:\n    \"\"\"\n    Converts a document to Docling's Document format.\n\n    Args:\n        source (str): Path to the source document.\n\n    Returns:\n        Document: Docling document object.\n    \"\"\"\n    rich_print(\n        f\"[blue][DocumentProcessor][/blue] Converting document: [yellow]{source}[/yellow]\"\n    )\n    result = self.converter.convert(source)\n\n    # Cache the document and source for potential reuse\n    self._last_document = result.document\n    self._last_source = source\n\n    rich_print(\n        f\"[blue][DocumentProcessor][/blue] Converted [cyan]{result.document.num_pages()}[/cyan] pages\"\n    )\n    return result.document\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_processor.DocumentProcessor.extract_chunks","title":"<code>extract_chunks(document, with_stats=False)</code>","text":"<pre><code>extract_chunks(document: DoclingDocument, with_stats: Literal[True]) -&gt; tuple[List[str], dict]\n</code></pre><pre><code>extract_chunks(document: DoclingDocument, with_stats: Literal[False] = False) -&gt; List[str]\n</code></pre> <p>Extract structure-aware chunks from document using HybridChunker.</p> <p>This replaces naive text splitting with semantic chunking that preserves: - Tables - Lists - Section hierarchies - Semantic boundaries</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>DoclingDocument</code> <p>DoclingDocument from convert_to_docling_doc()</p> required <code>with_stats</code> <code>bool</code> <p>If True, return (chunks, stats). If False, return just chunks.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>List of contextualized text chunks (or tuple with stats if with_stats=True)</p> Source code in <code>docling_graph/core/extractors/document_processor.py</code> <pre><code>def extract_chunks(self, document: DoclingDocument, with_stats: bool = False) -&gt; Any:\n    \"\"\"\n    Extract structure-aware chunks from document using HybridChunker.\n\n    This replaces naive text splitting with semantic chunking that preserves:\n    - Tables\n    - Lists\n    - Section hierarchies\n    - Semantic boundaries\n\n    Args:\n        document: DoclingDocument from convert_to_docling_doc()\n        with_stats: If True, return (chunks, stats). If False, return just chunks.\n\n    Returns:\n        List of contextualized text chunks (or tuple with stats if with_stats=True)\n    \"\"\"\n    if not self.chunker:\n        raise ValueError(\n            \"Chunker not initialized. Pass chunker_config to __init__() to enable chunking.\"\n        )\n\n    if with_stats:\n        chunks, stats = self.chunker.chunk_document_with_stats(document)\n        rich_print(\n            f\"[blue][DocumentProcessor][/blue] Created [cyan]{stats['total_chunks']}[/cyan] chunks \"\n            f\"(avg: {stats['avg_tokens']:.0f} tokens, max: {stats['max_tokens_in_chunk']} tokens)\"\n        )\n        return chunks, stats\n    else:\n        chunks = self.chunker.chunk_document(document)\n        rich_print(\n            f\"[blue][DocumentProcessor][/blue] Created [cyan]{len(chunks)}[/cyan] \"\n            \"structure-aware chunks\"\n        )\n        return chunks\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_processor.DocumentProcessor.extract_full_markdown","title":"<code>extract_full_markdown(document)</code>","text":"<p>Extracts the full document as a single Markdown string.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Docling document object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Complete document in Markdown format.</p> Source code in <code>docling_graph/core/extractors/document_processor.py</code> <pre><code>def extract_full_markdown(self, document: DoclingDocument) -&gt; str:\n    \"\"\"\n    Extracts the full document as a single Markdown string.\n\n    Args:\n        document (Document): Docling document object.\n\n    Returns:\n        str: Complete document in Markdown format.\n    \"\"\"\n    md = document.export_to_markdown()\n    rich_print(\n        f\"[blue][DocumentProcessor][/blue] Extracted full document Markdown ([cyan]{len(md)}[/cyan] chars)\"\n    )\n    return md\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_processor.DocumentProcessor.extract_page_markdowns","title":"<code>extract_page_markdowns(document)</code>","text":"<p>Extracts Markdown content for each page.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Docling document object.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of Markdown strings, one per page.</p> Source code in <code>docling_graph/core/extractors/document_processor.py</code> <pre><code>def extract_page_markdowns(self, document: DoclingDocument) -&gt; List[str]:\n    \"\"\"\n    Extracts Markdown content for each page.\n\n    Args:\n        document (Document): Docling document object.\n\n    Returns:\n        List[str]: List of Markdown strings, one per page.\n    \"\"\"\n    page_markdowns = []\n    for page_no in sorted(document.pages.keys()):\n        md = document.export_to_markdown(page_no=page_no)\n        page_markdowns.append(md)\n\n    rich_print(\n        f\"[blue][DocumentProcessor][/blue] Extracted Markdown for [cyan]{len(page_markdowns)}[/cyan] pages\"\n    )\n    return page_markdowns\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_processor.DocumentProcessor.process_document","title":"<code>process_document(source)</code>","text":"<p>High-level helper to get per-page markdowns from a source file.</p> <p>This wraps conversion and page extraction into a single call, which simplifies strategy code and matches the interface commonly mocked in tests.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Path to the source document.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of Markdown strings, one per page.</p> Source code in <code>docling_graph/core/extractors/document_processor.py</code> <pre><code>def process_document(self, source: str) -&gt; List[str]:\n    \"\"\"High-level helper to get per-page markdowns from a source file.\n\n    This wraps conversion and page extraction into a single call, which\n    simplifies strategy code and matches the interface commonly mocked in tests.\n\n    Args:\n        source: Path to the source document.\n\n    Returns:\n        List of Markdown strings, one per page.\n    \"\"\"\n    rich_print(\"[blue][DocumentProcessor][/blue] Processing document into per-page markdowns\")\n    document = self.convert_to_docling_doc(source)\n    return self.extract_page_markdowns(document)\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_processor.DocumentProcessor.process_document_with_chunking","title":"<code>process_document_with_chunking(source)</code>","text":"<p>Process document with structure-aware chunking instead of page-by-page.</p> <p>This is the recommended approach for LLM extraction as it: - Preserves tables and lists - Respects semantic boundaries - Optimizes for context window usage</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Path to the source document.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of structure-aware text chunks</p> Source code in <code>docling_graph/core/extractors/document_processor.py</code> <pre><code>def process_document_with_chunking(self, source: str) -&gt; List[str]:\n    \"\"\"\n    Process document with structure-aware chunking instead of page-by-page.\n\n    This is the recommended approach for LLM extraction as it:\n    - Preserves tables and lists\n    - Respects semantic boundaries\n    - Optimizes for context window usage\n\n    Args:\n        source: Path to the source document.\n\n    Returns:\n        List of structure-aware text chunks\n    \"\"\"\n    rich_print(\n        \"[blue][DocumentProcessor][/blue] Processing document with structure-aware chunking\"\n    )\n    document = self.convert_to_docling_doc(source)\n    return self.extract_chunks(document)\n</code></pre>"},{"location":"api/extractors/#documentchunker","title":"DocumentChunker","text":""},{"location":"api/extractors/#docling_graph.core.extractors.document_chunker.DocumentChunker","title":"<code>docling_graph.core.extractors.document_chunker.DocumentChunker</code>","text":"<p>Structure-preserving document chunker using Docling's HybridChunker.</p> Source code in <code>docling_graph/core/extractors/document_chunker.py</code> <pre><code>class DocumentChunker:\n    \"\"\"Structure-preserving document chunker using Docling's HybridChunker.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer_name: str | None = None,\n        max_tokens: int | None = None,\n        provider: str | None = None,\n        merge_peers: bool = True,\n        schema_size: int = 0,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the chunker with smart defaults based on provider or custom tokenizer.\n\n        Now uses centralized llm_config.py registry with dynamic adjustment based on schema complexity.\n\n        Args:\n            tokenizer_name: Name of the tokenizer to use\n            max_tokens: Maximum tokens per chunk (if None, calculated from provider)\n            provider: LLM provider name (e.g., \"watsonx\", \"openai\")\n            merge_peers: Whether to merge peer sections in chunking\n            schema_size: Size of Pydantic schema JSON for dynamic chunk sizing\n        \"\"\"\n        self.tokenizer: Union[HuggingFaceTokenizer, OpenAITokenizer]\n\n        # Step 1: Determine tokenizer name\n        if tokenizer_name is None and provider is not None:\n            tokenizer_name = get_tokenizer_for_provider(provider)\n\n        elif tokenizer_name is None:\n            tokenizer_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n        # Step 2: Determine max_tokens (using centralized lookup with schema awareness)\n        if max_tokens is None:\n            if provider is not None:\n                max_tokens = get_recommended_chunk_size(provider, \"\", schema_size)\n            else:\n                max_tokens = 5120\n\n        # Step 3: Initialize tokenizer and chunker\n        if tokenizer_name != \"tiktoken\":\n            hf_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n            self.tokenizer = HuggingFaceTokenizer(\n                tokenizer=hf_tokenizer,\n                max_tokens=max_tokens,\n            )\n        else:\n            # Special handling for OpenAI tiktoken\n            try:\n                import tiktoken\n\n                tt_tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n                self.tokenizer = OpenAITokenizer(\n                    tokenizer=tt_tokenizer,\n                    max_tokens=max_tokens,\n                )\n            except ImportError:\n                rich_print(\n                    \"[yellow][DocumentChunker][/yellow] tiktoken not installed, \"\n                    \"falling back to HuggingFace tokenizer\"\n                )\n                hf_tokenizer = AutoTokenizer.from_pretrained(\n                    \"sentence-transformers/all-MiniLM-L6-v2\"\n                )\n                self.tokenizer = HuggingFaceTokenizer(\n                    tokenizer=hf_tokenizer,\n                    max_tokens=max_tokens,\n                )\n\n        # Step 4: Create HybridChunker instance\n        self.chunker = HybridChunker(\n            tokenizer=self.tokenizer,\n            merge_peers=merge_peers,\n        )\n\n        self.max_tokens = max_tokens\n        self.tokenizer_name = tokenizer_name\n        self.merge_peers = merge_peers\n\n        rich_print(\n            f\"[blue][DocumentChunker][/blue] Initialized with:\\n\"\n            f\" \u2022 Tokenizer: [cyan]{tokenizer_name}[/cyan]\\n\"\n            f\" \u2022 Max tokens/chunk: [yellow]{max_tokens}[/yellow]\\n\"\n            f\" \u2022 Merge peers: {merge_peers}\"\n        )\n\n    @staticmethod\n    def calculate_recommended_max_tokens(\n        context_limit: int,\n        system_prompt_tokens: int = 500,\n        response_buffer_tokens: int = 500,\n    ) -&gt; int:\n        \"\"\"\n        Calculate recommended max_tokens for a given context window.\n\n        Formula:\n        available = context_limit - system_prompt - response_buffer\n        max_tokens = available * 0.8  # Reserve 20% for metadata enrichment\n\n        Args:\n            context_limit: Total context window (e.g., 8000 for Mistral-Large)\n            system_prompt_tokens: Estimated tokens for system prompt (default: 500)\n            response_buffer_tokens: Space reserved for LLM output (default: 500)\n\n        Returns:\n            Recommended max_tokens value for chunker\n        \"\"\"\n        available = context_limit - system_prompt_tokens - response_buffer_tokens\n        recommended = int(available * 0.8)\n        return max(512, recommended)  # Minimum 512 tokens\n\n    def chunk_document(self, document: DoclingDocument) -&gt; List[str]:\n        \"\"\"\n        Chunk a DoclingDocument into structure-aware text chunks.\n\n        Args:\n            document: Parsed DoclingDocument from DocumentConverter\n\n        Returns:\n            List of contextualized text chunks, ready for LLM consumption\n        \"\"\"\n        chunks = []\n\n        # Chunk the document using HybridChunker\n        chunk_iter = self.chunker.chunk(dl_doc=document)\n\n        for chunk in chunk_iter:\n            # Use contextualized text (includes metadata like headers, section captions)\n            # This is essential for LLM extraction to understand chunk context\n            enriched_text = self.chunker.contextualize(chunk=chunk)\n            chunks.append(enriched_text)\n\n        return chunks\n\n    def chunk_document_with_stats(self, document: DoclingDocument) -&gt; tuple[List[str], dict]:\n        \"\"\"\n        Chunk document and return tokenization statistics.\n        Useful for debugging/optimization to understand chunk distribution.\n\n        Args:\n            document: Parsed DoclingDocument\n\n        Returns:\n            Tuple of (chunks, stats) where stats contains:\n            - total_chunks: number of chunks\n            - chunk_tokens: list of token counts per chunk\n            - avg_tokens: average tokens per chunk\n            - max_tokens_in_chunk: maximum tokens in any chunk\n            - total_tokens: sum of all chunk tokens\n        \"\"\"\n        chunks = []\n        chunk_tokens = []\n\n        chunk_iter = self.chunker.chunk(dl_doc=document)\n\n        for chunk in chunk_iter:\n            enriched_text = self.chunker.contextualize(chunk=chunk)\n            chunks.append(enriched_text)\n\n            # Count tokens for this chunk\n            num_tokens = self.tokenizer.count_tokens(enriched_text)\n            chunk_tokens.append(num_tokens)\n\n        stats = {\n            \"total_chunks\": len(chunks),\n            \"chunk_tokens\": chunk_tokens,\n            \"avg_tokens\": sum(chunk_tokens) / len(chunk_tokens) if chunk_tokens else 0,\n            \"max_tokens_in_chunk\": max(chunk_tokens) if chunk_tokens else 0,\n            \"total_tokens\": sum(chunk_tokens),\n        }\n\n        return chunks, stats\n\n    def chunk_text_fallback(self, text: str) -&gt; List[str]:\n        \"\"\"\n        Fallback chunker for raw text when DoclingDocument unavailable.\n\n        This is a simple token-based splitter that respects sentence boundaries.\n        For best results, always use chunk_document() with a DoclingDocument.\n\n        Args:\n            text: Raw text string (e.g., plain Markdown)\n\n        Returns:\n            List of text chunks\n        \"\"\"\n        # Rough heuristic: 1 token \u2248 4 characters for most tokenizers\n        max_chars = self.max_tokens * 4\n\n        if len(text) &lt;= max_chars:\n            return [text]\n\n        chunks = []\n        current_pos = 0\n\n        while current_pos &lt; len(text):\n            end_pos = min(current_pos + max_chars, len(text))\n\n            # Try to break at sentence/semantic boundary\n            if end_pos &lt; len(text):\n                # Priority order for breaking points\n                for delimiter in [\". \", \"! \", \"? \", \"\\n\\n\", \"\\n\"]:\n                    last_break = text.rfind(delimiter, current_pos, end_pos)\n                    if last_break != -1:\n                        end_pos = last_break + len(delimiter)\n                        break\n\n            chunk = text[current_pos:end_pos].strip()\n            if chunk:\n                chunks.append(chunk)\n\n            current_pos = end_pos\n\n        return chunks\n\n    def get_config_summary(self) -&gt; dict:\n        \"\"\"Get current chunker configuration as dictionary.\"\"\"\n        return {\n            \"tokenizer_name\": self.tokenizer_name,\n            \"max_tokens\": self.max_tokens,\n            \"merge_peers\": self.merge_peers,\n            \"tokenizer_class\": self.tokenizer.__class__.__name__,\n        }\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_chunker.DocumentChunker.__init__","title":"<code>__init__(tokenizer_name=None, max_tokens=None, provider=None, merge_peers=True, schema_size=0)</code>","text":"<p>Initialize the chunker with smart defaults based on provider or custom tokenizer.</p> <p>Now uses centralized llm_config.py registry with dynamic adjustment based on schema complexity.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer_name</code> <code>str | None</code> <p>Name of the tokenizer to use</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum tokens per chunk (if None, calculated from provider)</p> <code>None</code> <code>provider</code> <code>str | None</code> <p>LLM provider name (e.g., \"watsonx\", \"openai\")</p> <code>None</code> <code>merge_peers</code> <code>bool</code> <p>Whether to merge peer sections in chunking</p> <code>True</code> <code>schema_size</code> <code>int</code> <p>Size of Pydantic schema JSON for dynamic chunk sizing</p> <code>0</code> Source code in <code>docling_graph/core/extractors/document_chunker.py</code> <pre><code>def __init__(\n    self,\n    tokenizer_name: str | None = None,\n    max_tokens: int | None = None,\n    provider: str | None = None,\n    merge_peers: bool = True,\n    schema_size: int = 0,\n) -&gt; None:\n    \"\"\"\n    Initialize the chunker with smart defaults based on provider or custom tokenizer.\n\n    Now uses centralized llm_config.py registry with dynamic adjustment based on schema complexity.\n\n    Args:\n        tokenizer_name: Name of the tokenizer to use\n        max_tokens: Maximum tokens per chunk (if None, calculated from provider)\n        provider: LLM provider name (e.g., \"watsonx\", \"openai\")\n        merge_peers: Whether to merge peer sections in chunking\n        schema_size: Size of Pydantic schema JSON for dynamic chunk sizing\n    \"\"\"\n    self.tokenizer: Union[HuggingFaceTokenizer, OpenAITokenizer]\n\n    # Step 1: Determine tokenizer name\n    if tokenizer_name is None and provider is not None:\n        tokenizer_name = get_tokenizer_for_provider(provider)\n\n    elif tokenizer_name is None:\n        tokenizer_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n    # Step 2: Determine max_tokens (using centralized lookup with schema awareness)\n    if max_tokens is None:\n        if provider is not None:\n            max_tokens = get_recommended_chunk_size(provider, \"\", schema_size)\n        else:\n            max_tokens = 5120\n\n    # Step 3: Initialize tokenizer and chunker\n    if tokenizer_name != \"tiktoken\":\n        hf_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        self.tokenizer = HuggingFaceTokenizer(\n            tokenizer=hf_tokenizer,\n            max_tokens=max_tokens,\n        )\n    else:\n        # Special handling for OpenAI tiktoken\n        try:\n            import tiktoken\n\n            tt_tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n            self.tokenizer = OpenAITokenizer(\n                tokenizer=tt_tokenizer,\n                max_tokens=max_tokens,\n            )\n        except ImportError:\n            rich_print(\n                \"[yellow][DocumentChunker][/yellow] tiktoken not installed, \"\n                \"falling back to HuggingFace tokenizer\"\n            )\n            hf_tokenizer = AutoTokenizer.from_pretrained(\n                \"sentence-transformers/all-MiniLM-L6-v2\"\n            )\n            self.tokenizer = HuggingFaceTokenizer(\n                tokenizer=hf_tokenizer,\n                max_tokens=max_tokens,\n            )\n\n    # Step 4: Create HybridChunker instance\n    self.chunker = HybridChunker(\n        tokenizer=self.tokenizer,\n        merge_peers=merge_peers,\n    )\n\n    self.max_tokens = max_tokens\n    self.tokenizer_name = tokenizer_name\n    self.merge_peers = merge_peers\n\n    rich_print(\n        f\"[blue][DocumentChunker][/blue] Initialized with:\\n\"\n        f\" \u2022 Tokenizer: [cyan]{tokenizer_name}[/cyan]\\n\"\n        f\" \u2022 Max tokens/chunk: [yellow]{max_tokens}[/yellow]\\n\"\n        f\" \u2022 Merge peers: {merge_peers}\"\n    )\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_chunker.DocumentChunker.calculate_recommended_max_tokens","title":"<code>calculate_recommended_max_tokens(context_limit, system_prompt_tokens=500, response_buffer_tokens=500)</code>  <code>staticmethod</code>","text":"<p>Calculate recommended max_tokens for a given context window.</p> <p>Formula: available = context_limit - system_prompt - response_buffer max_tokens = available * 0.8  # Reserve 20% for metadata enrichment</p> <p>Parameters:</p> Name Type Description Default <code>context_limit</code> <code>int</code> <p>Total context window (e.g., 8000 for Mistral-Large)</p> required <code>system_prompt_tokens</code> <code>int</code> <p>Estimated tokens for system prompt (default: 500)</p> <code>500</code> <code>response_buffer_tokens</code> <code>int</code> <p>Space reserved for LLM output (default: 500)</p> <code>500</code> <p>Returns:</p> Type Description <code>int</code> <p>Recommended max_tokens value for chunker</p> Source code in <code>docling_graph/core/extractors/document_chunker.py</code> <pre><code>@staticmethod\ndef calculate_recommended_max_tokens(\n    context_limit: int,\n    system_prompt_tokens: int = 500,\n    response_buffer_tokens: int = 500,\n) -&gt; int:\n    \"\"\"\n    Calculate recommended max_tokens for a given context window.\n\n    Formula:\n    available = context_limit - system_prompt - response_buffer\n    max_tokens = available * 0.8  # Reserve 20% for metadata enrichment\n\n    Args:\n        context_limit: Total context window (e.g., 8000 for Mistral-Large)\n        system_prompt_tokens: Estimated tokens for system prompt (default: 500)\n        response_buffer_tokens: Space reserved for LLM output (default: 500)\n\n    Returns:\n        Recommended max_tokens value for chunker\n    \"\"\"\n    available = context_limit - system_prompt_tokens - response_buffer_tokens\n    recommended = int(available * 0.8)\n    return max(512, recommended)  # Minimum 512 tokens\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_chunker.DocumentChunker.chunk_document","title":"<code>chunk_document(document)</code>","text":"<p>Chunk a DoclingDocument into structure-aware text chunks.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>DoclingDocument</code> <p>Parsed DoclingDocument from DocumentConverter</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of contextualized text chunks, ready for LLM consumption</p> Source code in <code>docling_graph/core/extractors/document_chunker.py</code> <pre><code>def chunk_document(self, document: DoclingDocument) -&gt; List[str]:\n    \"\"\"\n    Chunk a DoclingDocument into structure-aware text chunks.\n\n    Args:\n        document: Parsed DoclingDocument from DocumentConverter\n\n    Returns:\n        List of contextualized text chunks, ready for LLM consumption\n    \"\"\"\n    chunks = []\n\n    # Chunk the document using HybridChunker\n    chunk_iter = self.chunker.chunk(dl_doc=document)\n\n    for chunk in chunk_iter:\n        # Use contextualized text (includes metadata like headers, section captions)\n        # This is essential for LLM extraction to understand chunk context\n        enriched_text = self.chunker.contextualize(chunk=chunk)\n        chunks.append(enriched_text)\n\n    return chunks\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_chunker.DocumentChunker.chunk_document_with_stats","title":"<code>chunk_document_with_stats(document)</code>","text":"<p>Chunk document and return tokenization statistics. Useful for debugging/optimization to understand chunk distribution.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>DoclingDocument</code> <p>Parsed DoclingDocument</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>Tuple of (chunks, stats) where stats contains:</p> <code>dict</code> <ul> <li>total_chunks: number of chunks</li> </ul> <code>tuple[List[str], dict]</code> <ul> <li>chunk_tokens: list of token counts per chunk</li> </ul> <code>tuple[List[str], dict]</code> <ul> <li>avg_tokens: average tokens per chunk</li> </ul> <code>tuple[List[str], dict]</code> <ul> <li>max_tokens_in_chunk: maximum tokens in any chunk</li> </ul> <code>tuple[List[str], dict]</code> <ul> <li>total_tokens: sum of all chunk tokens</li> </ul> Source code in <code>docling_graph/core/extractors/document_chunker.py</code> <pre><code>def chunk_document_with_stats(self, document: DoclingDocument) -&gt; tuple[List[str], dict]:\n    \"\"\"\n    Chunk document and return tokenization statistics.\n    Useful for debugging/optimization to understand chunk distribution.\n\n    Args:\n        document: Parsed DoclingDocument\n\n    Returns:\n        Tuple of (chunks, stats) where stats contains:\n        - total_chunks: number of chunks\n        - chunk_tokens: list of token counts per chunk\n        - avg_tokens: average tokens per chunk\n        - max_tokens_in_chunk: maximum tokens in any chunk\n        - total_tokens: sum of all chunk tokens\n    \"\"\"\n    chunks = []\n    chunk_tokens = []\n\n    chunk_iter = self.chunker.chunk(dl_doc=document)\n\n    for chunk in chunk_iter:\n        enriched_text = self.chunker.contextualize(chunk=chunk)\n        chunks.append(enriched_text)\n\n        # Count tokens for this chunk\n        num_tokens = self.tokenizer.count_tokens(enriched_text)\n        chunk_tokens.append(num_tokens)\n\n    stats = {\n        \"total_chunks\": len(chunks),\n        \"chunk_tokens\": chunk_tokens,\n        \"avg_tokens\": sum(chunk_tokens) / len(chunk_tokens) if chunk_tokens else 0,\n        \"max_tokens_in_chunk\": max(chunk_tokens) if chunk_tokens else 0,\n        \"total_tokens\": sum(chunk_tokens),\n    }\n\n    return chunks, stats\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_chunker.DocumentChunker.chunk_text_fallback","title":"<code>chunk_text_fallback(text)</code>","text":"<p>Fallback chunker for raw text when DoclingDocument unavailable.</p> <p>This is a simple token-based splitter that respects sentence boundaries. For best results, always use chunk_document() with a DoclingDocument.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Raw text string (e.g., plain Markdown)</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of text chunks</p> Source code in <code>docling_graph/core/extractors/document_chunker.py</code> <pre><code>def chunk_text_fallback(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Fallback chunker for raw text when DoclingDocument unavailable.\n\n    This is a simple token-based splitter that respects sentence boundaries.\n    For best results, always use chunk_document() with a DoclingDocument.\n\n    Args:\n        text: Raw text string (e.g., plain Markdown)\n\n    Returns:\n        List of text chunks\n    \"\"\"\n    # Rough heuristic: 1 token \u2248 4 characters for most tokenizers\n    max_chars = self.max_tokens * 4\n\n    if len(text) &lt;= max_chars:\n        return [text]\n\n    chunks = []\n    current_pos = 0\n\n    while current_pos &lt; len(text):\n        end_pos = min(current_pos + max_chars, len(text))\n\n        # Try to break at sentence/semantic boundary\n        if end_pos &lt; len(text):\n            # Priority order for breaking points\n            for delimiter in [\". \", \"! \", \"? \", \"\\n\\n\", \"\\n\"]:\n                last_break = text.rfind(delimiter, current_pos, end_pos)\n                if last_break != -1:\n                    end_pos = last_break + len(delimiter)\n                    break\n\n        chunk = text[current_pos:end_pos].strip()\n        if chunk:\n            chunks.append(chunk)\n\n        current_pos = end_pos\n\n    return chunks\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.document_chunker.DocumentChunker.get_config_summary","title":"<code>get_config_summary()</code>","text":"<p>Get current chunker configuration as dictionary.</p> Source code in <code>docling_graph/core/extractors/document_chunker.py</code> <pre><code>def get_config_summary(self) -&gt; dict:\n    \"\"\"Get current chunker configuration as dictionary.\"\"\"\n    return {\n        \"tokenizer_name\": self.tokenizer_name,\n        \"max_tokens\": self.max_tokens,\n        \"merge_peers\": self.merge_peers,\n        \"tokenizer_class\": self.tokenizer.__class__.__name__,\n    }\n</code></pre>"},{"location":"api/extractors/#chunkbatcher","title":"ChunkBatcher","text":""},{"location":"api/extractors/#docling_graph.core.extractors.chunk_batcher.ChunkBatcher","title":"<code>docling_graph.core.extractors.chunk_batcher.ChunkBatcher</code>","text":"<p>Intelligently batch chunks to optimize context window usage.</p> Source code in <code>docling_graph/core/extractors/chunk_batcher.py</code> <pre><code>class ChunkBatcher:\n    \"\"\"Intelligently batch chunks to optimize context window usage.\"\"\"\n\n    # Overhead per chunk when batched (metadata, separators, etc.)\n    CHUNK_OVERHEAD_TOKENS = 50\n\n    def __init__(\n        self,\n        context_limit: int,\n        system_prompt_tokens: int = 500,\n        response_buffer_tokens: int = 500,\n        merge_threshold: float = 0.85,\n    ) -&gt; None:\n        \"\"\"\n        Initialize batcher with context constraints.\n\n        Args:\n            context_limit: Total context window (e.g., 3500 for granite-4.0-1b)\n            system_prompt_tokens: Tokens for system prompt (default: 500)\n            response_buffer_tokens: Tokens reserved for response (default: 500)\n            merge_threshold: Merge chunks if batch is &lt;this% of available context\n                (default: 0.85 = 85%, prevents many tiny batches)\n        \"\"\"\n        self.context_limit = context_limit\n        self.system_prompt_tokens = system_prompt_tokens\n        self.response_buffer_tokens = response_buffer_tokens\n        self.merge_threshold = merge_threshold\n\n        # Available tokens for content\n        self.available_tokens = context_limit - system_prompt_tokens - response_buffer_tokens\n\n        rich_print(\n            f\"[blue][ChunkBatcher][/blue] Initialized with:\\n\"\n            f\" \u2022 Context limit: [yellow]{context_limit:,}[/yellow] tokens\\n\"\n            f\" \u2022 Available for content: [cyan]{self.available_tokens:,}[/cyan] tokens\\n\"\n            f\" \u2022 Merge threshold: {merge_threshold * 100:.0f}%\"\n        )\n\n    def batch_chunks(\n        self,\n        chunks: List[str],\n        tokenizer_fn: Callable[[str], int] | None = None,\n    ) -&gt; List[ChunkBatch]:\n        \"\"\"\n        Batch chunks to fit context window efficiently.\n\n        Strategy:\n        1. Group chunks that fit together in context\n        2. Merge undersized batches if below merge_threshold\n        3. Minimize total number of API calls\n\n        Args:\n            chunks: List of chunk texts\n            tokenizer_fn: Optional function to count tokens accurately.\n                If None, uses rough heuristic (tokens \u2248 chars / 4)\n\n        Returns:\n            List of ChunkBatch objects ready for LLM extraction\n        \"\"\"\n        if not chunks:\n            return []\n\n        # Helper: estimate tokens for a string\n        def count_tokens(text: str) -&gt; int:\n            if tokenizer_fn:\n                try:\n                    return tokenizer_fn(text)\n                except Exception:\n                    pass\n            # Fallback: rough heuristic\n            return len(text) // 4\n\n        # Phase 1: Create candidate batches (greedy packing)\n        batches: List[ChunkBatch] = []\n        current_batch_chunks: List[str] = []\n        current_batch_indices: List[int] = []\n        current_tokens = 0\n\n        for chunk_idx, chunk_text in enumerate(chunks):\n            chunk_tokens = count_tokens(chunk_text) + self.CHUNK_OVERHEAD_TOKENS\n\n            # Check if adding this chunk exceeds available context\n            potential_total = current_tokens + chunk_tokens\n\n            if current_batch_chunks and potential_total &gt; self.available_tokens:\n                # Start new batch\n                batches.append(\n                    ChunkBatch(\n                        batch_id=len(batches),\n                        chunks=current_batch_chunks.copy(),\n                        total_tokens=current_tokens,\n                        chunk_indices=current_batch_indices.copy(),\n                    )\n                )\n                current_batch_chunks = [chunk_text]\n                current_batch_indices = [chunk_idx]\n                current_tokens = chunk_tokens\n            else:\n                # Add to current batch\n                current_batch_chunks.append(chunk_text)\n                current_batch_indices.append(chunk_idx)\n                current_tokens = potential_total\n\n        # Add final batch\n        if current_batch_chunks:\n            batches.append(\n                ChunkBatch(\n                    batch_id=len(batches),\n                    chunks=current_batch_chunks,\n                    total_tokens=current_tokens,\n                    chunk_indices=current_batch_indices,\n                )\n            )\n\n        # Phase 2: Merge undersized batches (if below threshold)\n        merged_batches = self._merge_undersized_batches(batches)\n\n        # Log summary\n        self._log_batching_summary(\n            total_chunks=len(chunks),\n            batches=merged_batches,\n            total_tokens=sum(b.total_tokens for b in merged_batches),\n        )\n\n        return merged_batches\n\n    def _merge_undersized_batches(\n        self,\n        batches: List[ChunkBatch],\n    ) -&gt; List[ChunkBatch]:\n        \"\"\"\n        Merge batches that are below merge_threshold of available context.\n\n        This prevents many small API calls and improves context utilization.\n        \"\"\"\n        if len(batches) &lt;= 1:\n            return batches\n\n        threshold_tokens = int(self.available_tokens * self.merge_threshold)\n        merged: List[ChunkBatch] = []\n\n        i = 0\n        while i &lt; len(batches):\n            current = batches[i]\n\n            # If batch is already large enough, keep it\n            if current.total_tokens &gt;= threshold_tokens:\n                merged.append(current)\n                i += 1\n                continue\n\n            # Try to merge with next batch(es)\n            combined_chunks = current.chunks.copy()\n            combined_indices = current.chunk_indices.copy()\n            combined_tokens = current.total_tokens\n\n            j = i + 1\n            while j &lt; len(batches):\n                next_batch = batches[j]\n                potential_total = combined_tokens + next_batch.total_tokens\n\n                # Stop if merging would exceed context\n                if potential_total &gt; self.available_tokens:\n                    break\n\n                # Merge this batch\n                combined_chunks.extend(next_batch.chunks)\n                combined_indices.extend(next_batch.chunk_indices)\n                combined_tokens = potential_total\n                j += 1\n\n            # Create merged batch\n            merged.append(\n                ChunkBatch(\n                    batch_id=len(merged),\n                    chunks=combined_chunks,\n                    total_tokens=combined_tokens,\n                    chunk_indices=combined_indices,\n                )\n            )\n\n            i = j\n\n        return merged\n\n    def _log_batching_summary(\n        self,\n        total_chunks: int,\n        batches: List[ChunkBatch],\n        total_tokens: int,\n    ) -&gt; None:\n        \"\"\"Log batching statistics.\"\"\"\n        reduction = (total_chunks - len(batches)) / max(1, total_chunks) * 100\n        avg_batch_size = sum(b.chunk_count for b in batches) / max(1, len(batches))\n        avg_utilization = (\n            total_tokens / (len(batches) * self.available_tokens) * 100 if batches else 0\n        )\n\n        rich_print(\n            f\"[blue][ChunkBatcher][/blue] Batching summary:\\n\"\n            f\"  \u2022 Total chunks: [cyan]{total_chunks}[/cyan]\\n\"\n            f\"  \u2022 Batches created: [yellow]{len(batches)}[/yellow] ([green]-{reduction:.0f}%[/green] API calls)\\n\"\n            f\"  \u2022 Avg chunks/batch: {avg_batch_size:.1f}\\n\"\n            f\"  \u2022 Context utilization: {avg_utilization:.1f}%\"\n        )\n\n        # Log per-batch details\n        for batch in batches:\n            utilization = batch.total_tokens / self.available_tokens * 100\n            rich_print(\n                f\"  \u2514\u2500 Batch {batch.batch_id}: \"\n                f\"{batch.chunk_count} chunks \"\n                f\"({batch.total_tokens:,} tokens, {utilization:.0f}% utilized)\"\n            )\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.chunk_batcher.ChunkBatcher.__init__","title":"<code>__init__(context_limit, system_prompt_tokens=500, response_buffer_tokens=500, merge_threshold=0.85)</code>","text":"<p>Initialize batcher with context constraints.</p> <p>Parameters:</p> Name Type Description Default <code>context_limit</code> <code>int</code> <p>Total context window (e.g., 3500 for granite-4.0-1b)</p> required <code>system_prompt_tokens</code> <code>int</code> <p>Tokens for system prompt (default: 500)</p> <code>500</code> <code>response_buffer_tokens</code> <code>int</code> <p>Tokens reserved for response (default: 500)</p> <code>500</code> <code>merge_threshold</code> <code>float</code> <p>Merge chunks if batch is &lt;this% of available context (default: 0.85 = 85%, prevents many tiny batches)</p> <code>0.85</code> Source code in <code>docling_graph/core/extractors/chunk_batcher.py</code> <pre><code>def __init__(\n    self,\n    context_limit: int,\n    system_prompt_tokens: int = 500,\n    response_buffer_tokens: int = 500,\n    merge_threshold: float = 0.85,\n) -&gt; None:\n    \"\"\"\n    Initialize batcher with context constraints.\n\n    Args:\n        context_limit: Total context window (e.g., 3500 for granite-4.0-1b)\n        system_prompt_tokens: Tokens for system prompt (default: 500)\n        response_buffer_tokens: Tokens reserved for response (default: 500)\n        merge_threshold: Merge chunks if batch is &lt;this% of available context\n            (default: 0.85 = 85%, prevents many tiny batches)\n    \"\"\"\n    self.context_limit = context_limit\n    self.system_prompt_tokens = system_prompt_tokens\n    self.response_buffer_tokens = response_buffer_tokens\n    self.merge_threshold = merge_threshold\n\n    # Available tokens for content\n    self.available_tokens = context_limit - system_prompt_tokens - response_buffer_tokens\n\n    rich_print(\n        f\"[blue][ChunkBatcher][/blue] Initialized with:\\n\"\n        f\" \u2022 Context limit: [yellow]{context_limit:,}[/yellow] tokens\\n\"\n        f\" \u2022 Available for content: [cyan]{self.available_tokens:,}[/cyan] tokens\\n\"\n        f\" \u2022 Merge threshold: {merge_threshold * 100:.0f}%\"\n    )\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.chunk_batcher.ChunkBatcher.batch_chunks","title":"<code>batch_chunks(chunks, tokenizer_fn=None)</code>","text":"<p>Batch chunks to fit context window efficiently.</p> <p>Strategy: 1. Group chunks that fit together in context 2. Merge undersized batches if below merge_threshold 3. Minimize total number of API calls</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>List[str]</code> <p>List of chunk texts</p> required <code>tokenizer_fn</code> <code>Callable[[str], int] | None</code> <p>Optional function to count tokens accurately. If None, uses rough heuristic (tokens \u2248 chars / 4)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ChunkBatch]</code> <p>List of ChunkBatch objects ready for LLM extraction</p> Source code in <code>docling_graph/core/extractors/chunk_batcher.py</code> <pre><code>def batch_chunks(\n    self,\n    chunks: List[str],\n    tokenizer_fn: Callable[[str], int] | None = None,\n) -&gt; List[ChunkBatch]:\n    \"\"\"\n    Batch chunks to fit context window efficiently.\n\n    Strategy:\n    1. Group chunks that fit together in context\n    2. Merge undersized batches if below merge_threshold\n    3. Minimize total number of API calls\n\n    Args:\n        chunks: List of chunk texts\n        tokenizer_fn: Optional function to count tokens accurately.\n            If None, uses rough heuristic (tokens \u2248 chars / 4)\n\n    Returns:\n        List of ChunkBatch objects ready for LLM extraction\n    \"\"\"\n    if not chunks:\n        return []\n\n    # Helper: estimate tokens for a string\n    def count_tokens(text: str) -&gt; int:\n        if tokenizer_fn:\n            try:\n                return tokenizer_fn(text)\n            except Exception:\n                pass\n        # Fallback: rough heuristic\n        return len(text) // 4\n\n    # Phase 1: Create candidate batches (greedy packing)\n    batches: List[ChunkBatch] = []\n    current_batch_chunks: List[str] = []\n    current_batch_indices: List[int] = []\n    current_tokens = 0\n\n    for chunk_idx, chunk_text in enumerate(chunks):\n        chunk_tokens = count_tokens(chunk_text) + self.CHUNK_OVERHEAD_TOKENS\n\n        # Check if adding this chunk exceeds available context\n        potential_total = current_tokens + chunk_tokens\n\n        if current_batch_chunks and potential_total &gt; self.available_tokens:\n            # Start new batch\n            batches.append(\n                ChunkBatch(\n                    batch_id=len(batches),\n                    chunks=current_batch_chunks.copy(),\n                    total_tokens=current_tokens,\n                    chunk_indices=current_batch_indices.copy(),\n                )\n            )\n            current_batch_chunks = [chunk_text]\n            current_batch_indices = [chunk_idx]\n            current_tokens = chunk_tokens\n        else:\n            # Add to current batch\n            current_batch_chunks.append(chunk_text)\n            current_batch_indices.append(chunk_idx)\n            current_tokens = potential_total\n\n    # Add final batch\n    if current_batch_chunks:\n        batches.append(\n            ChunkBatch(\n                batch_id=len(batches),\n                chunks=current_batch_chunks,\n                total_tokens=current_tokens,\n                chunk_indices=current_batch_indices,\n            )\n        )\n\n    # Phase 2: Merge undersized batches (if below threshold)\n    merged_batches = self._merge_undersized_batches(batches)\n\n    # Log summary\n    self._log_batching_summary(\n        total_chunks=len(chunks),\n        batches=merged_batches,\n        total_tokens=sum(b.total_tokens for b in merged_batches),\n    )\n\n    return merged_batches\n</code></pre>"},{"location":"api/extractors/#extractorfactory","title":"ExtractorFactory","text":""},{"location":"api/extractors/#docling_graph.core.extractors.factory.ExtractorFactory","title":"<code>docling_graph.core.extractors.factory.ExtractorFactory</code>","text":"<p>Factory for creating the right extractor combination.</p> Source code in <code>docling_graph/core/extractors/factory.py</code> <pre><code>class ExtractorFactory:\n    \"\"\"Factory for creating the right extractor combination.\"\"\"\n\n    @staticmethod\n    def create_extractor(\n        processing_mode: Literal[\"one-to-one\", \"many-to-one\"],\n        backend_name: Literal[\"vlm\", \"llm\"],\n        model_name: str | None = None,\n        llm_client: BaseLlmClient | None = None,\n        docling_config: str = \"ocr\",\n        use_chunking: bool = True,\n        llm_consolidation: bool = False,\n    ) -&gt; BaseExtractor:\n        \"\"\"\n        Create an extractor based on configuration.\n\n        Args:\n            processing_mode (str): 'one-to-one' or 'many-to-one'\n            backend_name (str): 'vlm' or 'llm'\n            model_name (str): Model name for VLM (optional)\n            llm_client (BaseLlmClient): LLM client instance (optional)\n            docling_config (str): Docling pipeline configuration ('default' or 'vlm')\n            llm_consolidation (bool): Whether to use LLM consolidation.\n            use_chunking (bool): Whether to use chunking.\n\n        Returns:\n            BaseExtractor: Configured extractor instance.\n        \"\"\"\n        rich_print(\"[blue][ExtractorFactory][/blue] Creating extractor:\")\n        rich_print(f\" \u2022 Mode: [cyan]{processing_mode}[/cyan]\")\n        rich_print(f\" \u2022 Type: [cyan]{backend_name}[/cyan]\")\n        rich_print(f\" \u2022 Docling: [cyan]{docling_config}[/cyan]\")\n        # --- ADDED PRINT STATEMENTS START ---\n        if backend_name == \"llm\":\n            rich_print(f\" \u2022 Consolidation: [cyan]{llm_consolidation}[/cyan]\")\n        rich_print(f\" \u2022 Chunking: [cyan]{use_chunking}[/cyan]\")\n        # --- ADDED PRINT STATEMENTS END ---\n\n        # Create backend instance\n        backend_obj: Backend\n        if backend_name == \"vlm\":\n            if not model_name:\n                raise ValueError(\"VLM requires model_name parameter\")\n            backend_obj = VlmBackend(model_name=model_name)\n        elif backend_name == \"llm\":\n            if not llm_client:\n                raise ValueError(\"LLM requires llm_client parameter\")\n            backend_obj = LlmBackend(llm_client=llm_client)\n        else:\n            raise ValueError(f\"Unknown backend: {backend_name}\")\n\n        # Create strategy with docling_config\n        extractor: BaseExtractor\n\n        if processing_mode == \"one-to-one\":\n            # OneToOneStrategy only takes backend and docling_config\n            # It doesn't use chunking or consolidation args\n            extractor = OneToOneStrategy(\n                backend=backend_obj,\n                docling_config=docling_config,\n            )\n        elif processing_mode == \"many-to-one\":\n            # Build args specifically for ManyToOne\n            strategy_args: dict[str, Any] = {\n                \"backend\": backend_obj,\n                \"docling_config\": docling_config,\n                \"use_chunking\": use_chunking,\n            }\n            if backend_name == \"llm\":\n                strategy_args[\"llm_consolidation\"] = llm_consolidation\n\n            extractor = ManyToOneStrategy(**strategy_args)\n        else:\n            raise ValueError(f\"Unknown processing_mode: {processing_mode}\")\n\n        rich_print(\n            f\"[blue][ExtractorFactory][/blue] Created [green]{extractor.__class__.__name__}[/green]\"\n        )\n        return extractor\n</code></pre>"},{"location":"api/extractors/#docling_graph.core.extractors.factory.ExtractorFactory.create_extractor","title":"<code>create_extractor(processing_mode, backend_name, model_name=None, llm_client=None, docling_config='ocr', use_chunking=True, llm_consolidation=False)</code>  <code>staticmethod</code>","text":"<p>Create an extractor based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>processing_mode</code> <code>str</code> <p>'one-to-one' or 'many-to-one'</p> required <code>backend_name</code> <code>str</code> <p>'vlm' or 'llm'</p> required <code>model_name</code> <code>str</code> <p>Model name for VLM (optional)</p> <code>None</code> <code>llm_client</code> <code>BaseLlmClient</code> <p>LLM client instance (optional)</p> <code>None</code> <code>docling_config</code> <code>str</code> <p>Docling pipeline configuration ('default' or 'vlm')</p> <code>'ocr'</code> <code>llm_consolidation</code> <code>bool</code> <p>Whether to use LLM consolidation.</p> <code>False</code> <code>use_chunking</code> <code>bool</code> <p>Whether to use chunking.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>BaseExtractor</code> <code>BaseExtractor</code> <p>Configured extractor instance.</p> Source code in <code>docling_graph/core/extractors/factory.py</code> <pre><code>@staticmethod\ndef create_extractor(\n    processing_mode: Literal[\"one-to-one\", \"many-to-one\"],\n    backend_name: Literal[\"vlm\", \"llm\"],\n    model_name: str | None = None,\n    llm_client: BaseLlmClient | None = None,\n    docling_config: str = \"ocr\",\n    use_chunking: bool = True,\n    llm_consolidation: bool = False,\n) -&gt; BaseExtractor:\n    \"\"\"\n    Create an extractor based on configuration.\n\n    Args:\n        processing_mode (str): 'one-to-one' or 'many-to-one'\n        backend_name (str): 'vlm' or 'llm'\n        model_name (str): Model name for VLM (optional)\n        llm_client (BaseLlmClient): LLM client instance (optional)\n        docling_config (str): Docling pipeline configuration ('default' or 'vlm')\n        llm_consolidation (bool): Whether to use LLM consolidation.\n        use_chunking (bool): Whether to use chunking.\n\n    Returns:\n        BaseExtractor: Configured extractor instance.\n    \"\"\"\n    rich_print(\"[blue][ExtractorFactory][/blue] Creating extractor:\")\n    rich_print(f\" \u2022 Mode: [cyan]{processing_mode}[/cyan]\")\n    rich_print(f\" \u2022 Type: [cyan]{backend_name}[/cyan]\")\n    rich_print(f\" \u2022 Docling: [cyan]{docling_config}[/cyan]\")\n    # --- ADDED PRINT STATEMENTS START ---\n    if backend_name == \"llm\":\n        rich_print(f\" \u2022 Consolidation: [cyan]{llm_consolidation}[/cyan]\")\n    rich_print(f\" \u2022 Chunking: [cyan]{use_chunking}[/cyan]\")\n    # --- ADDED PRINT STATEMENTS END ---\n\n    # Create backend instance\n    backend_obj: Backend\n    if backend_name == \"vlm\":\n        if not model_name:\n            raise ValueError(\"VLM requires model_name parameter\")\n        backend_obj = VlmBackend(model_name=model_name)\n    elif backend_name == \"llm\":\n        if not llm_client:\n            raise ValueError(\"LLM requires llm_client parameter\")\n        backend_obj = LlmBackend(llm_client=llm_client)\n    else:\n        raise ValueError(f\"Unknown backend: {backend_name}\")\n\n    # Create strategy with docling_config\n    extractor: BaseExtractor\n\n    if processing_mode == \"one-to-one\":\n        # OneToOneStrategy only takes backend and docling_config\n        # It doesn't use chunking or consolidation args\n        extractor = OneToOneStrategy(\n            backend=backend_obj,\n            docling_config=docling_config,\n        )\n    elif processing_mode == \"many-to-one\":\n        # Build args specifically for ManyToOne\n        strategy_args: dict[str, Any] = {\n            \"backend\": backend_obj,\n            \"docling_config\": docling_config,\n            \"use_chunking\": use_chunking,\n        }\n        if backend_name == \"llm\":\n            strategy_args[\"llm_consolidation\"] = llm_consolidation\n\n        extractor = ManyToOneStrategy(**strategy_args)\n    else:\n        raise ValueError(f\"Unknown processing_mode: {processing_mode}\")\n\n    rich_print(\n        f\"[blue][ExtractorFactory][/blue] Created [green]{extractor.__class__.__name__}[/green]\"\n    )\n    return extractor\n</code></pre>"},{"location":"api/extractors/#see-also","title":"See Also","text":"<ul> <li>LLM Clients API - LLM backend integration</li> <li>Configuration Guide - Extraction configuration</li> </ul>"},{"location":"api/llm-clients/","title":"LLM Clients API","text":"<p>LLM client implementations for various providers.</p>"},{"location":"api/llm-clients/#basellmclient","title":"BaseLLMClient","text":""},{"location":"api/llm-clients/#docling_graph.llm_clients.base.BaseLlmClient","title":"<code>docling_graph.llm_clients.base.BaseLlmClient</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all LLM clients (Mistral, Ollama, OpenAI, etc.). Defines a common interface for the ManyToOneExtractor.</p> Source code in <code>docling_graph/llm_clients/base.py</code> <pre><code>class BaseLlmClient(ABC):\n    \"\"\"\n    Abstract base class for all LLM clients (Mistral, Ollama, OpenAI, etc.).\n    Defines a common interface for the ManyToOneExtractor.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, model: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize the client.\n        All client implementations must accept at least a 'model' argument.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_json_response(self, prompt: str | dict[str, str], schema_json: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Executes the LLM call with the given prompt and schema.\n\n        Args:\n            prompt (str | dict[str, str]): The full prompt to send to the model (legacy string or structured dict).\n            schema_json (str): The Pantic schema (for models that support it).\n\n        Returns:\n            Dict[str, Any]: The parsed JSON dictionary from the LLM.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def context_limit(self) -&gt; int:\n        \"\"\"\n        Returns the effective context limit (in tokens) for the model.\n        This should be a conservative number, leaving room for prompts.\n        \"\"\"\n</code></pre>"},{"location":"api/llm-clients/#docling_graph.llm_clients.base.BaseLlmClient.context_limit","title":"<code>context_limit</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the effective context limit (in tokens) for the model. This should be a conservative number, leaving room for prompts.</p>"},{"location":"api/llm-clients/#docling_graph.llm_clients.base.BaseLlmClient.__init__","title":"<code>__init__(model, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Initialize the client. All client implementations must accept at least a 'model' argument.</p> Source code in <code>docling_graph/llm_clients/base.py</code> <pre><code>@abstractmethod\ndef __init__(self, model: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize the client.\n    All client implementations must accept at least a 'model' argument.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/llm-clients/#docling_graph.llm_clients.base.BaseLlmClient.get_json_response","title":"<code>get_json_response(prompt, schema_json)</code>  <code>abstractmethod</code>","text":"<p>Executes the LLM call with the given prompt and schema.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str | dict[str, str]</code> <p>The full prompt to send to the model (legacy string or structured dict).</p> required <code>schema_json</code> <code>str</code> <p>The Pantic schema (for models that support it).</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The parsed JSON dictionary from the LLM.</p> Source code in <code>docling_graph/llm_clients/base.py</code> <pre><code>@abstractmethod\ndef get_json_response(self, prompt: str | dict[str, str], schema_json: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Executes the LLM call with the given prompt and schema.\n\n    Args:\n        prompt (str | dict[str, str]): The full prompt to send to the model (legacy string or structured dict).\n        schema_json (str): The Pantic schema (for models that support it).\n\n    Returns:\n        Dict[str, Any]: The parsed JSON dictionary from the LLM.\n    \"\"\"\n</code></pre>"},{"location":"api/llm-clients/#openaiclient","title":"OpenAIClient","text":""},{"location":"api/llm-clients/#docling_graph.llm_clients.openai.OpenAIClient","title":"<code>docling_graph.llm_clients.openai.OpenAIClient</code>","text":"<p>               Bases: <code>BaseLlmClient</code></p> <p>OpenAI API implementation with proper message structure.</p> Source code in <code>docling_graph/llm_clients/openai.py</code> <pre><code>class OpenAIClient(BaseLlmClient):\n    \"\"\"OpenAI API implementation with proper message structure.\"\"\"\n\n    def __init__(self, model: str) -&gt; None:\n        self.model = model\n        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\n                \"[OpenAIClient] [red]Error:[/red] OPENAI_API_KEY not set. \"\n                \"Please set it in your environment or .env file.\"\n            )\n\n        # Initialize OpenAI client\n        self.client = OpenAI(api_key=self.api_key)\n\n        # Use centralized config registry\n        self._context_limit = get_context_limit(\"openai\", model)\n\n        rich_print(f\"[OpenAIClient] Initialized for [blue]{self.model}[/blue]\")\n\n    def get_json_response(self, prompt: str | dict[str, str], schema_json: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Execute OpenAI chat completion with JSON mode.\n        Official docs: https://platform.openai.com/docs/guides/structured-outputs\n\n        Args:\n            prompt: Either a string (legacy) or dict with 'system' and 'user' keys.\n            schema_json: JSON schema (for reference, not directly used by OpenAI).\n\n        Returns:\n            Parsed JSON response from OpenAI.\n        \"\"\"\n        if TYPE_CHECKING:\n            from openai.types.chat import ChatCompletionMessageParam\n\n            messages: list[ChatCompletionMessageParam]\n\n        if isinstance(prompt, dict):\n            messages = [\n                {\"role\": \"system\", \"content\": prompt[\"system\"]},\n                {\"role\": \"user\", \"content\": prompt[\"user\"]},\n            ]\n        else:\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a helpful assistant that responds in JSON format.\",\n                },\n                {\"role\": \"user\", \"content\": prompt},\n            ]\n\n        try:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                response_format={\"type\": \"json_object\"},\n                temperature=0.1,\n            )\n\n            content = response.choices[0].message.content or \"\"\n\n            try:\n                parsed_json = json.loads(content)\n\n                if not parsed_json or (\n                    isinstance(parsed_json, dict) and not any(parsed_json.values())\n                ):\n                    rich_print(\"[yellow]Warning:[/yellow] OpenAI returned empty or all-null JSON\")\n\n                if isinstance(parsed_json, dict):\n                    return cast(Dict[str, Any], parsed_json)\n                else:\n                    rich_print(\n                        \"[yellow]Warning:[/yellow] Expected a JSON object; got non-dict. \"\n                        \"Returning empty dict.\"\n                    )\n                    return {}\n\n            except json.JSONDecodeError as e:\n                rich_print(f\"[red]Error:[/red] Failed to parse OpenAI response as JSON: {e}\")\n                rich_print(f\"[yellow]Raw response:[/yellow] {content}\")\n                return {}\n\n        except Exception as e:\n            rich_print(f\"[red]Error:[/red] OpenAI API call failed: {type(e).__name__}: {e}\")\n            return {}\n\n    @property\n    def context_limit(self) -&gt; int:\n        return self._context_limit\n</code></pre>"},{"location":"api/llm-clients/#docling_graph.llm_clients.openai.OpenAIClient.get_json_response","title":"<code>get_json_response(prompt, schema_json)</code>","text":"<p>Execute OpenAI chat completion with JSON mode. Official docs: https://platform.openai.com/docs/guides/structured-outputs</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str | dict[str, str]</code> <p>Either a string (legacy) or dict with 'system' and 'user' keys.</p> required <code>schema_json</code> <code>str</code> <p>JSON schema (for reference, not directly used by OpenAI).</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Parsed JSON response from OpenAI.</p> Source code in <code>docling_graph/llm_clients/openai.py</code> <pre><code>def get_json_response(self, prompt: str | dict[str, str], schema_json: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Execute OpenAI chat completion with JSON mode.\n    Official docs: https://platform.openai.com/docs/guides/structured-outputs\n\n    Args:\n        prompt: Either a string (legacy) or dict with 'system' and 'user' keys.\n        schema_json: JSON schema (for reference, not directly used by OpenAI).\n\n    Returns:\n        Parsed JSON response from OpenAI.\n    \"\"\"\n    if TYPE_CHECKING:\n        from openai.types.chat import ChatCompletionMessageParam\n\n        messages: list[ChatCompletionMessageParam]\n\n    if isinstance(prompt, dict):\n        messages = [\n            {\"role\": \"system\", \"content\": prompt[\"system\"]},\n            {\"role\": \"user\", \"content\": prompt[\"user\"]},\n        ]\n    else:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant that responds in JSON format.\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n    try:\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            response_format={\"type\": \"json_object\"},\n            temperature=0.1,\n        )\n\n        content = response.choices[0].message.content or \"\"\n\n        try:\n            parsed_json = json.loads(content)\n\n            if not parsed_json or (\n                isinstance(parsed_json, dict) and not any(parsed_json.values())\n            ):\n                rich_print(\"[yellow]Warning:[/yellow] OpenAI returned empty or all-null JSON\")\n\n            if isinstance(parsed_json, dict):\n                return cast(Dict[str, Any], parsed_json)\n            else:\n                rich_print(\n                    \"[yellow]Warning:[/yellow] Expected a JSON object; got non-dict. \"\n                    \"Returning empty dict.\"\n                )\n                return {}\n\n        except json.JSONDecodeError as e:\n            rich_print(f\"[red]Error:[/red] Failed to parse OpenAI response as JSON: {e}\")\n            rich_print(f\"[yellow]Raw response:[/yellow] {content}\")\n            return {}\n\n    except Exception as e:\n        rich_print(f\"[red]Error:[/red] OpenAI API call failed: {type(e).__name__}: {e}\")\n        return {}\n</code></pre>"},{"location":"api/llm-clients/#mistralclient","title":"MistralClient","text":""},{"location":"api/llm-clients/#docling_graph.llm_clients.mistral.MistralClient","title":"<code>docling_graph.llm_clients.mistral.MistralClient</code>","text":"<p>               Bases: <code>BaseLlmClient</code></p> <p>Mistral API implementation matching official example.</p> Source code in <code>docling_graph/llm_clients/mistral.py</code> <pre><code>class MistralClient(BaseLlmClient):\n    \"\"\"Mistral API implementation matching official example.\"\"\"\n\n    def __init__(self, model: str) -&gt; None:\n        self.model = model\n        self.api_key = os.getenv(\"MISTRAL_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\n                \"[MistralClient] [red]Error:[/red] MISTRAL_API_KEY not set. \"\n                \"Please set it in your environment or .env file.\"\n            )\n\n        # Initialize Mistral client\n        self.client = Mistral(api_key=self.api_key)\n\n        # Use centralized config registry\n        self._context_limit = get_context_limit(\"mistral\", model)\n\n        rich_print(f\"[MistralClient] Initialized for [blue]{self.model}[/blue]\")\n\n    def get_json_response(self, prompt: str | dict[str, str], schema_json: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Execute Mistral chat.complete with proper message structure.\n        Official example: https://docs.mistral.ai/api/endpoint/chat\n\n        Args:\n            prompt: Either a string (legacy) or dict with 'system' and 'user' keys.\n            schema_json: JSON schema (for reference).\n\n        Returns:\n            Parsed JSON response from Mistral.\n        \"\"\"\n        messages: list[dict[str, str]] = []\n\n        if isinstance(prompt, dict):\n            system_content = prompt.get(\"system\", \"\")\n            user_content = prompt.get(\"user\", \"\")\n\n            if not system_content or not user_content:\n                rich_print(\"[yellow]Warning:[/yellow] Empty system or user prompt\")\n                rich_print(f\" System: {bool(system_content)}\")\n                rich_print(f\" User: {bool(user_content)}\")\n\n            if system_content:\n                messages.append({\"role\": \"system\", \"content\": system_content})\n\n            messages.append(\n                {\"role\": \"user\", \"content\": user_content or \"Please provide a JSON response.\"}\n            )\n        else:\n            if not prompt:\n                rich_print(\"[yellow]Warning:[/yellow] Empty prompt string\")\n                prompt = \"Please provide a JSON response.\"\n\n            messages.append({\"role\": \"user\", \"content\": prompt})\n\n        try:\n            res = self.client.chat.complete(\n                model=self.model,\n                messages=cast(Any, messages),\n                response_format={\"type\": \"json_object\"},\n                temperature=0.1,\n            )\n\n            response_content = res.choices[0].message.content\n\n            if not response_content:\n                rich_print(\"[red]Error:[/red] Mistral returned empty content\")\n                return {}\n\n            try:\n                if isinstance(response_content, str):\n                    raw = response_content\n                else:\n                    parts: list[str] = []\n                    for chunk in response_content:\n                        text = getattr(chunk, \"text\", None)\n                        if isinstance(text, str):\n                            parts.append(text)\n                    raw = \"\".join(parts)\n\n                parsed = json.loads(raw)\n\n                if not parsed or (isinstance(parsed, dict) and not any(parsed.values())):\n                    rich_print(\"[yellow]Warning:[/yellow] Mistral returned empty or all-null JSON\")\n\n                if isinstance(parsed, dict):\n                    return cast(Dict[str, Any], parsed)\n                else:\n                    rich_print(\n                        \"[yellow]Warning:[/yellow] Expected a JSON object; got non-dict. \"\n                        \"Returning empty dict.\"\n                    )\n                    return {}\n\n            except json.JSONDecodeError as e:\n                rich_print(f\"[red]Error:[/red] Failed to parse Mistral response as JSON: {e}\")\n                rich_print(f\"[yellow]Raw response:[/yellow] {response_content}\")\n                return {}\n\n        except Exception as e:\n            rich_print(f\"[red]Error:[/red] Mistral API call failed: {type(e).__name__}: {e}\")\n            return {}\n\n    @property\n    def context_limit(self) -&gt; int:\n        return self._context_limit\n</code></pre>"},{"location":"api/llm-clients/#docling_graph.llm_clients.mistral.MistralClient.get_json_response","title":"<code>get_json_response(prompt, schema_json)</code>","text":"<p>Execute Mistral chat.complete with proper message structure. Official example: https://docs.mistral.ai/api/endpoint/chat</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str | dict[str, str]</code> <p>Either a string (legacy) or dict with 'system' and 'user' keys.</p> required <code>schema_json</code> <code>str</code> <p>JSON schema (for reference).</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Parsed JSON response from Mistral.</p> Source code in <code>docling_graph/llm_clients/mistral.py</code> <pre><code>def get_json_response(self, prompt: str | dict[str, str], schema_json: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Execute Mistral chat.complete with proper message structure.\n    Official example: https://docs.mistral.ai/api/endpoint/chat\n\n    Args:\n        prompt: Either a string (legacy) or dict with 'system' and 'user' keys.\n        schema_json: JSON schema (for reference).\n\n    Returns:\n        Parsed JSON response from Mistral.\n    \"\"\"\n    messages: list[dict[str, str]] = []\n\n    if isinstance(prompt, dict):\n        system_content = prompt.get(\"system\", \"\")\n        user_content = prompt.get(\"user\", \"\")\n\n        if not system_content or not user_content:\n            rich_print(\"[yellow]Warning:[/yellow] Empty system or user prompt\")\n            rich_print(f\" System: {bool(system_content)}\")\n            rich_print(f\" User: {bool(user_content)}\")\n\n        if system_content:\n            messages.append({\"role\": \"system\", \"content\": system_content})\n\n        messages.append(\n            {\"role\": \"user\", \"content\": user_content or \"Please provide a JSON response.\"}\n        )\n    else:\n        if not prompt:\n            rich_print(\"[yellow]Warning:[/yellow] Empty prompt string\")\n            prompt = \"Please provide a JSON response.\"\n\n        messages.append({\"role\": \"user\", \"content\": prompt})\n\n    try:\n        res = self.client.chat.complete(\n            model=self.model,\n            messages=cast(Any, messages),\n            response_format={\"type\": \"json_object\"},\n            temperature=0.1,\n        )\n\n        response_content = res.choices[0].message.content\n\n        if not response_content:\n            rich_print(\"[red]Error:[/red] Mistral returned empty content\")\n            return {}\n\n        try:\n            if isinstance(response_content, str):\n                raw = response_content\n            else:\n                parts: list[str] = []\n                for chunk in response_content:\n                    text = getattr(chunk, \"text\", None)\n                    if isinstance(text, str):\n                        parts.append(text)\n                raw = \"\".join(parts)\n\n            parsed = json.loads(raw)\n\n            if not parsed or (isinstance(parsed, dict) and not any(parsed.values())):\n                rich_print(\"[yellow]Warning:[/yellow] Mistral returned empty or all-null JSON\")\n\n            if isinstance(parsed, dict):\n                return cast(Dict[str, Any], parsed)\n            else:\n                rich_print(\n                    \"[yellow]Warning:[/yellow] Expected a JSON object; got non-dict. \"\n                    \"Returning empty dict.\"\n                )\n                return {}\n\n        except json.JSONDecodeError as e:\n            rich_print(f\"[red]Error:[/red] Failed to parse Mistral response as JSON: {e}\")\n            rich_print(f\"[yellow]Raw response:[/yellow] {response_content}\")\n            return {}\n\n    except Exception as e:\n        rich_print(f\"[red]Error:[/red] Mistral API call failed: {type(e).__name__}: {e}\")\n        return {}\n</code></pre>"},{"location":"api/llm-clients/#geminiclient","title":"GeminiClient","text":""},{"location":"api/llm-clients/#docling_graph.llm_clients.gemini.GeminiClient","title":"<code>docling_graph.llm_clients.gemini.GeminiClient</code>","text":"<p>               Bases: <code>BaseLlmClient</code></p> <p>Google Gemini API implementation with proper JSON response format.</p> Source code in <code>docling_graph/llm_clients/gemini.py</code> <pre><code>class GeminiClient(BaseLlmClient):\n    \"\"\"Google Gemini API implementation with proper JSON response format.\"\"\"\n\n    def __init__(self, model: str) -&gt; None:\n        self.model = model\n        self.api_key = os.getenv(\"GEMINI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\n                \"[GeminiClient] [red]Error:[/red] GEMINI_API_KEY not set. \"\n                \"Please set it in your environment or .env file.\"\n            )\n\n        # Initialize Gemini client\n        self.client = genai.Client(api_key=self.api_key)\n\n        # Use centralized config registry\n        self._context_limit = get_context_limit(\"google\", model)\n\n        rich_print(f\"[GeminiClient] Initialized for [blue]{self.model}[/blue]\")\n\n    def get_json_response(self, prompt: str | dict[str, str], schema_json: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Execute Gemini generate_content with JSON response mode.\n\n        Official docs: https://ai.google.dev/gemini-api/docs/structured-output\n\n        Args:\n            prompt: Either a string (legacy) or dict with 'system' and 'user' keys.\n            schema_json: JSON schema (for reference).\n\n        Returns:\n            Parsed JSON response from Gemini.\n        \"\"\"\n        # Handle both legacy string prompts and new dict prompts\n        if isinstance(prompt, dict):\n            # Combine system and user into single content\n            contents = f\"{prompt['system']}\\n\\n{prompt['user']}\"\n        else:\n            contents = prompt\n\n        try:\n            # Configure JSON response mode (official method)\n            config = types.GenerateContentConfig(\n                response_mime_type=\"application/json\",\n                temperature=0.1,  # Low temperature for consistent extraction\n            )\n\n            # Generate content\n            response = self.client.models.generate_content(\n                model=self.model, contents=contents, config=config\n            )\n\n            # Get response text\n            response_text = response.text\n\n            # Parse JSON\n            try:\n                parsed: Any = json.loads(response_text)\n                # Normalize to a dict for return type consistency\n                if isinstance(parsed, dict):\n                    result: Dict[str, Any] = parsed\n                elif isinstance(parsed, list):\n                    result = {\"result\": parsed}\n                else:\n                    result = {\"value\": parsed}\n\n                # Validate it's not empty\n                if not result or (isinstance(result, dict) and not any(result.values())):\n                    rich_print(\"[yellow]Warning:[/yellow] Gemini returned empty or all-null JSON\")\n\n                return result\n\n            except json.JSONDecodeError as e:\n                rich_print(f\"[red]Error:[/red] Failed to parse Gemini response as JSON: {e}\")\n                rich_print(f\"[yellow]Raw response:[/yellow] {response_text}\")\n                return {}\n\n        except Exception as e:\n            rich_print(f\"[red]Error:[/red] Gemini API call failed: {type(e).__name__}: {e}\")\n            return {}\n\n    @property\n    def context_limit(self) -&gt; int:\n        return self._context_limit\n</code></pre>"},{"location":"api/llm-clients/#docling_graph.llm_clients.gemini.GeminiClient.get_json_response","title":"<code>get_json_response(prompt, schema_json)</code>","text":"<p>Execute Gemini generate_content with JSON response mode.</p> <p>Official docs: https://ai.google.dev/gemini-api/docs/structured-output</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str | dict[str, str]</code> <p>Either a string (legacy) or dict with 'system' and 'user' keys.</p> required <code>schema_json</code> <code>str</code> <p>JSON schema (for reference).</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Parsed JSON response from Gemini.</p> Source code in <code>docling_graph/llm_clients/gemini.py</code> <pre><code>def get_json_response(self, prompt: str | dict[str, str], schema_json: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Execute Gemini generate_content with JSON response mode.\n\n    Official docs: https://ai.google.dev/gemini-api/docs/structured-output\n\n    Args:\n        prompt: Either a string (legacy) or dict with 'system' and 'user' keys.\n        schema_json: JSON schema (for reference).\n\n    Returns:\n        Parsed JSON response from Gemini.\n    \"\"\"\n    # Handle both legacy string prompts and new dict prompts\n    if isinstance(prompt, dict):\n        # Combine system and user into single content\n        contents = f\"{prompt['system']}\\n\\n{prompt['user']}\"\n    else:\n        contents = prompt\n\n    try:\n        # Configure JSON response mode (official method)\n        config = types.GenerateContentConfig(\n            response_mime_type=\"application/json\",\n            temperature=0.1,  # Low temperature for consistent extraction\n        )\n\n        # Generate content\n        response = self.client.models.generate_content(\n            model=self.model, contents=contents, config=config\n        )\n\n        # Get response text\n        response_text = response.text\n\n        # Parse JSON\n        try:\n            parsed: Any = json.loads(response_text)\n            # Normalize to a dict for return type consistency\n            if isinstance(parsed, dict):\n                result: Dict[str, Any] = parsed\n            elif isinstance(parsed, list):\n                result = {\"result\": parsed}\n            else:\n                result = {\"value\": parsed}\n\n            # Validate it's not empty\n            if not result or (isinstance(result, dict) and not any(result.values())):\n                rich_print(\"[yellow]Warning:[/yellow] Gemini returned empty or all-null JSON\")\n\n            return result\n\n        except json.JSONDecodeError as e:\n            rich_print(f\"[red]Error:[/red] Failed to parse Gemini response as JSON: {e}\")\n            rich_print(f\"[yellow]Raw response:[/yellow] {response_text}\")\n            return {}\n\n    except Exception as e:\n        rich_print(f\"[red]Error:[/red] Gemini API call failed: {type(e).__name__}: {e}\")\n        return {}\n</code></pre>"},{"location":"api/llm-clients/#watsonxclient","title":"WatsonXClient","text":""},{"location":"api/llm-clients/#docling_graph.llm_clients.watsonx.WatsonxClient","title":"<code>docling_graph.llm_clients.watsonx.WatsonxClient</code>","text":"<p>               Bases: <code>BaseLlmClient</code></p> <p>IBM WatsonX API implementation with proper message structure.</p> Source code in <code>docling_graph/llm_clients/watsonx.py</code> <pre><code>class WatsonxClient(BaseLlmClient):\n    \"\"\"IBM WatsonX API implementation with proper message structure.\"\"\"\n\n    def __init__(self, model: str) -&gt; None:\n        # Check if the required packages are available\n        if _WatsonxLLM is None or _Credentials is None:\n            raise ImportError(\n                \"\\nWatsonX client requires 'ibm-watsonx-ai' package.\\n\"\n                \"Install with: pip install 'docling-graph[watsonx]'\\n\"\n                \"Or: pip install ibm-watsonx-ai\"\n            )\n\n        self.model = model\n        self.api_key = os.getenv(\"WATSONX_API_KEY\")\n        self.project_id = os.getenv(\"WATSONX_PROJECT_ID\")\n        self.url = os.getenv(\"WATSONX_URL\", \"https://us-south.ml.cloud.ibm.com\")\n\n        if not self.api_key:\n            raise ValueError(\n                \"[WatsonxClient] [red]Error:[/red] WATSONX_API_KEY not set. \"\n                \"Please set it in your environment or .env file.\"\n            )\n\n        if not self.project_id:\n            raise ValueError(\n                \"[WatsonxClient] [red]Error:[/red] WATSONX_PROJECT_ID not set. \"\n                \"Please set it in your environment or .env file.\"\n            )\n\n        # Initialize WatsonX credentials\n        credentials = Credentials(url=self.url, api_key=self.api_key)\n\n        # Initialize WatsonX model\n        self.client = WatsonxLLM(\n            model_id=self.model,\n            credentials=credentials,\n            project_id=self.project_id,\n        )\n\n        # Get model configuration from centralized config\n        model_config = get_model_config(\"watsonx\", model)\n        if model_config:\n            self._context_limit = model_config.context_limit\n            self._max_new_tokens = model_config.max_new_tokens\n            rich_print(f\"[WatsonxClient] Initialized for [blue]{self.model}[/blue]\")\n            rich_print(f\"[WatsonxClient] Context: [cyan]{self._context_limit:,}[/cyan] tokens\")\n            rich_print(f\"[WatsonxClient] Max output: [cyan]{self._max_new_tokens:,}[/cyan] tokens\")\n        else:\n            # Fallback for unknown models\n            self._context_limit = 8192\n            self._max_new_tokens = 2048\n            rich_print(\n                f\"[WatsonxClient] [yellow]Warning:[/yellow] Model '{model}' not in config, using defaults\"\n            )\n            rich_print(f\"[WatsonxClient] Context: [cyan]{self._context_limit:,}[/cyan] tokens\")\n            rich_print(f\"[WatsonxClient] Max output: [cyan]{self._max_new_tokens:,}[/cyan] tokens\")\n\n        rich_print(f\"[WatsonxClient] Using endpoint: [cyan]{self.url}[/cyan]\")\n\n    def get_json_response(self, prompt: str | dict, schema_json: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Execute WatsonX chat completion with JSON mode.\n\n        Official docs: https://ibm.github.io/watsonx-ai-python-sdk/fm_chat.html\n\n        Args:\n            prompt: Either a string (legacy) or dict with 'system' and 'user' keys.\n            schema_json: JSON schema (for reference, not directly used by WatsonX).\n\n        Returns:\n            Parsed JSON response from WatsonX.\n        \"\"\"\n        # Build the prompt text\n        if isinstance(prompt, dict):\n            # Combine system and user messages\n            system_content = prompt.get(\"system\", \"\")\n            user_content = prompt.get(\"user\", \"\")\n\n            # Format as a conversation\n            prompt_text = f\"{system_content}\\n\\n{user_content}\"\n        else:\n            # Legacy string prompt\n            prompt_text = prompt\n\n        # Add JSON instruction to ensure JSON output\n        prompt_text += \"\\n\\nRespond with valid JSON only.\"\n\n        try:\n            # Configure generation parameters\n            params = {\n                \"decoding_method\": \"greedy\",\n                \"temperature\": 0.1,  # Low temperature for consistent extraction\n                \"max_new_tokens\": self._max_new_tokens,\n                \"min_new_tokens\": 1,\n                \"repetition_penalty\": 1.0,\n            }\n\n            # Generate response\n            response = self.client.generate_text(prompt=prompt_text, params=params)\n\n            # Extract the generated text\n            if response is None:\n                rich_print(\"[red]Error:[/red] WatsonX returned None response\")\n                return {}\n\n            if not response or (isinstance(response, str) and not response.strip()):\n                rich_print(\"[red]Error:[/red] WatsonX returned empty response\")\n                return {}\n\n            # Parse JSON from response\n            try:\n                # Clean the response (remove markdown code blocks if present)\n                content = str(response).strip()\n\n                # Handle markdown code blocks with optional language specifier\n                if \"```json\" in content:\n                    # Extract content between ```json and ```\n                    start_idx = content.find(\"```json\") + 7\n                    end_idx = content.find(\"```\", start_idx)\n                    if end_idx != -1:\n                        content = content[start_idx:end_idx]\n                elif \"```\" in content:\n                    # Extract content between ``` and ```\n                    start_idx = content.find(\"```\") + 3\n                    end_idx = content.find(\"```\", start_idx)\n                    if end_idx != -1:\n                        content = content[start_idx:end_idx]\n                else:\n                    # No markdown blocks - look for JSON object or array\n                    # Find the first { or [ which indicates start of JSON\n                    json_start = -1\n                    for char in [\"{\", \"[\"]:\n                        idx = content.find(char)\n                        if idx != -1 and (json_start == -1 or idx &lt; json_start):\n                            json_start = idx\n\n                    if json_start != -1:\n                        content = content[json_start:]\n\n                content = content.strip()\n\n                parsed_json = json.loads(content)\n\n                # Validate it's not empty\n                if not parsed_json or (\n                    isinstance(parsed_json, dict) and not any(parsed_json.values())\n                ):\n                    rich_print(\"[yellow]Warning:[/yellow] WatsonX returned empty or all-null JSON\")\n\n                if isinstance(parsed_json, dict):\n                    return cast(Dict[str, Any], parsed_json)\n                else:\n                    rich_print(\n                        \"[yellow]Warning:[/yellow] Expected a JSON object; got non-dict. Returning empty dict.\"\n                    )\n                    return {}\n\n            except json.JSONDecodeError as e:\n                rich_print(f\"[red]Error:[/red] Failed to parse WatsonX response as JSON: {e}\")\n                rich_print(f\"[yellow]Raw response:[/yellow] {response}\")\n                return {}\n\n        except Exception as e:\n            rich_print(f\"[red]Error:[/red] WatsonX API call failed: {e}\")\n            import traceback\n\n            traceback.print_exc()\n            return {}\n\n    @property\n    def context_limit(self) -&gt; int:\n        return self._context_limit\n</code></pre>"},{"location":"api/llm-clients/#docling_graph.llm_clients.watsonx.WatsonxClient.get_json_response","title":"<code>get_json_response(prompt, schema_json)</code>","text":"<p>Execute WatsonX chat completion with JSON mode.</p> <p>Official docs: https://ibm.github.io/watsonx-ai-python-sdk/fm_chat.html</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str | dict</code> <p>Either a string (legacy) or dict with 'system' and 'user' keys.</p> required <code>schema_json</code> <code>str</code> <p>JSON schema (for reference, not directly used by WatsonX).</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Parsed JSON response from WatsonX.</p> Source code in <code>docling_graph/llm_clients/watsonx.py</code> <pre><code>def get_json_response(self, prompt: str | dict, schema_json: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Execute WatsonX chat completion with JSON mode.\n\n    Official docs: https://ibm.github.io/watsonx-ai-python-sdk/fm_chat.html\n\n    Args:\n        prompt: Either a string (legacy) or dict with 'system' and 'user' keys.\n        schema_json: JSON schema (for reference, not directly used by WatsonX).\n\n    Returns:\n        Parsed JSON response from WatsonX.\n    \"\"\"\n    # Build the prompt text\n    if isinstance(prompt, dict):\n        # Combine system and user messages\n        system_content = prompt.get(\"system\", \"\")\n        user_content = prompt.get(\"user\", \"\")\n\n        # Format as a conversation\n        prompt_text = f\"{system_content}\\n\\n{user_content}\"\n    else:\n        # Legacy string prompt\n        prompt_text = prompt\n\n    # Add JSON instruction to ensure JSON output\n    prompt_text += \"\\n\\nRespond with valid JSON only.\"\n\n    try:\n        # Configure generation parameters\n        params = {\n            \"decoding_method\": \"greedy\",\n            \"temperature\": 0.1,  # Low temperature for consistent extraction\n            \"max_new_tokens\": self._max_new_tokens,\n            \"min_new_tokens\": 1,\n            \"repetition_penalty\": 1.0,\n        }\n\n        # Generate response\n        response = self.client.generate_text(prompt=prompt_text, params=params)\n\n        # Extract the generated text\n        if response is None:\n            rich_print(\"[red]Error:[/red] WatsonX returned None response\")\n            return {}\n\n        if not response or (isinstance(response, str) and not response.strip()):\n            rich_print(\"[red]Error:[/red] WatsonX returned empty response\")\n            return {}\n\n        # Parse JSON from response\n        try:\n            # Clean the response (remove markdown code blocks if present)\n            content = str(response).strip()\n\n            # Handle markdown code blocks with optional language specifier\n            if \"```json\" in content:\n                # Extract content between ```json and ```\n                start_idx = content.find(\"```json\") + 7\n                end_idx = content.find(\"```\", start_idx)\n                if end_idx != -1:\n                    content = content[start_idx:end_idx]\n            elif \"```\" in content:\n                # Extract content between ``` and ```\n                start_idx = content.find(\"```\") + 3\n                end_idx = content.find(\"```\", start_idx)\n                if end_idx != -1:\n                    content = content[start_idx:end_idx]\n            else:\n                # No markdown blocks - look for JSON object or array\n                # Find the first { or [ which indicates start of JSON\n                json_start = -1\n                for char in [\"{\", \"[\"]:\n                    idx = content.find(char)\n                    if idx != -1 and (json_start == -1 or idx &lt; json_start):\n                        json_start = idx\n\n                if json_start != -1:\n                    content = content[json_start:]\n\n            content = content.strip()\n\n            parsed_json = json.loads(content)\n\n            # Validate it's not empty\n            if not parsed_json or (\n                isinstance(parsed_json, dict) and not any(parsed_json.values())\n            ):\n                rich_print(\"[yellow]Warning:[/yellow] WatsonX returned empty or all-null JSON\")\n\n            if isinstance(parsed_json, dict):\n                return cast(Dict[str, Any], parsed_json)\n            else:\n                rich_print(\n                    \"[yellow]Warning:[/yellow] Expected a JSON object; got non-dict. Returning empty dict.\"\n                )\n                return {}\n\n        except json.JSONDecodeError as e:\n            rich_print(f\"[red]Error:[/red] Failed to parse WatsonX response as JSON: {e}\")\n            rich_print(f\"[yellow]Raw response:[/yellow] {response}\")\n            return {}\n\n    except Exception as e:\n        rich_print(f\"[red]Error:[/red] WatsonX API call failed: {e}\")\n        import traceback\n\n        traceback.print_exc()\n        return {}\n</code></pre>"},{"location":"api/llm-clients/#ollamaclient","title":"OllamaClient","text":""},{"location":"api/llm-clients/#docling_graph.llm_clients.ollama.OllamaClient","title":"<code>docling_graph.llm_clients.ollama.OllamaClient</code>","text":"<p>               Bases: <code>BaseLlmClient</code></p> <p>Ollama (local LLM) implementation with proper message structure.</p> Source code in <code>docling_graph/llm_clients/ollama.py</code> <pre><code>class OllamaClient(BaseLlmClient):\n    \"\"\"Ollama (local LLM) implementation with proper message structure.\"\"\"\n\n    def __init__(self, model: str) -&gt; None:\n        if ollama is None:\n            raise ImportError(\n                \"Ollama client could not be imported. Please install it with: pip install ollama\"\n            )\n\n        self.model = model\n\n        # Use centralized config registry (ollama provider)\n        self._context_limit = get_context_limit(\"ollama\", model)\n\n        try:\n            rich_print(f\"[OllamaClient] Checking Ollama connection and model '{self.model}'...\")\n            ollama.show(self.model)\n            rich_print(f\"[OllamaClient] Initialized with Ollama model: [blue]{self.model}[/blue]\")\n        except Exception as e:\n            rich_print(f\"[red]Ollama connection error:[/red] {e}\")\n            rich_print(\"Please ensure:\")\n            rich_print(\"  1. Ollama is running: ollama serve\")\n            rich_print(f\"  2. Model is available: ollama pull {self.model}\")\n            raise RuntimeError(str(e)) from e\n\n    def get_json_response(self, prompt: str | dict[str, str], schema_json: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Execute Ollama chat with JSON format.\n\n        Official docs: https://ollama.com/blog/structured-outputs\n\n        Args:\n            prompt: Either a string (legacy) or dict with 'system' and 'user' keys.\n            schema_json: JSON schema (can be used as format parameter).\n\n        Returns:\n            Parsed JSON response from Ollama.\n        \"\"\"\n        # Handle both legacy string prompts and new dict prompts\n        if isinstance(prompt, dict):\n            messages = [\n                {\"role\": \"system\", \"content\": prompt[\"system\"]},\n                {\"role\": \"user\", \"content\": prompt[\"user\"]},\n            ]\n        else:\n            # Legacy: treat entire prompt as user message\n            messages = [{\"role\": \"user\", \"content\": prompt}]\n\n        try:\n            # Call Ollama with JSON format (official method)\n            res = ollama.chat(\n                model=self.model,\n                messages=messages,\n                format=\"json\",  # Official JSON mode - ensures valid JSON\n                options={\n                    \"temperature\": 0.1,  # Low temperature for consistent extraction\n                },\n            )\n\n            # Get response content\n            raw_json = res[\"message\"][\"content\"]\n\n            # Parse JSON\n            try:\n                parsed = json.loads(raw_json)\n\n                # Validate it's not empty\n                if not parsed or (isinstance(parsed, dict) and not any(parsed.values())):\n                    rich_print(\"[yellow]Warning:[/yellow] Ollama returned empty or all-null JSON\")\n\n                # Ensure return type matches Dict[str, Any]\n                if isinstance(parsed, dict):\n                    return cast(Dict[str, Any], parsed)\n                else:\n                    rich_print(\n                        \"[yellow]Warning:[/yellow] Expected a JSON object; got non-dict. Returning empty dict.\"\n                    )\n                    return {}\n\n            except json.JSONDecodeError as e:\n                rich_print(f\"[red]Error:[/red] Failed to parse Ollama response as JSON: {e}\")\n                rich_print(f\"[yellow]Raw response:[/yellow] {raw_json}\")\n                return {}\n\n        except Exception as e:\n            rich_print(f\"[red]Error:[/red] Ollama API call failed: {type(e).__name__}: {e}\")\n            return {}\n\n    @property\n    def context_limit(self) -&gt; int:\n        return self._context_limit\n</code></pre>"},{"location":"api/llm-clients/#docling_graph.llm_clients.ollama.OllamaClient.get_json_response","title":"<code>get_json_response(prompt, schema_json)</code>","text":"<p>Execute Ollama chat with JSON format.</p> <p>Official docs: https://ollama.com/blog/structured-outputs</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str | dict[str, str]</code> <p>Either a string (legacy) or dict with 'system' and 'user' keys.</p> required <code>schema_json</code> <code>str</code> <p>JSON schema (can be used as format parameter).</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Parsed JSON response from Ollama.</p> Source code in <code>docling_graph/llm_clients/ollama.py</code> <pre><code>def get_json_response(self, prompt: str | dict[str, str], schema_json: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Execute Ollama chat with JSON format.\n\n    Official docs: https://ollama.com/blog/structured-outputs\n\n    Args:\n        prompt: Either a string (legacy) or dict with 'system' and 'user' keys.\n        schema_json: JSON schema (can be used as format parameter).\n\n    Returns:\n        Parsed JSON response from Ollama.\n    \"\"\"\n    # Handle both legacy string prompts and new dict prompts\n    if isinstance(prompt, dict):\n        messages = [\n            {\"role\": \"system\", \"content\": prompt[\"system\"]},\n            {\"role\": \"user\", \"content\": prompt[\"user\"]},\n        ]\n    else:\n        # Legacy: treat entire prompt as user message\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    try:\n        # Call Ollama with JSON format (official method)\n        res = ollama.chat(\n            model=self.model,\n            messages=messages,\n            format=\"json\",  # Official JSON mode - ensures valid JSON\n            options={\n                \"temperature\": 0.1,  # Low temperature for consistent extraction\n            },\n        )\n\n        # Get response content\n        raw_json = res[\"message\"][\"content\"]\n\n        # Parse JSON\n        try:\n            parsed = json.loads(raw_json)\n\n            # Validate it's not empty\n            if not parsed or (isinstance(parsed, dict) and not any(parsed.values())):\n                rich_print(\"[yellow]Warning:[/yellow] Ollama returned empty or all-null JSON\")\n\n            # Ensure return type matches Dict[str, Any]\n            if isinstance(parsed, dict):\n                return cast(Dict[str, Any], parsed)\n            else:\n                rich_print(\n                    \"[yellow]Warning:[/yellow] Expected a JSON object; got non-dict. Returning empty dict.\"\n                )\n                return {}\n\n        except json.JSONDecodeError as e:\n            rich_print(f\"[red]Error:[/red] Failed to parse Ollama response as JSON: {e}\")\n            rich_print(f\"[yellow]Raw response:[/yellow] {raw_json}\")\n            return {}\n\n    except Exception as e:\n        rich_print(f\"[red]Error:[/red] Ollama API call failed: {type(e).__name__}: {e}\")\n        return {}\n</code></pre>"},{"location":"api/llm-clients/#vllmclient","title":"VLLMClient","text":""},{"location":"api/llm-clients/#docling_graph.llm_clients.vllm.VllmClient","title":"<code>docling_graph.llm_clients.vllm.VllmClient</code>","text":"<p>               Bases: <code>BaseLlmClient</code></p> <p>vLLM client implementation using OpenAI-compatible API.</p> Source code in <code>docling_graph/llm_clients/vllm.py</code> <pre><code>class VllmClient(BaseLlmClient):\n    \"\"\"vLLM client implementation using OpenAI-compatible API.\"\"\"\n\n    def __init__(\n        self, model: str, base_url: str = \"http://localhost:8000/v1\", api_key: str = \"EMPTY\"\n    ) -&gt; None:\n        \"\"\"Initialize vLLM client.\"\"\"\n        self.model = model\n        self.base_url = base_url\n        self.api_key = api_key\n\n        # Initialize OpenAI client pointing to vLLM server\n        self.client = OpenAI(base_url=base_url, api_key=api_key)\n\n        # Use centralized config registry (vllm provider)\n        self._context_limit = get_context_limit(\"vllm\", model)\n\n        # Test connection\n        try:\n            rich_print(\n                f\"[blue][VllmClient][/blue] Connecting to vLLM server at: [cyan]{self.base_url}[/cyan]\"\n            )\n            self.client.models.list()\n            rich_print(\"[blue][VllmClient][/blue] Connected successfully\")\n            rich_print(f\"[blue][VllmClient][/blue] Using model: [blue]{self.model}[/blue]\")\n        except Exception as e:\n            rich_print(f\"[red]vLLM connection failed:[/red] {e}\")\n            rich_print(\"\\n[yellow]Setup instructions:[/yellow]\")\n            rich_print(\"  1. Start vLLM server in a separate terminal:\")\n            rich_print(f\"     [cyan]vllm serve {self.model}[/cyan]\")\n            rich_print(\"  2. Wait for server to load (may take 1-2 minutes)\")\n            rich_print(f\"  3. Ensure server is accessible at: [cyan]{self.base_url}[/cyan]\")\n            rich_print(\"\\n[dim]On Windows: Run vLLM server in WSL2 or Docker[/dim]\")\n            raise\n\n    def get_json_response(self, prompt: str | dict[str, str], schema_json: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Execute vLLM chat with JSON format using OpenAI-compatible API.\n\n        Args:\n            prompt: Either a string (legacy) or dict with 'system' and 'user' keys.\n            schema_json: JSON schema (can be used for guided decoding).\n\n        Returns:\n            Parsed JSON response from vLLM.\n        \"\"\"\n        # Handle both legacy string prompts and new dict prompts\n        # Define once to avoid mypy no-redef when annotating in both branches\n        messages: list[ChatCompletionMessageParam]\n        if isinstance(prompt, dict):\n            messages = [\n                {\"role\": \"system\", \"content\": prompt.get(\"system\", \"\")},\n                {\"role\": \"user\", \"content\": prompt.get(\"user\", \"\")},\n            ]\n        else:\n            # Legacy: treat entire prompt as user message\n            messages = [{\"role\": \"user\", \"content\": prompt}]\n\n        try:\n            # Call vLLM via OpenAI-compatible API with JSON mode\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                temperature=0.1,  # Low temperature for consistent extraction\n                response_format={\"type\": \"json_object\"},  # Force JSON output\n                # vLLM supports extra_body for additional parameters like guided_json\n                # extra_body={\"guided_json\": schema_json}  # Uncomment for schema validation\n            )\n\n            # Get response content\n            raw_json = response.choices[0].message.content\n\n            # Parse JSON\n            if not raw_json:\n                rich_print(\"[red]Error:[/red] vLLM returned empty content\")\n                return {}\n\n            try:\n                parsed_json = json.loads(raw_json)\n\n                # Validate it's not empty\n                if not parsed_json or (\n                    isinstance(parsed_json, dict) and not any(parsed_json.values())\n                ):\n                    rich_print(\"[yellow]Warning:[/yellow] vLLM returned empty or all-null JSON\")\n\n                # Ensure return type matches Dict[str, Any]\n                if isinstance(parsed_json, dict):\n                    return cast(Dict[str, Any], parsed_json)\n                else:\n                    rich_print(\n                        \"[yellow]Warning:[/yellow] Expected a JSON object; got non-dict. Returning empty dict.\"\n                    )\n                    return {}\n            except json.JSONDecodeError as e:\n                rich_print(f\"[red]Error:[/red] Failed to parse vLLM response as JSON: {e}\")\n                rich_print(f\"[yellow]Raw response:[/yellow] {raw_json}\")\n                return {}\n\n        except Exception as e:\n            rich_print(f\"[red]Error:[/red] vLLM API call failed: {type(e).__name__}: {e}\")\n            return {}\n\n    @property\n    def context_limit(self) -&gt; int:\n        \"\"\"Return context window size.\"\"\"\n        return self._context_limit\n</code></pre>"},{"location":"api/llm-clients/#docling_graph.llm_clients.vllm.VllmClient.context_limit","title":"<code>context_limit</code>  <code>property</code>","text":"<p>Return context window size.</p>"},{"location":"api/llm-clients/#docling_graph.llm_clients.vllm.VllmClient.__init__","title":"<code>__init__(model, base_url='http://localhost:8000/v1', api_key='EMPTY')</code>","text":"<p>Initialize vLLM client.</p> Source code in <code>docling_graph/llm_clients/vllm.py</code> <pre><code>def __init__(\n    self, model: str, base_url: str = \"http://localhost:8000/v1\", api_key: str = \"EMPTY\"\n) -&gt; None:\n    \"\"\"Initialize vLLM client.\"\"\"\n    self.model = model\n    self.base_url = base_url\n    self.api_key = api_key\n\n    # Initialize OpenAI client pointing to vLLM server\n    self.client = OpenAI(base_url=base_url, api_key=api_key)\n\n    # Use centralized config registry (vllm provider)\n    self._context_limit = get_context_limit(\"vllm\", model)\n\n    # Test connection\n    try:\n        rich_print(\n            f\"[blue][VllmClient][/blue] Connecting to vLLM server at: [cyan]{self.base_url}[/cyan]\"\n        )\n        self.client.models.list()\n        rich_print(\"[blue][VllmClient][/blue] Connected successfully\")\n        rich_print(f\"[blue][VllmClient][/blue] Using model: [blue]{self.model}[/blue]\")\n    except Exception as e:\n        rich_print(f\"[red]vLLM connection failed:[/red] {e}\")\n        rich_print(\"\\n[yellow]Setup instructions:[/yellow]\")\n        rich_print(\"  1. Start vLLM server in a separate terminal:\")\n        rich_print(f\"     [cyan]vllm serve {self.model}[/cyan]\")\n        rich_print(\"  2. Wait for server to load (may take 1-2 minutes)\")\n        rich_print(f\"  3. Ensure server is accessible at: [cyan]{self.base_url}[/cyan]\")\n        rich_print(\"\\n[dim]On Windows: Run vLLM server in WSL2 or Docker[/dim]\")\n        raise\n</code></pre>"},{"location":"api/llm-clients/#docling_graph.llm_clients.vllm.VllmClient.get_json_response","title":"<code>get_json_response(prompt, schema_json)</code>","text":"<p>Execute vLLM chat with JSON format using OpenAI-compatible API.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str | dict[str, str]</code> <p>Either a string (legacy) or dict with 'system' and 'user' keys.</p> required <code>schema_json</code> <code>str</code> <p>JSON schema (can be used for guided decoding).</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Parsed JSON response from vLLM.</p> Source code in <code>docling_graph/llm_clients/vllm.py</code> <pre><code>def get_json_response(self, prompt: str | dict[str, str], schema_json: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Execute vLLM chat with JSON format using OpenAI-compatible API.\n\n    Args:\n        prompt: Either a string (legacy) or dict with 'system' and 'user' keys.\n        schema_json: JSON schema (can be used for guided decoding).\n\n    Returns:\n        Parsed JSON response from vLLM.\n    \"\"\"\n    # Handle both legacy string prompts and new dict prompts\n    # Define once to avoid mypy no-redef when annotating in both branches\n    messages: list[ChatCompletionMessageParam]\n    if isinstance(prompt, dict):\n        messages = [\n            {\"role\": \"system\", \"content\": prompt.get(\"system\", \"\")},\n            {\"role\": \"user\", \"content\": prompt.get(\"user\", \"\")},\n        ]\n    else:\n        # Legacy: treat entire prompt as user message\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    try:\n        # Call vLLM via OpenAI-compatible API with JSON mode\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            temperature=0.1,  # Low temperature for consistent extraction\n            response_format={\"type\": \"json_object\"},  # Force JSON output\n            # vLLM supports extra_body for additional parameters like guided_json\n            # extra_body={\"guided_json\": schema_json}  # Uncomment for schema validation\n        )\n\n        # Get response content\n        raw_json = response.choices[0].message.content\n\n        # Parse JSON\n        if not raw_json:\n            rich_print(\"[red]Error:[/red] vLLM returned empty content\")\n            return {}\n\n        try:\n            parsed_json = json.loads(raw_json)\n\n            # Validate it's not empty\n            if not parsed_json or (\n                isinstance(parsed_json, dict) and not any(parsed_json.values())\n            ):\n                rich_print(\"[yellow]Warning:[/yellow] vLLM returned empty or all-null JSON\")\n\n            # Ensure return type matches Dict[str, Any]\n            if isinstance(parsed_json, dict):\n                return cast(Dict[str, Any], parsed_json)\n            else:\n                rich_print(\n                    \"[yellow]Warning:[/yellow] Expected a JSON object; got non-dict. Returning empty dict.\"\n                )\n                return {}\n        except json.JSONDecodeError as e:\n            rich_print(f\"[red]Error:[/red] Failed to parse vLLM response as JSON: {e}\")\n            rich_print(f\"[yellow]Raw response:[/yellow] {raw_json}\")\n            return {}\n\n    except Exception as e:\n        rich_print(f\"[red]Error:[/red] vLLM API call failed: {type(e).__name__}: {e}\")\n        return {}\n</code></pre>"},{"location":"api/llm-clients/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - LLM configuration</li> <li>WatsonX Integration - WatsonX setup guide</li> <li>Examples - Usage examples</li> </ul>"},{"location":"api/pipeline/","title":"Pipeline API","text":"<p>The pipeline module provides the main entry point for running document-to-graph conversions.</p>"},{"location":"api/pipeline/#run_pipeline","title":"run_pipeline","text":""},{"location":"api/pipeline/#docling_graph.pipeline.run_pipeline","title":"<code>docling_graph.pipeline.run_pipeline(config)</code>","text":"<p>Runs the extraction and graph conversion pipeline.</p> Source code in <code>docling_graph/pipeline.py</code> <pre><code>def run_pipeline(config: Union[PipelineConfig, Dict[str, Any]]) -&gt; None:\n    \"\"\"Runs the extraction and graph conversion pipeline.\"\"\"\n    # Normalize to typed config up-front\n    cfg: PipelineConfig = config if isinstance(config, PipelineConfig) else PipelineConfig(**config)\n\n    # Use normalized flat dict for downstream access\n    conf: Dict[str, Any] = cfg.to_dict()\n\n    rich_print(\"\\n--- [blue]Starting Docling-Graph Pipeline[/blue] ---\")\n\n    # Create shared registry for deterministic node IDs across batches\n    node_registry = NodeIDRegistry()\n\n    # Validate modes\n    processing_mode = cast(Literal[\"one-to-one\", \"many-to-one\"], conf[\"processing_mode\"])\n    backend_literal = cast(Literal[\"vlm\", \"llm\"], conf[\"backend\"])\n\n    inference = cast(str, conf[\"inference\"])\n    docling_config = cast(str, conf[\"docling_config\"])\n    reverse_edges = cast(bool, conf.get(\"reverse_edges\", False))\n    llm_consolidation = cast(bool, conf.get(\"llm_consolidation\", True))\n    use_chunking = cast(bool, conf.get(\"use_chunking\", True))\n\n    output_dir = Path(conf.get(\"output_dir\", \"outputs\"))\n    output_dir.mkdir(parents=True, exist_ok=True)\n    base_name = Path(conf[\"source\"]).stem\n\n    extractor = None\n    llm_client: BaseLlmClient | None = None\n\n    try:\n        # 1. Load Template\n        template_val = conf[\"template\"]\n        if isinstance(template_val, str):\n            template_class = _load_template_class(template_val)\n        elif isinstance(template_val, type):\n            template_class = template_val\n        else:\n            raise TypeError(\n                \"Template must be a dotted path string or a Pydantic BaseModel subclass.\"\n            )\n\n        # 2. Get model configuration\n        models_config = cast(Dict[str, Any], conf[\"models\"])\n        model_config = _get_model_config(\n            models_config,\n            backend_literal,\n            inference,\n            conf.get(\"model_override\"),\n            conf.get(\"provider_override\"),\n        )\n\n        rich_print(\n            f\"[blue][Pipeline][/blue] Using model: [cyan]{model_config['model']}[/cyan] \"\n            f\"(provider: {model_config['provider']})\"\n        )\n\n        # 3. Create extractor\n        if backend_literal == \"vlm\":\n            extractor = ExtractorFactory.create_extractor(\n                processing_mode=processing_mode,\n                backend_name=backend_literal,\n                model_name=model_config[\"model\"],\n                docling_config=docling_config,\n            )\n        else:\n            llm_client = _initialize_llm_client(model_config[\"provider\"], model_config[\"model\"])\n            extractor = ExtractorFactory.create_extractor(\n                processing_mode=processing_mode,\n                backend_name=backend_literal,\n                llm_client=llm_client,\n                docling_config=docling_config,\n                llm_consolidation=llm_consolidation,\n                use_chunking=use_chunking,\n            )\n\n        # 4. Run Extraction\n        rich_print(\"[blue][Pipeline][/blue] Starting extraction...\")\n        extracted_models = extractor.extract(conf[\"source\"], template_class)\n\n        if not extracted_models:\n            rich_print(\"[yellow][Pipeline][/yellow] No models extracted.\")\n            return\n\n        rich_print(\n            f\"[green][Pipeline][/green] Successfully extracted \"\n            f\"[cyan]{len(extracted_models)}[/cyan] item(s).\"\n        )\n\n        # 5. Export Docling outputs (if configured)\n        if (\n            conf.get(\"export_docling\", True)\n            or conf.get(\"export_docling_json\", True)\n            or conf.get(\"export_markdown\", True)\n        ):\n            rich_print(\"[blue][Pipeline][/blue] Exporting Docling document and markdown...\")\n            docling_exporter = DoclingExporter(output_dir=output_dir)\n\n            # Reuse the already-converted document from the extraction phase\n            if hasattr(extractor, \"doc_processor\") and extractor.doc_processor.last_document:\n                docling_document = extractor.doc_processor.last_document\n                rich_print(\n                    \"[blue][Pipeline][/blue] Reusing cached DoclingDocument (avoiding duplicate conversion)\"\n                )\n                docling_exporter.export_document(\n                    docling_document,\n                    base_name=base_name,\n                    include_json=conf.get(\"export_docling_json\", True),\n                    include_markdown=conf.get(\"export_markdown\", True),\n                    per_page=conf.get(\"export_per_page_markdown\", False),\n                )\n            else:\n                rich_print(\n                    \"[yellow][Pipeline][/yellow] No cached document available, skipping Docling export\"\n                )\n\n        # 6. Convert to Graph\n        rich_print(\"[blue][Pipeline][/blue] Converting Pydantic model(s) to Knowledge Graph...\")\n\n        converter = GraphConverter(\n            add_reverse_edges=reverse_edges,\n            validate_graph=True,\n            registry=node_registry,\n        )\n\n        try:\n            knowledge_graph, graph_metadata = converter.pydantic_list_to_graph(extracted_models)\n        except ValueError as e:\n            rich_print(f\"[red][Pipeline] Graph creation failed:[/red] {e}\")\n            raise\n\n        rich_print(\n            f\"[green][Pipeline][/green] Graph created with \"\n            f\"[blue]{graph_metadata.node_count} nodes[/blue] \"\n            f\"and [blue]{graph_metadata.edge_count} edges[/blue].\"\n        )\n\n        # 7. Export graph\n        export_format = cast(str, conf.get(\"export_format\", \"csv\"))\n        rich_print(\n            f\"[blue][Pipeline][/blue] Exporting graph data in [cyan]{export_format.upper()}[/cyan] format...\"\n        )\n\n        if export_format == \"csv\":\n            CSVExporter().export(knowledge_graph, output_dir)\n            rich_print(f\"[green]\u2192[/green] Saved CSV files to [green]{output_dir}[/green]\")\n        elif export_format == \"cypher\":\n            cypher_path = output_dir / f\"{base_name}_graph.cypher\"\n            CypherExporter().export(knowledge_graph, cypher_path)\n            rich_print(f\"[green]\u2192[/green] Saved Cypher script to [green]{cypher_path}[/green]\")\n        else:\n            raise ValueError(f\"Unknown export format: {export_format}\")\n\n        # Always export to JSON\n        json_path = output_dir / f\"{base_name}_graph.json\"\n        JSONExporter().export(knowledge_graph, json_path)\n        rich_print(f\"[green]\u2192[/green] Saved JSON to [green]{json_path}[/green]\")\n\n        # 8. Reports and visualization\n        rich_print(\"[blue][Pipeline][/blue] Generating reports and visualizations...\")\n        report_path = output_dir / f\"{base_name}_report\"\n        ReportGenerator().visualize(\n            knowledge_graph, report_path, source_model_count=len(extracted_models)\n        )\n        rich_print(f\"[green]\u2192[/green] Generated markdown report at {report_path}\")\n\n        html_path = output_dir / f\"{base_name}_graph.html\"\n        InteractiveVisualizer().save_cytoscape_graph(knowledge_graph, html_path)\n        rich_print(f\"[green]\u2192[/green] Generated interactive HTML graph at {html_path}\")\n\n        rich_print(\"--- [blue]Pipeline Finished Successfully[/blue] ---\")\n\n    finally:\n        # Cleanup resources\n        rich_print(\"Cleaning up resources...\")\n        if extractor and hasattr(extractor, \"backend\") and hasattr(extractor.backend, \"cleanup\"):\n            extractor.backend.cleanup()\n        if (\n            extractor\n            and hasattr(extractor, \"doc_processor\")\n            and hasattr(extractor.doc_processor, \"cleanup\")\n        ):\n            extractor.doc_processor.cleanup()\n        if llm_client is not None:\n            del llm_client\n\n        import gc\n\n        gc.collect()\n        try:\n            import torch\n\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except ImportError:\n            pass\n</code></pre>"},{"location":"api/pipeline/#pipelineconfig","title":"PipelineConfig","text":""},{"location":"api/pipeline/#docling_graph.config.PipelineConfig","title":"<code>docling_graph.config.PipelineConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type-safe configuration for the docling-graph pipeline. This is the SINGLE SOURCE OF TRUTH for all defaults. All other modules should reference these defaults via PipelineConfig, not duplicate them.</p> Source code in <code>docling_graph/config.py</code> <pre><code>class PipelineConfig(BaseModel):\n    \"\"\"\n    Type-safe configuration for the docling-graph pipeline.\n    This is the SINGLE SOURCE OF TRUTH for all defaults.\n    All other modules should reference these defaults via PipelineConfig, not duplicate them.\n    \"\"\"\n\n    # Optional fields (empty by default, filled in at runtime)\n    source: Union[str, Path] = Field(default=\"\", description=\"Path to the source document\")\n    template: Union[str, type[BaseModel]] = Field(\n        default=\"\", description=\"Pydantic template class or dotted path string\"\n    )\n\n    # Core processing settings (with defaults)\n    backend: Literal[\"llm\", \"vlm\"] = Field(default=\"llm\")\n    inference: Literal[\"local\", \"remote\"] = Field(default=\"local\")\n    processing_mode: Literal[\"one-to-one\", \"many-to-one\"] = Field(default=\"many-to-one\")\n\n    # Docling settings (with defaults)\n    docling_config: Literal[\"ocr\", \"vision\"] = Field(default=\"ocr\")\n\n    # Model overrides\n    model_override: str | None = None\n    provider_override: str | None = None\n\n    # Models configuration (flat only, with defaults)\n    models: ModelsConfig = Field(default_factory=ModelsConfig)\n\n    # Extract settings (with defaults)\n    use_chunking: bool = True\n    llm_consolidation: bool = False\n    max_batch_size: int = 1\n\n    # Export settings (with defaults)\n    export_format: Literal[\"csv\", \"cypher\"] = Field(default=\"csv\")\n    export_docling: bool = Field(default=True)\n    export_docling_json: bool = Field(default=True)\n    export_markdown: bool = Field(default=True)\n    export_per_page_markdown: bool = Field(default=False)\n\n    # Graph settings (with defaults)\n    reverse_edges: bool = Field(default=False)\n\n    # Output settings (with defaults)\n    output_dir: Union[str, Path] = Field(default=\"outputs\")\n\n    @field_validator(\"source\", \"output_dir\")\n    @classmethod\n    def _path_to_str(cls, v: Union[str, Path]) -&gt; str:\n        return str(v)\n\n    @model_validator(mode=\"after\")\n    def _validate_vlm_constraints(self) -&gt; Self:\n        if self.backend == \"vlm\" and self.inference == \"remote\":\n            raise ValueError(\n                \"VLM backend currently only supports local inference. Use inference='local' or backend='llm'.\"\n            )\n        return self\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert config to dictionary format expected by run_pipeline.\"\"\"\n        return {\n            \"source\": self.source,\n            \"template\": self.template,\n            \"backend\": self.backend,\n            \"inference\": self.inference,\n            \"processing_mode\": self.processing_mode,\n            \"docling_config\": self.docling_config,\n            \"use_chunking\": self.use_chunking,\n            \"llm_consolidation\": self.llm_consolidation,\n            \"model_override\": self.model_override,\n            \"provider_override\": self.provider_override,\n            \"export_format\": self.export_format,\n            \"export_docling\": self.export_docling,\n            \"export_docling_json\": self.export_docling_json,\n            \"export_markdown\": self.export_markdown,\n            \"export_per_page_markdown\": self.export_per_page_markdown,\n            \"reverse_edges\": self.reverse_edges,\n            \"output_dir\": self.output_dir,\n            \"models\": self.models.model_dump(),\n        }\n\n    def run(self) -&gt; None:\n        \"\"\"Convenience method to run the pipeline with this configuration.\"\"\"\n        from docling_graph.pipeline import run_pipeline\n\n        run_pipeline(self.to_dict())\n\n    @classmethod\n    def generate_yaml_dict(cls) -&gt; Dict[str, Any]:\n        \"\"\"\n        Generate a YAML-compatible config dict with all defaults.\n        This is used by init.py to create config_template.yaml and config.yaml\n        without hardcoding defaults in multiple places.\n        \"\"\"\n        default_config = cls()\n        return {\n            \"defaults\": {\n                \"backend\": default_config.backend,\n                \"inference\": default_config.inference,\n                \"processing_mode\": default_config.processing_mode,\n                \"export_format\": default_config.export_format,\n            },\n            \"docling\": {\n                \"pipeline\": default_config.docling_config,\n                \"export\": {\n                    \"docling_json\": default_config.export_docling_json,\n                    \"markdown\": default_config.export_markdown,\n                    \"per_page_markdown\": default_config.export_per_page_markdown,\n                },\n            },\n            \"models\": default_config.models.model_dump(),\n            \"output\": {\n                \"directory\": str(default_config.output_dir),\n            },\n        }\n</code></pre>"},{"location":"api/pipeline/#docling_graph.config.PipelineConfig.generate_yaml_dict","title":"<code>generate_yaml_dict()</code>  <code>classmethod</code>","text":"<p>Generate a YAML-compatible config dict with all defaults. This is used by init.py to create config_template.yaml and config.yaml without hardcoding defaults in multiple places.</p> Source code in <code>docling_graph/config.py</code> <pre><code>@classmethod\ndef generate_yaml_dict(cls) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a YAML-compatible config dict with all defaults.\n    This is used by init.py to create config_template.yaml and config.yaml\n    without hardcoding defaults in multiple places.\n    \"\"\"\n    default_config = cls()\n    return {\n        \"defaults\": {\n            \"backend\": default_config.backend,\n            \"inference\": default_config.inference,\n            \"processing_mode\": default_config.processing_mode,\n            \"export_format\": default_config.export_format,\n        },\n        \"docling\": {\n            \"pipeline\": default_config.docling_config,\n            \"export\": {\n                \"docling_json\": default_config.export_docling_json,\n                \"markdown\": default_config.export_markdown,\n                \"per_page_markdown\": default_config.export_per_page_markdown,\n            },\n        },\n        \"models\": default_config.models.model_dump(),\n        \"output\": {\n            \"directory\": str(default_config.output_dir),\n        },\n    }\n</code></pre>"},{"location":"api/pipeline/#docling_graph.config.PipelineConfig.run","title":"<code>run()</code>","text":"<p>Convenience method to run the pipeline with this configuration.</p> Source code in <code>docling_graph/config.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Convenience method to run the pipeline with this configuration.\"\"\"\n    from docling_graph.pipeline import run_pipeline\n\n    run_pipeline(self.to_dict())\n</code></pre>"},{"location":"api/pipeline/#docling_graph.config.PipelineConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert config to dictionary format expected by run_pipeline.</p> Source code in <code>docling_graph/config.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert config to dictionary format expected by run_pipeline.\"\"\"\n    return {\n        \"source\": self.source,\n        \"template\": self.template,\n        \"backend\": self.backend,\n        \"inference\": self.inference,\n        \"processing_mode\": self.processing_mode,\n        \"docling_config\": self.docling_config,\n        \"use_chunking\": self.use_chunking,\n        \"llm_consolidation\": self.llm_consolidation,\n        \"model_override\": self.model_override,\n        \"provider_override\": self.provider_override,\n        \"export_format\": self.export_format,\n        \"export_docling\": self.export_docling,\n        \"export_docling_json\": self.export_docling_json,\n        \"export_markdown\": self.export_markdown,\n        \"export_per_page_markdown\": self.export_per_page_markdown,\n        \"reverse_edges\": self.reverse_edges,\n        \"output_dir\": self.output_dir,\n        \"models\": self.models.model_dump(),\n    }\n</code></pre>"},{"location":"api/pipeline/#usage-examples","title":"Usage Examples","text":""},{"location":"api/pipeline/#basic-usage","title":"Basic Usage","text":"<pre><code>from docling_graph import run_pipeline, PipelineConfig\nfrom your_templates import YourTemplate\n\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    output_dir=\"outputs\"\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"api/pipeline/#with-custom-configuration","title":"With Custom Configuration","text":"<pre><code>from docling_graph import run_pipeline, PipelineConfig\nfrom docling_graph.llm_clients import LLMConfig\n\nllm_config = LLMConfig(\n    provider=\"openai\",\n    model=\"gpt-4\",\n    temperature=0.1,\n    max_tokens=4000\n)\n\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    llm_config=llm_config,\n    processing_mode=\"many-to-one\",\n    use_chunking=True,\n    output_dir=\"outputs\"\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"api/pipeline/#batch-processing","title":"Batch Processing","text":"<pre><code>from pathlib import Path\nfrom docling_graph import run_pipeline, PipelineConfig\n\ndocuments = Path(\"documents\").glob(\"*.pdf\")\n\nfor doc in documents:\n    config = PipelineConfig(\n        source=str(doc),\n        template=YourTemplate,\n        backend=\"llm\",\n        output_dir=f\"outputs/{doc.stem}\"\n    )\n\n    try:\n        run_pipeline(config)\n        print(f\"Processed {doc.name}\")\n    except Exception as e:\n        print(f\"Failed {doc.name}: {e}\")\n</code></pre>"},{"location":"api/pipeline/#see-also","title":"See Also","text":"<ul> <li>Configuration - Configuration options</li> <li>Quick Start - Getting started guide</li> <li>Examples - More examples</li> </ul>"},{"location":"concepts/","title":"Concepts","text":"<p>Welcome to the Docling Graph concepts documentation. This section provides in-depth explanations of the core concepts, architecture, and design principles behind Docling Graph.</p>"},{"location":"concepts/#overview","title":"Overview","text":"<p>Docling Graph transforms unstructured documents into validated knowledge graphs through a flexible, config-driven pipeline. Understanding these core concepts will help you effectively use and extend the toolkit.</p>"},{"location":"concepts/#core-concepts","title":"Core Concepts","text":""},{"location":"concepts/#architecture","title":"Architecture","text":"<p>Learn about the overall system architecture, component interactions, and the pipeline flow from document to knowledge graph.</p>"},{"location":"concepts/#extraction-backends","title":"Extraction Backends","text":"<p>Understand the two extraction families (VLM and LLM), their differences, and when to use each approach.</p>"},{"location":"concepts/#processing-strategies","title":"Processing Strategies","text":"<p>Explore the one-to-one and many-to-one processing modes and how they affect document handling.</p>"},{"location":"concepts/#pydantic-templates","title":"Pydantic Templates","text":"<p>Deep dive into how Pydantic models serve as extraction schemas and define graph structure.</p>"},{"location":"concepts/#graph-construction","title":"Graph Construction","text":"<p>Learn how validated Pydantic objects are converted into NetworkX directed graphs with semantic relationships.</p>"},{"location":"concepts/#export-formats","title":"Export Formats","text":"<p>Understand the various export formats (CSV, Cypher, JSON) and their use cases.</p>"},{"location":"concepts/#configuration-system","title":"Configuration System","text":"<p>Explore the configuration system and how to customize pipeline behavior.</p>"},{"location":"concepts/#why-knowledge-graphs","title":"Why Knowledge Graphs?","text":"<p>Traditional document processing approaches convert text to vectors or embeddings, which lose precise semantic relationships between entities. Docling Graph addresses this by:</p> <ul> <li>Preserving Relationships: Maintains explicit connections between entities (e.g., \"who issued what to whom\")</li> <li>Enabling Explainability: Provides audit trails showing how information connects</li> <li>Supporting Complex Queries: Enables traversal of exact relationships for domain-specific questions</li> <li>Ensuring Validation: Validates data against schemas before graph construction</li> </ul> <p>This is critical for complex domains like chemistry, finance, physics, and legal documents where understanding exact entity connections is essential for production systems, regulatory compliance, and scientific accuracy.</p>"},{"location":"concepts/#getting-started","title":"Getting Started","text":"<p>If you're new to Docling Graph, we recommend reading the concepts in this order:</p> <ol> <li>Architecture - Understand the big picture</li> <li>Extraction Backends - Choose your extraction approach</li> <li>Pydantic Templates - Learn to define extraction schemas</li> <li>Processing Strategies - Select the right processing mode</li> <li>Graph Construction - Understand graph generation</li> <li>Export Formats - Choose your output format</li> </ol>"},{"location":"concepts/#visual-overview","title":"Visual Overview","text":"<p>For a visual representation of the complete pipeline, see the Pipeline Flowchart.</p>"},{"location":"concepts/architecture/","title":"Architecture","text":"<p>This document provides an overview of Docling Graph's architecture, explaining how components interact to transform documents into knowledge graphs.</p>"},{"location":"concepts/architecture/#system-overview","title":"System Overview","text":"<p>Docling Graph follows a modular, pipeline-based architecture with clear separation of concerns.</p> <p></p>"},{"location":"concepts/architecture/#core-components","title":"Core Components","text":""},{"location":"concepts/architecture/#1-pipeline-orchestrator","title":"1. Pipeline Orchestrator","text":"<p>Location: <code>docling_graph/pipeline.py</code></p> <p>The main entry point that orchestrates the entire conversion process:</p> <ul> <li>Loads and validates configuration</li> <li>Initializes components based on settings</li> <li>Manages resource lifecycle</li> <li>Coordinates data flow between components</li> </ul> <p>Key Function: <code>run_pipeline(config: PipelineConfig)</code></p>"},{"location":"concepts/architecture/#2-document-processor","title":"2. Document Processor","text":"<p>Location: <code>docling_graph/core/extractors/document_processor.py</code></p> <p>Handles document conversion using Docling:</p> <ul> <li>Converts documents to DoclingDocument format</li> <li>Extracts full markdown or per-page markdown</li> <li>Supports OCR and Vision pipelines</li> <li>Caches converted documents to avoid redundant processing</li> </ul> <p>Protocols: <code>DocumentProcessorProtocol</code></p>"},{"location":"concepts/architecture/#3-extraction-backends","title":"3. Extraction Backends","text":"<p>Location: <code>docling_graph/core/extractors/backends/</code></p> <p>Two backend families for structured extraction:</p>"},{"location":"concepts/architecture/#vlm-backend-vlm_backendpy","title":"VLM Backend (<code>vlm_backend.py</code>)","text":"<ul> <li>Uses Docling's NuExtract models</li> <li>Processes documents directly (images or PDFs)</li> <li>Ideal for structured documents with key-value pairs</li> <li>Local inference only</li> </ul>"},{"location":"concepts/architecture/#llm-backend-llm_backendpy","title":"LLM Backend (<code>llm_backend.py</code>)","text":"<ul> <li>Uses language models for extraction</li> <li>Processes markdown/text input</li> <li>Supports local (vLLM, Ollama) and remote (Mistral, OpenAI, Gemini, WatsonX) providers</li> <li>Includes chunking and consolidation strategies</li> </ul> <p>Protocols: <code>ExtractionBackendProtocol</code>, <code>TextExtractionBackendProtocol</code></p>"},{"location":"concepts/architecture/#4-llm-clients","title":"4. LLM Clients","text":"<p>Location: <code>docling_graph/llm_clients/</code></p> <p>Unified interface for multiple LLM providers:</p> <ul> <li>Base Client (<code>base.py</code>): Common interface and utilities</li> <li>Local Providers: vLLM, Ollama</li> <li>Remote Providers: Mistral, OpenAI, Gemini, WatsonX</li> </ul> <p>All clients implement <code>LLMClientProtocol</code> with: - <code>get_json_response()</code>: Execute LLM calls with JSON schema - <code>context_limit</code>: Token limit management</p>"},{"location":"concepts/architecture/#5-processing-strategies","title":"5. Processing Strategies","text":"<p>Location: <code>docling_graph/core/extractors/strategies/</code></p> <p>Two strategies for handling multi-page documents:</p>"},{"location":"concepts/architecture/#one-to-one-one_to_onepy","title":"One-to-One (<code>one_to_one.py</code>)","text":"<ul> <li>Processes each page independently</li> <li>Returns N Pydantic models (one per page)</li> <li>Useful for page-specific analysis</li> </ul>"},{"location":"concepts/architecture/#many-to-one-many_to_onepy","title":"Many-to-One (<code>many_to_one.py</code>)","text":"<ul> <li>Combines all pages into single extraction</li> <li>Returns 1 merged Pydantic model</li> <li>Supports chunking for large documents</li> <li>Optional LLM-based consolidation</li> </ul> <p>Protocol: <code>ExtractorProtocol</code></p>"},{"location":"concepts/architecture/#6-document-chunker","title":"6. Document Chunker","text":"<p>Location: <code>docling_graph/core/extractors/document_chunker.py</code></p> <p>Hybrid chunking strategy:</p> <ul> <li>Leverages Docling's document segmentation</li> <li>Applies semantic chunking with LLM context limits</li> <li>Preserves document structure (sections, tables, lists)</li> <li>Optimizes for token limits while maintaining coherence</li> </ul>"},{"location":"concepts/architecture/#7-graph-converter","title":"7. Graph Converter","text":"<p>Location: <code>docling_graph/core/converters/graph_converter.py</code></p> <p>Transforms validated Pydantic models into NetworkX graphs:</p> <ul> <li>Creates nodes from Pydantic models</li> <li>Generates stable node IDs using <code>graph_id_fields</code></li> <li>Establishes edges from <code>edge()</code> field definitions</li> <li>Supports reverse edge generation</li> <li>Validates graph structure</li> </ul> <p>Key Features: - Node ID Registry: Ensures deterministic, stable node IDs - Entity vs Component: Handles different deduplication strategies - Rich Metadata: Preserves all model data in node attributes</p>"},{"location":"concepts/architecture/#8-exporters","title":"8. Exporters","text":"<p>Location: <code>docling_graph/core/exporters/</code></p> <p>Multiple export formats for different use cases:</p> <ul> <li>CSV Exporter (<code>csv_exporter.py</code>): Neo4j-compatible nodes/edges CSV</li> <li>Cypher Exporter (<code>cypher_exporter.py</code>): Bulk import scripts</li> <li>JSON Exporter (<code>json_exporter.py</code>): General-purpose graph data</li> <li>Docling Exporter (<code>docling_exporter.py</code>): Original document + markdown</li> </ul>"},{"location":"concepts/architecture/#9-visualizers","title":"9. Visualizers","text":"<p>Location: <code>docling_graph/core/visualizers/</code></p> <p>Generate human-readable outputs:</p> <ul> <li>Interactive Visualizer (<code>interactive_visualizer.py</code>): Cytoscape.js HTML graphs</li> <li>Report Generator (<code>report_generator.py</code>): Detailed markdown reports</li> </ul>"},{"location":"concepts/architecture/#data-flow","title":"Data Flow","text":""},{"location":"concepts/architecture/#typical-execution-flow","title":"Typical Execution Flow","text":"<ol> <li>Configuration Loading</li> <li>Load <code>PipelineConfig</code> from dict or YAML</li> <li>Validate settings and resolve model configurations</li> <li> <p>Initialize output directories</p> </li> <li> <p>Template Loading</p> </li> <li>Import Pydantic template class</li> <li>Validate template structure</li> <li> <p>Extract graph metadata from model configs</p> </li> <li> <p>Document Processing</p> </li> <li>Convert source document using Docling</li> <li>Extract markdown (full or per-page)</li> <li> <p>Cache DoclingDocument for exports</p> </li> <li> <p>Extraction</p> </li> <li>Initialize appropriate backend (VLM or LLM)</li> <li>Apply processing strategy (one-to-one or many-to-one)</li> <li>For LLM: chunk if needed, batch process, consolidate</li> <li> <p>Validate extracted data against Pydantic schema</p> </li> <li> <p>Graph Construction</p> </li> <li>Convert validated models to NetworkX DiGraph</li> <li>Generate stable node IDs</li> <li>Create edges from relationship definitions</li> <li> <p>Validate graph structure</p> </li> <li> <p>Export</p> </li> <li>Export Docling document and markdown (if configured)</li> <li>Export graph to selected format (CSV/Cypher/JSON)</li> <li> <p>Generate visualizations (HTML, markdown reports)</p> </li> <li> <p>Cleanup</p> </li> <li>Release backend resources</li> <li>Clear GPU memory (if applicable)</li> <li>Close connections</li> </ol>"},{"location":"concepts/architecture/#protocol-based-design","title":"Protocol-Based Design","text":"<p>Docling Graph uses Python Protocols for type-safe, duck-typed interfaces:</p> <pre><code># Example: Backend Protocol\nclass ExtractionBackendProtocol(Protocol):\n    def extract_from_document(\n        self, source: str, template: Type[BaseModel]\n    ) -&gt; List[BaseModel]:\n        ...\n\n    def cleanup(self) -&gt; None:\n        ...\n</code></pre> <p>Benefits: - Type safety without rigid inheritance - Easy mocking for tests - Clear interface contracts - Flexible implementations</p>"},{"location":"concepts/architecture/#configuration-system","title":"Configuration System","text":"<p>Location: <code>docling_graph/config.py</code></p> <p>Type-safe configuration using Pydantic:</p> <pre><code>class PipelineConfig(BaseModel):\n    source: str\n    template: Union[str, Type[BaseModel]]\n    backend: Literal[\"llm\", \"vlm\"]\n    inference: Literal[\"local\", \"remote\"]\n    processing_mode: Literal[\"one-to-one\", \"many-to-one\"]\n    # ... additional settings\n</code></pre> <p>Key Features: - Single source of truth for defaults - Validation at configuration time - Easy programmatic and CLI usage - YAML export for persistence</p>"},{"location":"concepts/architecture/#extensibility-points","title":"Extensibility Points","text":""},{"location":"concepts/architecture/#adding-new-llm-providers","title":"Adding New LLM Providers","text":"<ol> <li>Create client class in <code>llm_clients/</code></li> <li>Implement <code>LLMClientProtocol</code></li> <li>Register in <code>llm_clients/__init__.py</code></li> <li>Update configuration templates</li> </ol>"},{"location":"concepts/architecture/#adding-new-export-formats","title":"Adding New Export Formats","text":"<ol> <li>Create exporter in <code>core/exporters/</code></li> <li>Inherit from <code>BaseExporter</code></li> <li>Implement <code>export()</code> method</li> <li>Register in pipeline</li> </ol>"},{"location":"concepts/architecture/#custom-processing-strategies","title":"Custom Processing Strategies","text":"<ol> <li>Create strategy in <code>core/extractors/strategies/</code></li> <li>Implement <code>ExtractorProtocol</code></li> <li>Register in <code>ExtractorFactory</code></li> </ol>"},{"location":"concepts/architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"concepts/architecture/#memory-management","title":"Memory Management","text":"<ul> <li>GPU Memory: Automatic cleanup after extraction</li> <li>Document Caching: Reuse converted documents</li> <li>Batch Processing: Configurable batch sizes for chunking</li> </ul>"},{"location":"concepts/architecture/#optimization-strategies","title":"Optimization Strategies","text":"<ul> <li>Chunking: Reduces memory footprint for large documents</li> <li>Lazy Loading: Import modules only when needed</li> <li>Resource Pooling: Reuse LLM clients across batches</li> </ul>"},{"location":"concepts/architecture/#error-handling","title":"Error Handling","text":"<p>The pipeline implements comprehensive error handling:</p> <ul> <li>Validation Errors: Caught at Pydantic model level</li> <li>Extraction Failures: Graceful degradation with logging</li> <li>Resource Cleanup: Guaranteed via try-finally blocks</li> <li>User Feedback: Rich console output with progress indicators</li> </ul>"},{"location":"concepts/architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Extraction Backends</li> <li>Understand Processing Strategies</li> <li>Explore Graph Construction</li> </ul>"},{"location":"concepts/configuration/","title":"Configuration System","text":"<p>Docling Graph uses a flexible, type-safe configuration system built on Pydantic. This document explains how to configure the pipeline for different use cases.</p>"},{"location":"concepts/configuration/#overview","title":"Overview","text":"<p>Configuration can be provided in three ways:</p> <ol> <li>Programmatic: Using <code>PipelineConfig</code> class</li> <li>YAML File: Using <code>config.yaml</code></li> <li>CLI Arguments: Command-line flags</li> </ol> <p>Location: <code>docling_graph/config.py</code></p>"},{"location":"concepts/configuration/#pipelineconfig-class","title":"PipelineConfig Class","text":""},{"location":"concepts/configuration/#basic-usage","title":"Basic Usage","text":"<pre><code>from docling_graph import PipelineConfig, run_pipeline\n\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    inference=\"remote\",\n    output_dir=\"outputs\"\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"concepts/configuration/#complete-configuration","title":"Complete Configuration","text":"<pre><code>config = PipelineConfig(\n    # Required\n    source=\"document.pdf\",\n    template=YourTemplate,\n\n    # Core Processing\n    backend=\"llm\",                    # \"llm\" or \"vlm\"\n    inference=\"local\",                # \"local\" or \"remote\"\n    processing_mode=\"many-to-one\",    # \"one-to-one\" or \"many-to-one\"\n\n    # Docling Settings\n    docling_config=\"ocr\",             # \"ocr\" or \"vision\"\n\n    # Model Selection\n    model_override=\"gpt-4-turbo\",     # Override default model\n    provider_override=\"openai\",       # Override default provider\n\n    # Extraction Settings\n    use_chunking=True,                # Enable document chunking\n    llm_consolidation=False,          # Use LLM for consolidation\n    max_batch_size=1,                 # Batch size for processing\n\n    # Export Settings\n    export_format=\"csv\",              # \"csv\" or \"cypher\"\n    export_docling=True,              # Export Docling document\n    export_docling_json=True,         # Export Docling JSON\n    export_markdown=True,             # Export markdown\n    export_per_page_markdown=False,   # Export per-page markdown\n\n    # Graph Settings\n    reverse_edges=False,              # Add reverse edges\n\n    # Output\n    output_dir=\"outputs\"              # Output directory\n)\n</code></pre>"},{"location":"concepts/configuration/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"concepts/configuration/#source-and-template","title":"Source and Template","text":"<pre><code># Source document\nsource: str | Path = \"document.pdf\"\n\n# Pydantic template (class or dotted path)\ntemplate: Type[BaseModel] | str = YourTemplate\n# or\ntemplate: str = \"module.path.YourTemplate\"\n</code></pre>"},{"location":"concepts/configuration/#backend-selection","title":"Backend Selection","text":"<pre><code># Backend type\nbackend: Literal[\"llm\", \"vlm\"] = \"llm\"\n\n# Inference location\ninference: Literal[\"local\", \"remote\"] = \"local\"\n\n# Note: VLM only supports local inference\n</code></pre>"},{"location":"concepts/configuration/#processing-mode","title":"Processing Mode","text":"<pre><code># How to handle multi-page documents\nprocessing_mode: Literal[\"one-to-one\", \"many-to-one\"] = \"many-to-one\"\n\n# one-to-one: Each page \u2192 separate model\n# many-to-one: All pages \u2192 single merged model\n</code></pre>"},{"location":"concepts/configuration/#docling-configuration","title":"Docling Configuration","text":"<pre><code># Docling pipeline type\ndocling_config: Literal[\"ocr\", \"vision\"] = \"ocr\"\n\n# ocr: Traditional OCR pipeline (most accurate for standard documents)\n# vision: Vision-Language Model pipeline (best for complex layouts)\n</code></pre>"},{"location":"concepts/configuration/#model-selection","title":"Model Selection","text":"<pre><code># Override default model\nmodel_override: str | None = \"gpt-4-turbo\"\n\n# Override default provider\nprovider_override: str | None = \"openai\"\n\n# Available providers:\n# Local: \"vllm\", \"ollama\"\n# Remote: \"mistral\", \"openai\", \"gemini\", \"watsonx\"\n</code></pre>"},{"location":"concepts/configuration/#extraction-settings","title":"Extraction Settings","text":"<pre><code># Enable hybrid chunking\nuse_chunking: bool = True\n\n# Use LLM for consolidation (many-to-one only)\nllm_consolidation: bool = False\n\n# Batch size for chunk processing\nmax_batch_size: int = 1\n</code></pre>"},{"location":"concepts/configuration/#export-settings","title":"Export Settings","text":"<pre><code># Primary export format\nexport_format: Literal[\"csv\", \"cypher\"] = \"csv\"\n\n# Docling exports\nexport_docling: bool = True\nexport_docling_json: bool = True\nexport_markdown: bool = True\nexport_per_page_markdown: bool = False\n</code></pre>"},{"location":"concepts/configuration/#graph-settings","title":"Graph Settings","text":"<pre><code># Add reverse edges for bidirectional traversal\nreverse_edges: bool = False\n</code></pre>"},{"location":"concepts/configuration/#output-settings","title":"Output Settings","text":"<pre><code># Output directory\noutput_dir: str | Path = \"outputs\"\n</code></pre>"},{"location":"concepts/configuration/#yaml-configuration","title":"YAML Configuration","text":""},{"location":"concepts/configuration/#configyaml-structure","title":"config.yaml Structure","text":"<pre><code># Default settings\ndefaults:\n  processing_mode: many-to-one\n  backend: llm\n  inference: local\n  export_format: csv\n\n# Docling pipeline configuration\ndocling:\n  pipeline: ocr  # ocr | vision\n  export:\n    docling_json: true\n    markdown: true\n    per_page_markdown: false\n\n# Model configurations\nmodels:\n  vlm:\n    local:\n      default_model: \"numind/NuExtract-2.0-8B\"\n      provider: \"docling\"\n\n  llm:\n    local:\n      default_model: \"ibm-granite/granite-4.0-1b\"\n      provider: \"vllm\"\n      providers:\n        vllm:\n          default_model: \"ibm-granite/granite-4.0-1b\"\n          base_url: \"http://localhost:8000/v1\"\n        ollama:\n          default_model: \"llama-3.1-8b\"\n\n    remote:\n      default_model: \"mistral-small-latest\"\n      provider: \"mistral\"\n      providers:\n        mistral:\n          default_model: \"mistral-small-latest\"\n        openai:\n          default_model: \"gpt-4-turbo\"\n        gemini:\n          default_model: \"gemini-2.5-flash\"\n\n# Output settings\noutput:\n  directory: \"outputs\"\n  create_visualizations: true\n  create_markdown: true\n</code></pre>"},{"location":"concepts/configuration/#using-yaml-config","title":"Using YAML Config","text":"<pre><code>import yaml\nfrom docling_graph import PipelineConfig, run_pipeline\n\n# Load YAML config\nwith open(\"config.yaml\") as f:\n    yaml_config = yaml.safe_load(f)\n\n# Create PipelineConfig\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    **yaml_config[\"defaults\"]\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"concepts/configuration/#cli-configuration","title":"CLI Configuration","text":""},{"location":"concepts/configuration/#command-line-arguments","title":"Command-Line Arguments","text":"<pre><code>docling-graph convert document.pdf \\\n    --template \"module.YourTemplate\" \\\n    --backend llm \\\n    --inference remote \\\n    --provider openai \\\n    --model gpt-4-turbo \\\n    --processing-mode many-to-one \\\n    --use-chunking \\\n    --no-llm-consolidation \\\n    --export-format csv \\\n    --output-dir outputs\n</code></pre>"},{"location":"concepts/configuration/#available-cli-flags","title":"Available CLI Flags","text":"<pre><code># Required\n--template TEXT              Pydantic template (dotted path)\n\n# Backend\n--backend [llm|vlm]         Extraction backend\n--inference [local|remote]  Inference location\n--provider TEXT             LLM/VLM provider\n--model TEXT                Model name/path\n\n# Processing\n--processing-mode [one-to-one|many-to-one]\n--docling-config [ocr|vision]\n--use-chunking / --no-chunking\n--llm-consolidation / --no-llm-consolidation\n\n# Export\n--export-format [csv|cypher]\n--export-docling / --no-export-docling\n--export-markdown / --no-export-markdown\n--per-page-markdown / --no-per-page-markdown\n\n# Graph\n--reverse-edges / --no-reverse-edges\n\n# Output\n--output-dir PATH           Output directory\n</code></pre>"},{"location":"concepts/configuration/#model-configuration","title":"Model Configuration","text":""},{"location":"concepts/configuration/#default-models","title":"Default Models","text":"<pre><code># VLM (local only)\nmodels.vlm.local.default_model = \"numind/NuExtract-2.0-8B\"\n\n# LLM (local)\nmodels.llm.local.default_model = \"ibm-granite/granite-4.0-1b\"\nmodels.llm.local.provider = \"vllm\"\n\n# LLM (remote)\nmodels.llm.remote.default_model = \"mistral-small-latest\"\nmodels.llm.remote.provider = \"mistral\"\n</code></pre>"},{"location":"concepts/configuration/#overriding-models","title":"Overriding Models","text":"<pre><code># Override in code\nconfig = PipelineConfig(\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"openai\",\n    model_override=\"gpt-4-turbo\"\n)\n\n# Override via CLI\ndocling-graph convert document.pdf \\\n    --provider openai \\\n    --model gpt-4-turbo\n</code></pre>"},{"location":"concepts/configuration/#provider-specific-models","title":"Provider-Specific Models","text":"<pre><code># In config.yaml\nmodels:\n  llm:\n    remote:\n      providers:\n        openai:\n          default_model: \"gpt-4-turbo\"\n        mistral:\n          default_model: \"mistral-large-latest\"\n        gemini:\n          default_model: \"gemini-2.5-flash\"\n</code></pre>"},{"location":"concepts/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"concepts/configuration/#api-keys","title":"API Keys","text":"<pre><code># OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Mistral\nexport MISTRAL_API_KEY=\"...\"\n\n# Google Gemini\nexport GEMINI_API_KEY=\"...\"\n\n# IBM WatsonX\nexport WATSONX_API_KEY=\"...\"\nexport WATSONX_PROJECT_ID=\"...\"\nexport WATSONX_URL=\"https://us-south.ml.cloud.ibm.com\"\n</code></pre>"},{"location":"concepts/configuration/#using-env-file","title":"Using .env File","text":"<pre><code># .env\nOPENAI_API_KEY=sk-...\nMISTRAL_API_KEY=...\nGEMINI_API_KEY=...\n</code></pre> <pre><code># Load environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# API keys are automatically picked up\nconfig = PipelineConfig(\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"openai\"\n)\n</code></pre>"},{"location":"concepts/configuration/#configuration-presets","title":"Configuration Presets","text":""},{"location":"concepts/configuration/#preset-1-fast-local-processing","title":"Preset 1: Fast Local Processing","text":"<pre><code>config = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"vlm\",\n    inference=\"local\",\n    processing_mode=\"one-to-one\",\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/configuration/#preset-2-high-quality-remote","title":"Preset 2: High-Quality Remote","text":"<pre><code>config = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"openai\",\n    model_override=\"gpt-4-turbo\",\n    processing_mode=\"many-to-one\",\n    use_chunking=True,\n    llm_consolidation=True,\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/configuration/#preset-3-cost-effective","title":"Preset 3: Cost-Effective","text":"<pre><code>config = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"mistral\",\n    model_override=\"mistral-small-latest\",\n    processing_mode=\"many-to-one\",\n    use_chunking=True,\n    llm_consolidation=False,  # Programmatic merge\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/configuration/#preset-4-privacy-focused","title":"Preset 4: Privacy-Focused","text":"<pre><code>config = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    inference=\"local\",\n    provider_override=\"ollama\",\n    model_override=\"llama-3.1-8b\",\n    processing_mode=\"many-to-one\",\n    use_chunking=True,\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/configuration/#validation","title":"Validation","text":""},{"location":"concepts/configuration/#automatic-validation","title":"Automatic Validation","text":"<p>PipelineConfig validates settings automatically:</p> <pre><code># This will raise ValidationError\nconfig = PipelineConfig(\n    backend=\"vlm\",\n    inference=\"remote\"  # Error: VLM only supports local\n)\n\n# ValidationError: VLM backend currently only supports local inference\n</code></pre>"},{"location":"concepts/configuration/#custom-validation","title":"Custom Validation","text":"<pre><code>from pydantic import ValidationError\n\ntry:\n    config = PipelineConfig(\n        source=\"document.pdf\",\n        template=\"invalid.path\",  # Invalid template path\n        backend=\"llm\"\n    )\nexcept ValidationError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"concepts/configuration/#best-practices","title":"Best Practices","text":""},{"location":"concepts/configuration/#1-use-type-safe-config","title":"1. Use Type-Safe Config","text":"<pre><code># Good: Type-safe\nconfig = PipelineConfig(\n    backend=\"llm\",\n    inference=\"remote\"\n)\n\n# Avoid: Dictionary (no validation)\nconfig_dict = {\n    \"backend\": \"llm\",\n    \"inference\": \"remote\"\n}\n</code></pre>"},{"location":"concepts/configuration/#2-separate-configs-by-environment","title":"2. Separate Configs by Environment","text":"<pre><code># development.py\ndev_config = PipelineConfig(\n    backend=\"llm\",\n    inference=\"local\",\n    provider_override=\"ollama\"\n)\n\n# production.py\nprod_config = PipelineConfig(\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"openai\",\n    model_override=\"gpt-4-turbo\"\n)\n</code></pre>"},{"location":"concepts/configuration/#3-use-environment-variables-for-secrets","title":"3. Use Environment Variables for Secrets","text":"<pre><code># Don't hardcode API keys\n# Bad:\nconfig = PipelineConfig(\n    provider_override=\"openai\",\n    api_key=\"sk-...\"  # Don't do this!\n)\n\n# Good: Use environment variables\n# API keys are automatically loaded from environment\nconfig = PipelineConfig(\n    provider_override=\"openai\"\n)\n</code></pre>"},{"location":"concepts/configuration/#4-document-your-configs","title":"4. Document Your Configs","text":"<pre><code># config.py\n\"\"\"\nConfiguration for invoice processing pipeline.\n\nUses OpenAI GPT-4 for high-quality extraction.\nProcesses documents in many-to-one mode with chunking.\n\"\"\"\n\nINVOICE_CONFIG = PipelineConfig(\n    template=Invoice,\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"openai\",\n    model_override=\"gpt-4-turbo\",\n    processing_mode=\"many-to-one\",\n    use_chunking=True,\n    output_dir=\"outputs/invoices\"\n)\n</code></pre>"},{"location":"concepts/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/configuration/#issue-configuration-not-applied","title":"Issue: Configuration Not Applied","text":"<p>Problem: Settings seem to be ignored</p> <p>Solution: Check precedence order: 1. CLI arguments (highest priority) 2. PipelineConfig parameters 3. YAML config 4. Defaults (lowest priority)</p>"},{"location":"concepts/configuration/#issue-model-not-found","title":"Issue: Model Not Found","text":"<p>Problem: \"Model not found\" error</p> <p>Solution: Verify model name and provider: <pre><code># Check available models in config.yaml\n# Ensure model name matches exactly\nconfig = PipelineConfig(\n    provider_override=\"openai\",\n    model_override=\"gpt-4-turbo\"  # Must match exactly\n)\n</code></pre></p>"},{"location":"concepts/configuration/#issue-api-key-not-found","title":"Issue: API Key Not Found","text":"<p>Problem: \"API key not set\" error</p> <p>Solution: Set environment variable: <pre><code>export OPENAI_API_KEY=\"your-key\"\n# or use .env file\n</code></pre></p>"},{"location":"concepts/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Extraction Backends</li> <li>Understand Processing Strategies</li> <li>Explore Architecture</li> </ul>"},{"location":"concepts/export-formats/","title":"Export Formats","text":"<p>Docling Graph supports multiple export formats to integrate with different downstream systems and use cases. This document explains each format and when to use it.</p>"},{"location":"concepts/export-formats/#overview","title":"Overview","text":"Format Purpose Best For CSV Neo4j admin import Database ingestion Cypher Bulk graph creation Neo4j scripting JSON General-purpose data API integration, archival Docling JSON Original document Document preservation Markdown Human-readable text Documentation, review HTML Interactive visualization Exploration, presentation"},{"location":"concepts/export-formats/#csv-export","title":"CSV Export","text":""},{"location":"concepts/export-formats/#overview_1","title":"Overview","text":"<p>Exports graph data as CSV files compatible with Neo4j's admin import tool.</p> <p>Location: <code>docling_graph/core/exporters/csv_exporter.py</code></p>"},{"location":"concepts/export-formats/#output-files","title":"Output Files","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 nodes.csv          # All nodes with properties\n\u2514\u2500\u2500 edges.csv          # All edges with properties\n</code></pre>"},{"location":"concepts/export-formats/#node-csv-format","title":"Node CSV Format","text":"<pre><code>:ID,name:string,age:int,email:string,:LABEL\nPerson_JohnDoe_1990-01-15,John Doe,34,john@example.com,Person\nPerson_JaneSmith_1985-06-20,Jane Smith,39,jane@example.com,Person\nOrganization_AcmeCorp,Acme Corp,,,Organization\n</code></pre> <p>Columns: - <code>:ID</code> - Unique node identifier - Property columns with type suffixes (<code>:string</code>, <code>:int</code>, <code>:float</code>, <code>:boolean</code>) - <code>:LABEL</code> - Node type/class</p>"},{"location":"concepts/export-formats/#edge-csv-format","title":"Edge CSV Format","text":"<pre><code>:START_ID,:END_ID,:TYPE\nDocument_INV001,Organization_AcmeCorp,ISSUED_BY\nDocument_INV001,Person_JohnDoe_1990-01-15,SENT_TO\n</code></pre> <p>Columns: - <code>:START_ID</code> - Source node ID - <code>:END_ID</code> - Target node ID - <code>:TYPE</code> - Edge label/relationship type</p>"},{"location":"concepts/export-formats/#configuration","title":"Configuration","text":"<pre><code>from docling_graph import PipelineConfig\n\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    export_format=\"csv\",  # Export as CSV\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/export-formats/#neo4j-import","title":"Neo4j Import","text":"<pre><code># Using Neo4j admin import tool\nneo4j-admin database import full \\\n    --nodes=outputs/nodes.csv \\\n    --relationships=outputs/edges.csv \\\n    --database=neo4j\n</code></pre>"},{"location":"concepts/export-formats/#advantages","title":"Advantages","text":"<p>\u2705 Fast Import: Optimized for bulk loading \u2705 Standard Format: Works with Neo4j admin tools \u2705 Type Safety: Explicit type annotations \u2705 Scalable: Handles large graphs efficiently  </p>"},{"location":"concepts/export-formats/#limitations","title":"Limitations","text":"<p>\u26a0\ufe0f Neo4j Specific: Primarily for Neo4j databases \u26a0\ufe0f Requires Admin: Needs Neo4j admin access \u26a0\ufe0f Offline Import: Database must be stopped  </p>"},{"location":"concepts/export-formats/#use-cases","title":"Use Cases","text":"<ul> <li>Initial database population</li> <li>Bulk data migration</li> <li>Large-scale graph imports</li> <li>Production deployments</li> </ul>"},{"location":"concepts/export-formats/#cypher-export","title":"Cypher Export","text":""},{"location":"concepts/export-formats/#overview_2","title":"Overview","text":"<p>Generates Cypher scripts for creating nodes and relationships in Neo4j.</p> <p>Location: <code>docling_graph/core/exporters/cypher_exporter.py</code></p>"},{"location":"concepts/export-formats/#output-file","title":"Output File","text":"<pre><code>output_dir/\n\u2514\u2500\u2500 document_graph.cypher\n</code></pre>"},{"location":"concepts/export-formats/#cypher-script-format","title":"Cypher Script Format","text":"<pre><code>// Create nodes\nCREATE (:Person {\n    id: 'Person_JohnDoe_1990-01-15',\n    name: 'John Doe',\n    age: 34,\n    email: 'john@example.com'\n});\n\nCREATE (:Organization {\n    id: 'Organization_AcmeCorp',\n    name: 'Acme Corp'\n});\n\n// Create relationships\nMATCH (a:Document {id: 'Document_INV001'})\nMATCH (b:Organization {id: 'Organization_AcmeCorp'})\nCREATE (a)-[:ISSUED_BY]-&gt;(b);\n\nMATCH (a:Document {id: 'Document_INV001'})\nMATCH (b:Person {id: 'Person_JohnDoe_1990-01-15'})\nCREATE (a)-[:SENT_TO]-&gt;(b);\n</code></pre>"},{"location":"concepts/export-formats/#configuration_1","title":"Configuration","text":"<pre><code>config = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    export_format=\"cypher\",  # Export as Cypher\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/export-formats/#neo4j-execution","title":"Neo4j Execution","text":"<pre><code># Using cypher-shell\ncat outputs/document_graph.cypher | cypher-shell -u neo4j -p password\n\n# Or in Neo4j Browser\n# Copy and paste the script content\n</code></pre>"},{"location":"concepts/export-formats/#advantages_1","title":"Advantages","text":"<p>\u2705 Online Import: Database can remain running \u2705 Flexible: Easy to modify scripts \u2705 Readable: Human-readable format \u2705 Incremental: Can add to existing data  </p>"},{"location":"concepts/export-formats/#limitations_1","title":"Limitations","text":"<p>\u26a0\ufe0f Slower: Less efficient than CSV import \u26a0\ufe0f Memory: Large scripts may cause issues \u26a0\ufe0f Transaction Size: May need to split large scripts  </p>"},{"location":"concepts/export-formats/#use-cases_1","title":"Use Cases","text":"<ul> <li>Development and testing</li> <li>Incremental updates</li> <li>Small to medium graphs</li> <li>When database downtime is not acceptable</li> </ul>"},{"location":"concepts/export-formats/#json-export","title":"JSON Export","text":""},{"location":"concepts/export-formats/#overview_3","title":"Overview","text":"<p>Exports graph data as JSON for general-purpose use.</p> <p>Location: <code>docling_graph/core/exporters/json_exporter.py</code></p>"},{"location":"concepts/export-formats/#output-file_1","title":"Output File","text":"<pre><code>output_dir/\n\u2514\u2500\u2500 document_graph.json\n</code></pre>"},{"location":"concepts/export-formats/#json-format","title":"JSON Format","text":"<pre><code>{\n  \"nodes\": [\n    {\n      \"id\": \"Person_JohnDoe_1990-01-15\",\n      \"type\": \"Person\",\n      \"properties\": {\n        \"name\": \"John Doe\",\n        \"age\": 34,\n        \"email\": \"john@example.com\",\n        \"date_of_birth\": \"1990-01-15\"\n      }\n    },\n    {\n      \"id\": \"Organization_AcmeCorp\",\n      \"type\": \"Organization\",\n      \"properties\": {\n        \"name\": \"Acme Corp\",\n        \"tax_id\": \"123456789\"\n      }\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"Document_INV001\",\n      \"target\": \"Organization_AcmeCorp\",\n      \"label\": \"ISSUED_BY\",\n      \"properties\": {}\n    },\n    {\n      \"source\": \"Document_INV001\",\n      \"target\": \"Person_JohnDoe_1990-01-15\",\n      \"label\": \"SENT_TO\",\n      \"properties\": {}\n    }\n  ],\n  \"metadata\": {\n    \"node_count\": 3,\n    \"edge_count\": 2,\n    \"node_types\": {\n      \"Person\": 1,\n      \"Organization\": 1,\n      \"Document\": 1\n    },\n    \"edge_types\": {\n      \"ISSUED_BY\": 1,\n      \"SENT_TO\": 1\n    }\n  }\n}\n</code></pre>"},{"location":"concepts/export-formats/#configuration_2","title":"Configuration","text":"<pre><code># JSON is always exported alongside other formats\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    export_format=\"csv\",  # Primary format\n    output_dir=\"outputs\"\n    # JSON is automatically created\n)\n</code></pre>"},{"location":"concepts/export-formats/#advantages_2","title":"Advantages","text":"<p>\u2705 Universal: Works with any system \u2705 Structured: Easy to parse and process \u2705 Complete: Includes all graph data \u2705 Portable: Platform-independent  </p>"},{"location":"concepts/export-formats/#use-cases_2","title":"Use Cases","text":"<ul> <li>API integration</li> <li>Data archival</li> <li>Custom processing pipelines</li> <li>Cross-platform data exchange</li> <li>Web applications</li> </ul>"},{"location":"concepts/export-formats/#docling-export","title":"Docling Export","text":""},{"location":"concepts/export-formats/#overview_4","title":"Overview","text":"<p>Exports the original Docling document structure and markdown representations.</p> <p>Location: <code>docling_graph/core/exporters/docling_exporter.py</code></p>"},{"location":"concepts/export-formats/#output-files_1","title":"Output Files","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 document.json           # Full Docling document structure\n\u251c\u2500\u2500 document.md            # Complete markdown\n\u2514\u2500\u2500 document_pages/        # Per-page markdown (optional)\n    \u251c\u2500\u2500 page_1.md\n    \u251c\u2500\u2500 page_2.md\n    \u2514\u2500\u2500 page_3.md\n</code></pre>"},{"location":"concepts/export-formats/#docling-json-format","title":"Docling JSON Format","text":"<p>Contains complete document structure: - Document metadata - Layout information - Tables and figures - Text content - Page boundaries</p>"},{"location":"concepts/export-formats/#configuration_3","title":"Configuration","text":"<pre><code>config = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    export_docling=True,              # Enable Docling export\n    export_docling_json=True,         # Export JSON structure\n    export_markdown=True,             # Export full markdown\n    export_per_page_markdown=False,   # Export per-page markdown\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/export-formats/#advantages_3","title":"Advantages","text":"<p>\u2705 Complete: Preserves all document information \u2705 Layout: Maintains document structure \u2705 Tables: Preserves table formatting \u2705 Figures: Includes figure references  </p>"},{"location":"concepts/export-formats/#use-cases_3","title":"Use Cases","text":"<ul> <li>Document archival</li> <li>Layout analysis</li> <li>Table extraction</li> <li>Multi-format conversion</li> <li>Document comparison</li> </ul>"},{"location":"concepts/export-formats/#html-visualization","title":"HTML Visualization","text":""},{"location":"concepts/export-formats/#overview_5","title":"Overview","text":"<p>Generates interactive HTML visualization using Cytoscape.js.</p> <p>Location: <code>docling_graph/core/visualizers/interactive_visualizer.py</code></p>"},{"location":"concepts/export-formats/#output-file_2","title":"Output File","text":"<pre><code>output_dir/\n\u2514\u2500\u2500 document_graph.html\n</code></pre>"},{"location":"concepts/export-formats/#features","title":"Features","text":"<ul> <li>Interactive: Click, drag, zoom</li> <li>Node Details: Click nodes to see properties</li> <li>Edge Labels: Hover to see relationships</li> <li>Search: Find specific nodes</li> <li>Layout: Automatic graph layout</li> <li>Styling: Color-coded by node type</li> </ul>"},{"location":"concepts/export-formats/#configuration_4","title":"Configuration","text":"<pre><code># HTML visualization is always generated\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    output_dir=\"outputs\"\n    # HTML is automatically created\n)\n</code></pre>"},{"location":"concepts/export-formats/#usage","title":"Usage","text":"<pre><code># Open in browser\nopen outputs/document_graph.html\n\n# Or serve with Python\npython -m http.server 8000\n# Navigate to http://localhost:8000/outputs/document_graph.html\n</code></pre>"},{"location":"concepts/export-formats/#advantages_4","title":"Advantages","text":"<p>\u2705 Visual: Easy to understand graph structure \u2705 Interactive: Explore relationships dynamically \u2705 Standalone: No dependencies, works offline \u2705 Shareable: Easy to share with stakeholders  </p>"},{"location":"concepts/export-formats/#use-cases_4","title":"Use Cases","text":"<ul> <li>Graph exploration</li> <li>Presentations</li> <li>Documentation</li> <li>Quality assurance</li> <li>Stakeholder communication</li> </ul>"},{"location":"concepts/export-formats/#markdown-report","title":"Markdown Report","text":""},{"location":"concepts/export-formats/#overview_6","title":"Overview","text":"<p>Generates detailed markdown reports with node and edge information.</p> <p>Location: <code>docling_graph/core/visualizers/report_generator.py</code></p>"},{"location":"concepts/export-formats/#output-files_2","title":"Output Files","text":"<pre><code>output_dir/\n\u2514\u2500\u2500 document_report/\n    \u251c\u2500\u2500 index.md           # Overview and statistics\n    \u251c\u2500\u2500 nodes.md          # All nodes with properties\n    \u2514\u2500\u2500 edges.md          # All edges with relationships\n</code></pre>"},{"location":"concepts/export-formats/#report-structure","title":"Report Structure","text":""},{"location":"concepts/export-formats/#indexmd","title":"index.md","text":"<pre><code># Graph Report\n\n## Statistics\n- Total Nodes: 10\n- Total Edges: 15\n- Node Types: Person (3), Organization (2), Document (5)\n- Edge Types: ISSUED_BY (5), SENT_TO (5), HAS_AUTHOR (5)\n\n## Source Information\n- Source Models: 1\n- Processing Mode: many-to-one\n- Extraction Backend: llm\n</code></pre>"},{"location":"concepts/export-formats/#nodesmd","title":"nodes.md","text":"<pre><code># Nodes\n\n## Person_JohnDoe_1990-01-15\n**Type**: Person\n\n**Properties**:\n- name: John Doe\n- age: 34\n- email: john@example.com\n- date_of_birth: 1990-01-15\n\n---\n\n## Organization_AcmeCorp\n**Type**: Organization\n\n**Properties**:\n- name: Acme Corp\n- tax_id: 123456789\n</code></pre>"},{"location":"concepts/export-formats/#edgesmd","title":"edges.md","text":"<pre><code># Edges\n\n## ISSUED_BY\n- Document_INV001 \u2192 Organization_AcmeCorp\n- Document_INV002 \u2192 Organization_AcmeCorp\n\n## SENT_TO\n- Document_INV001 \u2192 Person_JohnDoe_1990-01-15\n- Document_INV002 \u2192 Person_JaneSmith_1985-06-20\n</code></pre>"},{"location":"concepts/export-formats/#advantages_5","title":"Advantages","text":"<p>\u2705 Readable: Human-friendly format \u2705 Searchable: Easy to grep and search \u2705 Versionable: Works with Git \u2705 Detailed: Complete property information  </p>"},{"location":"concepts/export-formats/#use-cases_5","title":"Use Cases","text":"<ul> <li>Documentation</li> <li>Code reviews</li> <li>Audit trails</li> <li>Knowledge base</li> <li>Team collaboration</li> </ul>"},{"location":"concepts/export-formats/#choosing-export-formats","title":"Choosing Export Formats","text":""},{"location":"concepts/export-formats/#decision-matrix","title":"Decision Matrix","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Use Case                                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502  Production Database    \u2192  CSV (bulk import)           \u2502\n\u2502  Development/Testing    \u2192  Cypher (incremental)        \u2502\n\u2502  API Integration        \u2192  JSON                        \u2502\n\u2502  Exploration            \u2192  HTML Visualization          \u2502\n\u2502  Documentation          \u2192  Markdown Report             \u2502\n\u2502  Archival               \u2192  Docling JSON + JSON         \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/export-formats/#multiple-formats","title":"Multiple Formats","text":"<p>You can use multiple formats simultaneously:</p> <pre><code>config = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    export_format=\"csv\",              # Primary: CSV for Neo4j\n    export_docling=True,              # Also: Docling document\n    export_markdown=True,             # Also: Markdown\n    output_dir=\"outputs\"\n    # JSON and HTML are always created\n)\n</code></pre>"},{"location":"concepts/export-formats/#best-practices","title":"Best Practices","text":""},{"location":"concepts/export-formats/#for-production","title":"For Production","text":"<ol> <li>Use CSV: Fastest for bulk imports</li> <li>Validate First: Test with small datasets</li> <li>Backup: Keep original exports</li> <li>Version: Track export versions</li> </ol>"},{"location":"concepts/export-formats/#for-development","title":"For Development","text":"<ol> <li>Use Cypher: Easier to modify and test</li> <li>Use HTML: Visual verification</li> <li>Use Markdown: Documentation</li> <li>Keep JSON: Debugging and analysis</li> </ol>"},{"location":"concepts/export-formats/#for-integration","title":"For Integration","text":"<ol> <li>Use JSON: Universal format</li> <li>Document Schema: Provide JSON schema</li> <li>Version API: Track format changes</li> <li>Test Parsing: Validate with consumers</li> </ol>"},{"location":"concepts/export-formats/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/export-formats/#csv-import-fails","title":"CSV Import Fails","text":"<p>Problem: Neo4j import errors</p> <p>Solutions: - Check CSV format matches Neo4j version - Verify node IDs are unique - Ensure all referenced nodes exist - Check for special characters in data</p>"},{"location":"concepts/export-formats/#cypher-script-too-large","title":"Cypher Script Too Large","text":"<p>Problem: Memory errors or timeouts</p> <p>Solutions: - Split into smaller batches - Use CSV import instead - Increase Neo4j memory settings - Use PERIODIC COMMIT in Cypher</p>"},{"location":"concepts/export-formats/#json-too-large","title":"JSON Too Large","text":"<p>Problem: File size issues</p> <p>Solutions: - Stream processing instead of loading all - Compress JSON files - Split into multiple files - Use database export instead</p>"},{"location":"concepts/export-formats/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Graph Construction</li> <li>Understand Configuration System</li> <li>Explore Architecture</li> </ul>"},{"location":"concepts/extraction-backends/","title":"Extraction Backends","text":"<p>Docling Graph supports two families of extraction backends: Vision-Language Models (VLM) and Large Language Models (LLM). Each has distinct characteristics, strengths, and ideal use cases.</p>"},{"location":"concepts/extraction-backends/#overview","title":"Overview","text":"Aspect VLM Backend LLM Backend Input Raw documents (PDF, images) Markdown/text Processing Direct visual understanding Text-based extraction Inference Local only Local or remote Best For Structured forms, key-value pairs Complex documents, narratives Speed Fast for small documents Varies by provider Context Limited to page/document Supports chunking strategies"},{"location":"concepts/extraction-backends/#vlm-backend","title":"VLM Backend","text":""},{"location":"concepts/extraction-backends/#overview_1","title":"Overview","text":"<p>The VLM backend uses Docling's NuExtract models to extract structured information directly from document images or PDFs.</p> <p>Location: <code>docling_graph/core/extractors/backends/vlm_backend.py</code></p>"},{"location":"concepts/extraction-backends/#how-it-works","title":"How It Works","text":"<ol> <li>Direct Processing: Processes documents without markdown conversion</li> <li>Visual Understanding: Leverages vision-language models to understand layout and content</li> <li>Schema-Guided: Uses Pydantic schema to guide extraction</li> <li>Page-Level: Processes one page at a time</li> </ol>"},{"location":"concepts/extraction-backends/#supported-models","title":"Supported Models","text":"<ul> <li><code>numind/NuExtract-2.0-8B</code> (default, more accurate)</li> <li><code>numind/NuExtract-2.0-2B</code> (faster, less accurate)</li> </ul>"},{"location":"concepts/extraction-backends/#configuration","title":"Configuration","text":"<pre><code>from docling_graph import PipelineConfig\n\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"vlm\",           # Use VLM backend\n    inference=\"local\",       # Only local supported\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/extraction-backends/#advantages","title":"Advantages","text":"<p>\u2705 Fast for Small Documents: Efficient for single-page or few-page documents \u2705 No Markdown Conversion: Direct visual processing \u2705 Good for Forms: Excellent at extracting key-value pairs from structured layouts \u2705 Local Inference: No API costs or data privacy concerns  </p>"},{"location":"concepts/extraction-backends/#limitations","title":"Limitations","text":"<p>\u26a0\ufe0f Local Only: No remote inference option \u26a0\ufe0f Limited Context: Processes pages independently \u26a0\ufe0f Structured Documents: Best for forms, not complex narratives \u26a0\ufe0f GPU Required: Needs GPU for reasonable performance  </p>"},{"location":"concepts/extraction-backends/#ideal-use-cases","title":"Ideal Use Cases","text":"<ul> <li>ID Cards: Extracting name, DOB, ID number from identity documents</li> <li>Invoices: Structured invoice data with clear fields</li> <li>Forms: Application forms, registration documents</li> <li>Receipts: Simple receipt data extraction</li> <li>Certificates: Structured certificate information</li> </ul>"},{"location":"concepts/extraction-backends/#example","title":"Example","text":"<pre><code>from docling_graph import run_pipeline, PipelineConfig\nfrom templates.id_card import IDCard\n\nconfig = PipelineConfig(\n    source=\"id_card.jpg\",\n    template=IDCard,\n    backend=\"vlm\",\n    processing_mode=\"one-to-one\",  # Process each page separately\n    output_dir=\"outputs/id_cards\"\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"concepts/extraction-backends/#llm-backend","title":"LLM Backend","text":""},{"location":"concepts/extraction-backends/#overview_2","title":"Overview","text":"<p>The LLM backend uses large language models to extract structured information from markdown/text representations of documents.</p> <p>Location: <code>docling_graph/core/extractors/backends/llm_backend.py</code></p>"},{"location":"concepts/extraction-backends/#how-it-works_1","title":"How It Works","text":"<ol> <li>Markdown Conversion: Docling converts document to markdown first</li> <li>Text Processing: LLM processes markdown content</li> <li>Schema-Guided: Uses Pydantic schema with detailed field descriptions</li> <li>Chunking Support: Can split large documents into manageable chunks</li> <li>Consolidation: Merges extracted data from multiple chunks</li> </ol>"},{"location":"concepts/extraction-backends/#supported-providers","title":"Supported Providers","text":""},{"location":"concepts/extraction-backends/#local-providers","title":"Local Providers","text":"<ul> <li>vLLM: High-performance local inference server</li> <li>Requires GPU</li> <li>Excellent for batch processing</li> <li> <p>Default model: <code>ibm-granite/granite-4.0-1b</code></p> </li> <li> <p>Ollama: Easy local model management</p> </li> <li>CPU or GPU</li> <li>Simple setup</li> <li>Default model: <code>llama-3.1-8b</code></li> </ul>"},{"location":"concepts/extraction-backends/#remote-providers","title":"Remote Providers","text":"<ul> <li>Mistral AI: Fast, cost-effective</li> <li>Default model: <code>mistral-small-latest</code></li> <li> <p>Good balance of speed and quality</p> </li> <li> <p>OpenAI: High quality, widely used</p> </li> <li>Default model: <code>gpt-4-turbo</code></li> <li> <p>Excellent for complex extraction</p> </li> <li> <p>Google Gemini: Competitive pricing</p> </li> <li>Default model: <code>gemini-2.5-flash</code></li> <li> <p>Fast inference</p> </li> <li> <p>IBM WatsonX: Enterprise-grade</p> </li> <li>Granite, Llama, Mixtral models</li> <li>On-premises deployment options</li> </ul>"},{"location":"concepts/extraction-backends/#configuration_1","title":"Configuration","text":""},{"location":"concepts/extraction-backends/#local-inference-vllm","title":"Local Inference (vLLM)","text":"<pre><code>config = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    inference=\"local\",\n    provider_override=\"vllm\",\n    model_override=\"ibm-granite/granite-4.0-1b\",\n    use_chunking=True,\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/extraction-backends/#remote-inference-openai","title":"Remote Inference (OpenAI)","text":"<pre><code>config = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"openai\",\n    model_override=\"gpt-4-turbo\",\n    use_chunking=True,\n    llm_consolidation=True,  # Use LLM to merge chunks\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/extraction-backends/#advantages_1","title":"Advantages","text":"<p>\u2705 Complex Documents: Handles narratives, research papers, reports \u2705 Large Context: Supports chunking for documents of any size \u2705 Flexible Inference: Local or remote options \u2705 Multiple Providers: Choose based on cost, speed, quality \u2705 Rich Descriptions: Leverages detailed field descriptions for better extraction  </p>"},{"location":"concepts/extraction-backends/#limitations_1","title":"Limitations","text":"<p>\u26a0\ufe0f Requires Markdown: Needs Docling conversion first \u26a0\ufe0f API Costs: Remote inference incurs costs \u26a0\ufe0f Slower: Generally slower than VLM for small documents \u26a0\ufe0f Context Limits: Must chunk very large documents  </p>"},{"location":"concepts/extraction-backends/#ideal-use-cases_1","title":"Ideal Use Cases","text":"<ul> <li>Research Papers: Extracting methodology, results, conclusions</li> <li>Legal Documents: Complex contracts with nested clauses</li> <li>Technical Reports: Multi-section documents with relationships</li> <li>Insurance Policies: Detailed coverage terms and conditions</li> <li>Medical Records: Narrative clinical notes</li> </ul>"},{"location":"concepts/extraction-backends/#chunking-strategy","title":"Chunking Strategy","text":"<p>For large documents, the LLM backend uses a hybrid chunking approach:</p> <pre><code>config = PipelineConfig(\n    source=\"large_document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    use_chunking=True,           # Enable chunking\n    llm_consolidation=False,     # Programmatic merge (faster)\n    processing_mode=\"many-to-one\",\n    output_dir=\"outputs\"\n)\n</code></pre> <p>Chunking Process:</p> <ol> <li>Docling segments document (sections, tables, lists)</li> <li>Semantic chunking respects document structure</li> <li>Each chunk processed independently</li> <li>Results merged programmatically or via LLM</li> </ol> <p>Consolidation Options:</p> <ul> <li><code>llm_consolidation=False</code>: Fast programmatic merge (recommended)</li> <li><code>llm_consolidation=True</code>: LLM-based merge (more intelligent, slower)</li> </ul>"},{"location":"concepts/extraction-backends/#example-research-paper","title":"Example: Research Paper","text":"<pre><code>from docling_graph import run_pipeline, PipelineConfig\nfrom templates.research import ResearchPaper\n\nconfig = PipelineConfig(\n    source=\"research_paper.pdf\",\n    template=ResearchPaper,\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"mistral\",\n    model_override=\"mistral-medium-latest\",\n    processing_mode=\"many-to-one\",\n    use_chunking=True,\n    llm_consolidation=False,\n    output_dir=\"outputs/research\"\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"concepts/extraction-backends/#choosing-the-right-backend","title":"Choosing the Right Backend","text":""},{"location":"concepts/extraction-backends/#decision-matrix","title":"Decision Matrix","text":""},{"location":"concepts/extraction-backends/#document-type","title":"Document Type","text":"Document Type Examples Recommended Backend Structured Forms ID cards, invoices VLM Complex Narratives Research, reports LLM Mixed Content Policies, contracts LLM"},{"location":"concepts/extraction-backends/#document-size","title":"Document Size","text":"Size Recommendation Single Page VLM Few Pages (2\u20135) VLM or LLM Many Pages (5+) LLM (with chunking)"},{"location":"concepts/extraction-backends/#infrastructure-constraints","title":"Infrastructure Constraints","text":"Constraint Recommended Backend GPU Available VLM or Local LLM No GPU Remote LLM Privacy Concerns Local VLM or LLM Cost Sensitive Local or Mistral"},{"location":"concepts/extraction-backends/#quick-selection-guide","title":"Quick Selection Guide","text":"<p>Use VLM when: - Document is 1-3 pages - Content is structured (forms, tables) - You have GPU available - You need fast processing - Privacy is critical (local only)</p> <p>Use LLM when: - Document is complex or narrative - Document is large (5+ pages) - You need flexible inference options - Content requires deep understanding - You want to leverage remote APIs</p>"},{"location":"concepts/extraction-backends/#performance-comparison","title":"Performance Comparison","text":""},{"location":"concepts/extraction-backends/#speed","title":"Speed","text":"Backend Small Doc (1-2 pages) Large Doc (10+ pages) VLM \u26a1 Fast (seconds) \u26a0\ufe0f Slow (page-by-page) LLM (Local) \ud83d\udc22 Moderate (10-30s) \ud83d\udc22 Moderate (chunked) LLM (Remote) \u26a1 Fast (5-15s) \u26a1 Fast (parallel chunks)"},{"location":"concepts/extraction-backends/#accuracy","title":"Accuracy","text":"Backend Structured Forms Complex Narratives VLM \u2b50\u2b50\u2b50\u2b50\u2b50 Excellent \u2b50\u2b50\u2b50 Good LLM \u2b50\u2b50\u2b50\u2b50 Very Good \u2b50\u2b50\u2b50\u2b50\u2b50 Excellent"},{"location":"concepts/extraction-backends/#cost","title":"Cost","text":"Backend Setup Cost Runtime Cost VLM GPU required Free LLM (Local) GPU recommended Free LLM (Remote) None API charges"},{"location":"concepts/extraction-backends/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"concepts/extraction-backends/#custom-model-selection","title":"Custom Model Selection","text":"<pre><code># Override default models\nconfig = PipelineConfig(\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"openai\",\n    model_override=\"gpt-4o\",  # Use specific model\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/extraction-backends/#provider-specific-settings","title":"Provider-Specific Settings","text":"<pre><code># config.yaml\nmodels:\n  llm:\n    remote:\n      providers:\n        openai:\n          default_model: \"gpt-4-turbo\"\n        mistral:\n          default_model: \"mistral-large-latest\"\n</code></pre>"},{"location":"concepts/extraction-backends/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/extraction-backends/#vlm-issues","title":"VLM Issues","text":"<p>Problem: Out of memory errors Solution: Reduce batch size or use smaller model (2B instead of 8B)</p> <p>Problem: Slow processing Solution: Ensure GPU is available and CUDA is properly configured</p>"},{"location":"concepts/extraction-backends/#llm-issues","title":"LLM Issues","text":"<p>Problem: Context length exceeded Solution: Enable chunking with <code>use_chunking=True</code></p> <p>Problem: API rate limits Solution: Add delays between requests or use local inference</p> <p>Problem: Poor extraction quality Solution: Improve Pydantic template descriptions and examples</p>"},{"location":"concepts/extraction-backends/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Processing Strategies</li> <li>Understand Pydantic Templates</li> <li>Explore Configuration System</li> </ul>"},{"location":"concepts/graph-construction/","title":"Graph Construction","text":"<p>This document explains how Docling Graph converts validated Pydantic models into NetworkX directed graphs with semantic relationships.</p>"},{"location":"concepts/graph-construction/#overview","title":"Overview","text":"<p>Graph construction is the process of transforming structured Pydantic objects into a knowledge graph where:</p> <ul> <li>Nodes represent entities and components</li> <li>Edges represent relationships between nodes</li> <li>Attributes store rich metadata on nodes and edges</li> </ul> <p>Location: <code>docling_graph/core/converters/graph_converter.py</code></p>"},{"location":"concepts/graph-construction/#core-concepts","title":"Core Concepts","text":""},{"location":"concepts/graph-construction/#nodes","title":"Nodes","text":"<p>Every Pydantic model instance becomes a node in the graph:</p> <pre><code>class Person(BaseModel):\n    model_config = ConfigDict(\n        is_entity=True,\n        graph_id_fields=[\"name\", \"date_of_birth\"]\n    )\n    name: str\n    date_of_birth: str\n    email: str\n\n# Creates node:\n# ID: \"Person_JohnDoe_1990-01-15\"\n# Attributes: {name: \"John Doe\", date_of_birth: \"1990-01-15\", email: \"john@example.com\"}\n</code></pre>"},{"location":"concepts/graph-construction/#edges","title":"Edges","text":"<p>Relationships defined with the <code>edge()</code> helper become graph edges:</p> <pre><code>class Document(BaseModel):\n    issued_by: Organization = edge(\n        label=\"ISSUED_BY\",\n        description=\"Organization that issued this document\"\n    )\n\n# Creates edge:\n# Source: Document node\n# Target: Organization node\n# Label: \"ISSUED_BY\"\n</code></pre>"},{"location":"concepts/graph-construction/#node-ids","title":"Node IDs","text":"<p>Node IDs are generated using two strategies:</p>"},{"location":"concepts/graph-construction/#1-entity-nodes-with-graph_id_fields","title":"1. Entity Nodes (with graph_id_fields)","text":"<pre><code>class Person(BaseModel):\n    model_config = ConfigDict(\n        graph_id_fields=[\"first_name\", \"last_name\", \"dob\"]\n    )\n    first_name: str = \"John\"\n    last_name: str = \"Doe\"\n    dob: str = \"1990-01-15\"\n\n# Generated ID: \"Person_John_Doe_1990-01-15\"\n# Format: {ClassName}_{field1}_{field2}_{field3}\n</code></pre>"},{"location":"concepts/graph-construction/#2-component-nodes-content-based","title":"2. Component Nodes (content-based)","text":"<pre><code>class Address(BaseModel):\n    model_config = ConfigDict(is_entity=False)\n    street: str = \"123 Main St\"\n    city: str = \"Boston\"\n\n# Generated ID: hash of all field values\n# Format: {ClassName}_{content_hash}\n# Same content \u2192 Same ID (deduplication)\n</code></pre>"},{"location":"concepts/graph-construction/#graph-converter","title":"Graph Converter","text":""},{"location":"concepts/graph-construction/#basic-usage","title":"Basic Usage","text":"<pre><code>from docling_graph.core import GraphConverter\n\n# Create converter\nconverter = GraphConverter(\n    add_reverse_edges=False,\n    validate_graph=True\n)\n\n# Convert models to graph\nmodels = [person1, person2, organization]\ngraph, metadata = converter.pydantic_list_to_graph(models)\n\n# Result:\n# - graph: NetworkX DiGraph\n# - metadata: GraphMetadata with statistics\n</code></pre>"},{"location":"concepts/graph-construction/#configuration-options","title":"Configuration Options","text":"<pre><code>class GraphConverter:\n    def __init__(\n        self,\n        add_reverse_edges: bool = False,  # Add bidirectional edges\n        validate_graph: bool = True,      # Validate structure\n        registry: NodeIDRegistry = None   # Shared ID registry\n    ):\n        ...\n</code></pre>"},{"location":"concepts/graph-construction/#add_reverse_edges","title":"add_reverse_edges","text":"<p>Creates bidirectional relationships:</p> <pre><code># Original edge\nDocument --ISSUED_BY--&gt; Organization\n\n# With add_reverse_edges=True\nDocument --ISSUED_BY--&gt; Organization\nDocument &lt;--ISSUES-- Organization\n</code></pre>"},{"location":"concepts/graph-construction/#validate_graph","title":"validate_graph","text":"<p>Performs structural validation: - Checks for orphaned nodes - Validates edge connectivity - Ensures ID uniqueness</p>"},{"location":"concepts/graph-construction/#registry","title":"registry","text":"<p>Shared registry for deterministic IDs across batches:</p> <pre><code># Create shared registry\nregistry = NodeIDRegistry()\n\n# Use in multiple conversions\nconverter1 = GraphConverter(registry=registry)\nconverter2 = GraphConverter(registry=registry)\n\n# Same entities get same IDs across conversions\n</code></pre>"},{"location":"concepts/graph-construction/#node-id-registry","title":"Node ID Registry","text":"<p>Location: <code>docling_graph/core/converters/node_id_registry.py</code></p> <p>The registry ensures stable, deterministic node IDs:</p>"},{"location":"concepts/graph-construction/#features","title":"Features","text":"<ul> <li>Deterministic: Same input \u2192 same ID</li> <li>Collision Detection: Prevents ID conflicts</li> <li>Cross-Batch Consistency: Maintains IDs across multiple extractions</li> <li>Type-Safe: Tracks node types and metadata</li> </ul>"},{"location":"concepts/graph-construction/#usage","title":"Usage","text":"<pre><code>from docling_graph.core.converters import NodeIDRegistry\n\nregistry = NodeIDRegistry()\n\n# Register a node\nnode_id = registry.register_node(\n    model_instance=person,\n    model_class=Person,\n    id_fields=[\"name\", \"dob\"]\n)\n\n# Check if node exists\nif registry.has_node(node_id):\n    print(\"Node already registered\")\n\n# Get node metadata\nmetadata = registry.get_node_metadata(node_id)\n</code></pre>"},{"location":"concepts/graph-construction/#entity-vs-component","title":"Entity vs Component","text":""},{"location":"concepts/graph-construction/#entity-nodes","title":"Entity Nodes","text":"<p>Characteristics: - Unique, identifiable objects - Use <code>graph_id_fields</code> for stable IDs - Track individually in the graph</p> <p>Configuration: <pre><code>class Person(BaseModel):\n    model_config = ConfigDict(\n        is_entity=True,  # Explicit (optional)\n        graph_id_fields=[\"name\", \"dob\"]\n    )\n</code></pre></p> <p>Example: <pre><code># Two persons with same name but different DOB\nperson1 = Person(name=\"John Doe\", dob=\"1990-01-15\")\nperson2 = Person(name=\"John Doe\", dob=\"1985-05-20\")\n\n# Result: 2 separate nodes\n# - Person_JohnDoe_1990-01-15\n# - Person_JohnDoe_1985-05-20\n</code></pre></p>"},{"location":"concepts/graph-construction/#component-nodes","title":"Component Nodes","text":"<p>Characteristics: - Value objects - Deduplicated by content - Shared across entities</p> <p>Configuration: <pre><code>class Address(BaseModel):\n    model_config = ConfigDict(is_entity=False)\n    street: str\n    city: str\n</code></pre></p> <p>Example: <pre><code># Two persons at same address\naddress = Address(street=\"123 Main St\", city=\"Boston\")\nperson1 = Person(name=\"John\", address=address)\nperson2 = Person(name=\"Jane\", address=address)\n\n# Result: 3 nodes\n# - Person_John\n# - Person_Jane\n# - Address_{hash} (shared by both persons)\n</code></pre></p>"},{"location":"concepts/graph-construction/#edge-creation","title":"Edge Creation","text":""},{"location":"concepts/graph-construction/#edge-definition","title":"Edge Definition","text":"<p>Use the <code>edge()</code> helper in Pydantic models:</p> <pre><code>def edge(label: str, **kwargs: Any) -&gt; Any:\n    \"\"\"Create a Field with edge metadata.\"\"\"\n    return Field(..., json_schema_extra={\"edge_label\": label}, **kwargs)\n</code></pre>"},{"location":"concepts/graph-construction/#single-relationships","title":"Single Relationships","text":"<pre><code>class Document(BaseModel):\n    # Required single edge\n    issued_by: Organization = edge(\n        label=\"ISSUED_BY\",\n        description=\"Issuing organization\"\n    )\n\n    # Optional single edge\n    verified_by: Optional[Person] = edge(\n        label=\"VERIFIED_BY\",\n        description=\"Verifying person\"\n    )\n</code></pre>"},{"location":"concepts/graph-construction/#list-relationships","title":"List Relationships","text":"<pre><code>class Document(BaseModel):\n    # One-to-many relationship\n    authors: List[Person] = edge(\n        label=\"HAS_AUTHOR\",\n        default_factory=list,\n        description=\"Document authors\"\n    )\n</code></pre>"},{"location":"concepts/graph-construction/#edge-labels","title":"Edge Labels","text":"<p>Conventions: - Use ALL_CAPS with underscores - Use descriptive verb phrases - Be consistent across templates</p> <p>Common patterns: <pre><code># Authorship/Ownership\nISSUED_BY, CREATED_BY, OWNED_BY\n\n# Recipients\nSENT_TO, ADDRESSED_TO, DELIVERED_TO\n\n# Location\nLOCATED_AT, LIVES_AT, BASED_AT\n\n# Composition\nCONTAINS_ITEM, HAS_COMPONENT, INCLUDES_PART\n\n# Membership\nBELONGS_TO, PART_OF, MEMBER_OF\n\n# Processes\nHAS_PROCESS_STEP, HAS_EVALUATION, HAS_MEASUREMENT\n</code></pre></p>"},{"location":"concepts/graph-construction/#graph-structure","title":"Graph Structure","text":""},{"location":"concepts/graph-construction/#example-graph","title":"Example Graph","text":"<p>Given these models:</p> <pre><code>class Author(BaseModel):\n    model_config = ConfigDict(graph_id_fields=[\"name\"])\n    name: str\n    affiliation: str\n\nclass Paper(BaseModel):\n    model_config = ConfigDict(graph_id_fields=[\"title\"])\n    title: str\n    authors: List[Author] = edge(label=\"HAS_AUTHOR\", default_factory=list)\n    year: int\n\n# Instances\nauthor1 = Author(name=\"Dr. Smith\", affiliation=\"MIT\")\nauthor2 = Author(name=\"Dr. Jones\", affiliation=\"Stanford\")\npaper = Paper(\n    title=\"Advanced AI\",\n    authors=[author1, author2],\n    year=2024\n)\n</code></pre> <p>Resulting graph:</p> <pre><code>Nodes:\n  Paper_AdvancedAI\n    - title: \"Advanced AI\"\n    - year: 2024\n\n  Author_DrSmith\n    - name: \"Dr. Smith\"\n    - affiliation: \"MIT\"\n\n  Author_DrJones\n    - name: \"Dr. Jones\"\n    - affiliation: \"Stanford\"\n\nEdges:\n  Paper_AdvancedAI --HAS_AUTHOR--&gt; Author_DrSmith\n  Paper_AdvancedAI --HAS_AUTHOR--&gt; Author_DrJones\n</code></pre>"},{"location":"concepts/graph-construction/#networkx-representation","title":"NetworkX Representation","text":"<pre><code>import networkx as nx\n\n# Access nodes\nfor node_id, attrs in graph.nodes(data=True):\n    print(f\"Node: {node_id}\")\n    print(f\"Attributes: {attrs}\")\n\n# Access edges\nfor source, target, attrs in graph.edges(data=True):\n    print(f\"Edge: {source} --{attrs['label']}--&gt; {target}\")\n\n# Query graph\n# Find all authors of a paper\npaper_id = \"Paper_AdvancedAI\"\nauthors = list(graph.successors(paper_id))\n\n# Find all papers by an author\nauthor_id = \"Author_DrSmith\"\npapers = [n for n in graph.predecessors(author_id)]\n</code></pre>"},{"location":"concepts/graph-construction/#advanced-features","title":"Advanced Features","text":""},{"location":"concepts/graph-construction/#nested-relationships","title":"Nested Relationships","text":"<pre><code>class Component(BaseModel):\n    model_config = ConfigDict(graph_id_fields=[\"name\"])\n    name: str\n    material: Material = edge(label=\"USES_MATERIAL\")\n\nclass Assembly(BaseModel):\n    model_config = ConfigDict(graph_id_fields=[\"id\"])\n    id: str\n    components: List[Component] = edge(\n        label=\"HAS_COMPONENT\",\n        default_factory=list\n    )\n\n# Creates multi-level graph:\n# Assembly --HAS_COMPONENT--&gt; Component --USES_MATERIAL--&gt; Material\n</code></pre>"},{"location":"concepts/graph-construction/#deduplication","title":"Deduplication","text":"<p>Components with identical content are automatically deduplicated:</p> <pre><code># Same address used by multiple people\naddress = Address(street=\"123 Main St\", city=\"Boston\")\n\nperson1 = Person(name=\"John\", address=address)\nperson2 = Person(name=\"Jane\", address=address)\n\n# Result: Only 1 Address node, shared by both persons\n# Person_John --LIVES_AT--&gt; Address_{hash}\n# Person_Jane --LIVES_AT--&gt; Address_{hash}\n</code></pre>"},{"location":"concepts/graph-construction/#reverse-edges","title":"Reverse Edges","text":"<p>Enable bidirectional traversal:</p> <pre><code>converter = GraphConverter(add_reverse_edges=True)\n\n# Original\nDocument --ISSUED_BY--&gt; Organization\n\n# With reverse edges\nDocument --ISSUED_BY--&gt; Organization\nDocument &lt;--ISSUES-- Organization\n\n# Allows queries in both directions\n# \"What did Organization X issue?\"\n# \"Who issued Document Y?\"\n</code></pre>"},{"location":"concepts/graph-construction/#graph-metadata","title":"Graph Metadata","text":"<p>The converter returns metadata about the generated graph:</p> <pre><code>@dataclass\nclass GraphMetadata:\n    node_count: int           # Total nodes\n    edge_count: int           # Total edges\n    node_types: Dict[str, int]  # Count by type\n    edge_types: Dict[str, int]  # Count by label\n    source_model_count: int   # Input models\n\ngraph, metadata = converter.pydantic_list_to_graph(models)\n\nprint(f\"Nodes: {metadata.node_count}\")\nprint(f\"Edges: {metadata.edge_count}\")\nprint(f\"Node types: {metadata.node_types}\")\n# Output: {'Person': 2, 'Organization': 1, 'Address': 1}\n</code></pre>"},{"location":"concepts/graph-construction/#validation","title":"Validation","text":""},{"location":"concepts/graph-construction/#automatic-validation","title":"Automatic Validation","text":"<p>When <code>validate_graph=True</code>:</p> <pre><code>converter = GraphConverter(validate_graph=True)\n\n# Checks performed:\n# 1. All edges connect to existing nodes\n# 2. No orphaned nodes (unless root)\n# 3. Node IDs are unique\n# 4. Edge labels are present\n</code></pre>"},{"location":"concepts/graph-construction/#manual-validation","title":"Manual Validation","text":"<pre><code>from docling_graph.core.utils import validate_graph_structure\n\n# Validate after construction\nis_valid, errors = validate_graph_structure(graph)\n\nif not is_valid:\n    for error in errors:\n        print(f\"Validation error: {error}\")\n</code></pre>"},{"location":"concepts/graph-construction/#performance-considerations","title":"Performance Considerations","text":""},{"location":"concepts/graph-construction/#memory-usage","title":"Memory Usage","text":"<ul> <li>Small graphs (&lt; 1000 nodes): Negligible</li> <li>Medium graphs (1000-10000 nodes): ~10-50 MB</li> <li>Large graphs (&gt; 10000 nodes): Consider batch processing</li> </ul>"},{"location":"concepts/graph-construction/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Reuse Registry: Share <code>NodeIDRegistry</code> across batches</li> <li>Disable Validation: For trusted data, set <code>validate_graph=False</code></li> <li>Batch Processing: Process large document sets in chunks</li> <li>Lazy Loading: Don't load entire graph into memory if not needed</li> </ol>"},{"location":"concepts/graph-construction/#common-patterns","title":"Common Patterns","text":""},{"location":"concepts/graph-construction/#pattern-1-document-with-entities","title":"Pattern 1: Document with Entities","text":"<pre><code>class Document(BaseModel):\n    model_config = ConfigDict(graph_id_fields=[\"doc_id\"])\n    doc_id: str\n    issuer: Organization = edge(label=\"ISSUED_BY\")\n    recipient: Person = edge(label=\"SENT_TO\")\n    items: List[LineItem] = edge(label=\"CONTAINS_ITEM\", default_factory=list)\n</code></pre>"},{"location":"concepts/graph-construction/#pattern-2-hierarchical-structure","title":"Pattern 2: Hierarchical Structure","text":"<pre><code>class Section(BaseModel):\n    model_config = ConfigDict(graph_id_fields=[\"title\"])\n    title: str\n    subsections: List[\"Section\"] = edge(label=\"HAS_SUBSECTION\", default_factory=list)\n</code></pre>"},{"location":"concepts/graph-construction/#pattern-3-many-to-many-relationships","title":"Pattern 3: Many-to-Many Relationships","text":"<pre><code>class Student(BaseModel):\n    model_config = ConfigDict(graph_id_fields=[\"student_id\"])\n    student_id: str\n    courses: List[\"Course\"] = edge(label=\"ENROLLED_IN\", default_factory=list)\n\nclass Course(BaseModel):\n    model_config = ConfigDict(graph_id_fields=[\"course_id\"])\n    course_id: str\n    students: List[Student] = edge(label=\"HAS_STUDENT\", default_factory=list)\n</code></pre>"},{"location":"concepts/graph-construction/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/graph-construction/#issue-duplicate-nodes","title":"Issue: Duplicate Nodes","text":"<p>Symptom: Same entity appears multiple times</p> <p>Solution: Ensure <code>graph_id_fields</code> are set correctly</p> <pre><code># Wrong: No graph_id_fields\nclass Person(BaseModel):\n    name: str\n\n# Right: With graph_id_fields\nclass Person(BaseModel):\n    model_config = ConfigDict(graph_id_fields=[\"name\"])\n    name: str\n</code></pre>"},{"location":"concepts/graph-construction/#issue-missing-edges","title":"Issue: Missing Edges","text":"<p>Symptom: Relationships not appearing in graph</p> <p>Solution: Use <code>edge()</code> helper for relationship fields</p> <pre><code># Wrong: Regular field\nclass Document(BaseModel):\n    issuer: Organization\n\n# Right: Edge field\nclass Document(BaseModel):\n    issuer: Organization = edge(label=\"ISSUED_BY\")\n</code></pre>"},{"location":"concepts/graph-construction/#issue-id-collisions","title":"Issue: ID Collisions","text":"<p>Symptom: Different entities get same ID</p> <p>Solution: Add more fields to <code>graph_id_fields</code></p> <pre><code># Collision risk: Only name\nmodel_config = ConfigDict(graph_id_fields=[\"name\"])\n\n# Better: Name + DOB\nmodel_config = ConfigDict(graph_id_fields=[\"name\", \"date_of_birth\"])\n</code></pre>"},{"location":"concepts/graph-construction/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Export Formats</li> <li>Understand Pydantic Templates</li> <li>Explore Configuration System</li> </ul>"},{"location":"concepts/mermaid_flowchat/","title":"Mermaid flowchat","text":"<p>flowchart TD     A([\"Source Document\"]) --&gt; n4[\"Docling Graph Pipeline\"]     n2([\"Config\"]) --&gt; n4     n3([\"Pydantic Template\"]) --&gt; n4     n4 --&gt; n5[\"Extraction Factory\"]     n5 --&gt; n6[\"Docling Pipeline\"] &amp; n16([\"Prompt\"])     n6 --&gt; n7[\"OCR\"] &amp; n8[\"Vision\"] &amp; n25[\"Extract\"]     n8 --&gt; n9[\"Markdown Processor\"]     n7 --&gt; n9     n10[\"Conversion Strategy\"] --&gt; n11[\"One To One\"] &amp; n12[\"Many To One\"]     n13[\"Extraction Backend\"] --&gt; n14[\"LLM\"] &amp; n15[\"VLM\"]     n9 --&gt; n13     n16 --&gt; n13     n15 --&gt; n17([\"Extracted Content\"])     n14 --&gt; n17     n17 --&gt; n10     n12 --&gt; n18[\"Smart Template Merger\"]     n18 --&gt; n20([\"Populated Pydantic Model(s)\"])     n11 --&gt; n20     n20 --&gt; n21[\"Graph Converter\"]     n21 --&gt; n22([\"Knowledge Graph\"])     n22 --&gt; n23[\"Exporter\"] &amp; n24[\"Visualizer\"]     n23 --&gt; n29([\"CSV\"]) &amp; n30([\"Cypher\"]) &amp; n31([\"JSON\"]) &amp; n33[\"Knowledge Base\"]     n24 --&gt; n28([\"Images\"]) &amp; n27([\"HTML\"]) &amp; n26([\"Markdown\"])     n30 --&gt; n34[\"Batch Loader\"]     n31 --&gt; n34     n29 --&gt; n34     n34 --&gt; n33     n25 --&gt; n20     n4@{ shape: procs}     n5@{ shape: tag-proc}     n6@{ shape: procs}     n7@{ shape: lin-proc}     n8@{ shape: lin-proc}     n25@{ shape: lin-proc}     n9@{ shape: tag-proc}     n10@{ shape: procs}     n11@{ shape: lin-proc}     n12@{ shape: lin-proc}     n13@{ shape: procs}     n14@{ shape: lin-proc}     n15@{ shape: lin-proc}     n18@{ shape: tag-proc}     n21@{ shape: tag-proc}     n23@{ shape: tag-proc}     n24@{ shape: tag-proc}     n33@{ shape: db}     n34@{ shape: tag-proc}      A:::input      n4:::process      n2:::config      n3:::input      n5:::operator      n6:::process      n16:::config      n7:::process      n8:::process      n25:::process      n9:::operator      n10:::process      n11:::process      n12:::process      n13:::process      n14:::process      n15:::process      n17:::output      n18:::operator      n20:::output      n21:::operator      n22:::output      n23:::operator      n24:::operator      n29:::output      n30:::output      n31:::output      n33:::process      n28:::output      n27:::output      n26:::output      n34:::operator     classDef input fill:#E3F2FD,stroke:#90CAF9,color:#0D47A1     classDef config fill:#FFF8E1,stroke:#FFECB3,color:#5D4037     classDef output fill:#E8F5E9,stroke:#A5D6A7,color:#1B5E20     classDef decision fill:#FFE0B2,stroke:#FFB74D,color:#E65100     classDef data fill:#EDE7F6,stroke:#B39DDB,color:#4527A0     classDef operator fill:#F3E5F5,stroke:#CE93D8,color:#6A1B9A     classDef process fill:#ECEFF1,stroke:#B0BEC5,color:#263238</p>"},{"location":"concepts/pipeline-flowchart/","title":"Pipeline Flowchart","text":"<p>This page provides a visual representation of the complete Docling Graph pipeline, showing how documents flow through the system from input to knowledge graph output.</p>"},{"location":"concepts/pipeline-flowchart/#interactive-flowchart","title":"Interactive Flowchart","text":"<p>The following Mermaid diagram illustrates the complete pipeline architecture and data flow:</p> <p></p>"},{"location":"concepts/pipeline-flowchart/#pipeline-stages-explained","title":"Pipeline Stages Explained","text":""},{"location":"concepts/pipeline-flowchart/#stage-1-input-configuration","title":"Stage 1: Input &amp; Configuration","text":"<p>Components: - Source Document: PDF, image, or other supported format - Config: Pipeline configuration (backend, inference, processing mode) - Pydantic Template: Schema defining extraction structure and graph relationships</p> <p>Purpose: Define what to extract and how to process it.</p>"},{"location":"concepts/pipeline-flowchart/#stage-2-document-processing","title":"Stage 2: Document Processing","text":"<p>Components: - Docling Pipeline: Converts documents to structured format   - OCR: Traditional OCR pipeline for standard documents   - Vision: Vision-Language Model pipeline for complex layouts - Markdown Processor: Extracts markdown representation</p> <p>Purpose: Convert unstructured documents into processable text/markdown.</p>"},{"location":"concepts/pipeline-flowchart/#stage-3-extraction","title":"Stage 3: Extraction","text":"<p>Components: - Extraction Factory: Selects appropriate backend and strategy - Extraction Backend:    - VLM: Direct visual extraction from documents   - LLM: Text-based extraction from markdown - Prompt: Schema-guided extraction instructions</p> <p>Purpose: Extract structured data using AI models.</p>"},{"location":"concepts/pipeline-flowchart/#stage-4-processing-strategy","title":"Stage 4: Processing Strategy","text":"<p>Components: - Conversion Strategy: Determines how to handle multi-page documents   - One-to-One: Each page \u2192 separate model   - Many-to-One: All pages \u2192 single merged model - Smart Template Merger: Consolidates extracted data (for Many-to-One)</p> <p>Purpose: Organize extracted data according to processing strategy.</p>"},{"location":"concepts/pipeline-flowchart/#stage-5-validation","title":"Stage 5: Validation","text":"<p>Components: - Populated Pydantic Model(s): Validated, structured data instances</p> <p>Purpose: Ensure data quality through schema validation.</p>"},{"location":"concepts/pipeline-flowchart/#stage-6-graph-construction","title":"Stage 6: Graph Construction","text":"<p>Components: - Graph Converter: Transforms Pydantic models into NetworkX graph - Knowledge Graph: Directed graph with nodes and edges</p> <p>Purpose: Create semantic knowledge graph from validated data.</p>"},{"location":"concepts/pipeline-flowchart/#stage-7-export-visualization","title":"Stage 7: Export &amp; Visualization","text":"<p>Components: - Exporter: Generates multiple output formats   - CSV: Neo4j-compatible nodes/edges   - Cypher: Bulk import scripts   - JSON: General-purpose graph data - Visualizer: Creates human-readable outputs   - HTML: Interactive Cytoscape.js visualization   - Markdown: Detailed reports   - Images: Static graph visualizations - Batch Loader: Prepares data for database ingestion - Knowledge Base: Final graph database</p> <p>Purpose: Make graph data accessible in various formats.</p>"},{"location":"concepts/pipeline-flowchart/#data-flow-examples","title":"Data Flow Examples","text":""},{"location":"concepts/pipeline-flowchart/#example-1-vlm-one-to-one","title":"Example 1: VLM One-to-One","text":"<pre><code>ID Card (3 pages)\n    \u2193\nDocling Vision Pipeline\n    \u2193\nVLM Backend (NuExtract)\n    \u2193\nOne-to-One Strategy\n    \u2193\n[IDCard_1, IDCard_2, IDCard_3]\n    \u2193\nGraph Converter\n    \u2193\nKnowledge Graph (3 person nodes)\n    \u2193\nCSV Export \u2192 Neo4j\n</code></pre>"},{"location":"concepts/pipeline-flowchart/#example-2-llm-many-to-one-with-chunking","title":"Example 2: LLM Many-to-One with Chunking","text":"<pre><code>Research Paper (20 pages)\n    \u2193\nDocling OCR Pipeline\n    \u2193\nMarkdown Processor\n    \u2193\nDocument Chunker (4 chunks)\n    \u2193\nLLM Backend (Mistral)\n    \u2193\nMany-to-One Strategy\n    \u2193\nSmart Template Merger\n    \u2193\n[ResearchPaper_merged]\n    \u2193\nGraph Converter\n    \u2193\nKnowledge Graph (paper + authors + sections)\n    \u2193\nInteractive HTML Visualization\n</code></pre>"},{"location":"concepts/pipeline-flowchart/#key-decision-points","title":"Key Decision Points","text":""},{"location":"concepts/pipeline-flowchart/#1-backend-selection","title":"1. Backend Selection","text":"<pre><code>Document Type?\n\u251c\u2500 Structured Form \u2192 VLM Backend\n\u2514\u2500 Complex Narrative \u2192 LLM Backend\n</code></pre>"},{"location":"concepts/pipeline-flowchart/#2-processing-strategy","title":"2. Processing Strategy","text":"<pre><code>Pages Independent?\n\u251c\u2500 Yes \u2192 One-to-One\n\u2514\u2500 No \u2192 Many-to-One\n    \u2514\u2500 Document Size?\n        \u251c\u2500 Small (&lt; 5 pages) \u2192 No Chunking\n        \u2514\u2500 Large (\u2265 5 pages) \u2192 With Chunking\n</code></pre>"},{"location":"concepts/pipeline-flowchart/#3-consolidation-method","title":"3. Consolidation Method","text":"<pre><code>Many-to-One + Chunking?\n\u251c\u2500 Simple Merging \u2192 Programmatic (llm_consolidation=False)\n\u2514\u2500 Complex Merging \u2192 LLM-Based (llm_consolidation=True)\n</code></pre>"},{"location":"concepts/pipeline-flowchart/#component-interactions","title":"Component Interactions","text":""},{"location":"concepts/pipeline-flowchart/#extraction-factory","title":"Extraction Factory","text":"<p>The Extraction Factory is the central orchestrator:</p> <pre><code># Pseudo-code showing factory logic\nif backend == \"vlm\":\n    backend_instance = VLMBackend(model)\n    if processing_mode == \"one-to-one\":\n        strategy = OneToOneExtractor(backend_instance)\n    else:\n        strategy = ManyToOneExtractor(backend_instance)\n\nelif backend == \"llm\":\n    llm_client = get_client(provider, model)\n    backend_instance = LLMBackend(llm_client)\n    if processing_mode == \"one-to-one\":\n        strategy = OneToOneExtractor(backend_instance)\n    else:\n        strategy = ManyToOneExtractor(\n            backend_instance,\n            use_chunking=use_chunking,\n            llm_consolidation=llm_consolidation\n        )\n</code></pre>"},{"location":"concepts/pipeline-flowchart/#graph-converter","title":"Graph Converter","text":"<p>The Graph Converter transforms models to graphs:</p> <pre><code># Pseudo-code showing conversion logic\nfor model in pydantic_models:\n    # Create node from model\n    node_id = generate_stable_id(model)\n    graph.add_node(node_id, **model.dict())\n\n    # Create edges from relationships\n    for field_name, field_value in model:\n        if is_edge_field(field_name):\n            edge_label = get_edge_label(field_name)\n            target_id = generate_stable_id(field_value)\n            graph.add_edge(node_id, target_id, label=edge_label)\n</code></pre>"},{"location":"concepts/pipeline-flowchart/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"concepts/pipeline-flowchart/#pipeline-stages-by-time","title":"Pipeline Stages by Time","text":"<pre><code>Document Processing:    \u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591 20-30%\nExtraction:            \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591 40-50%\nGraph Conversion:      \u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 10-15%\nExport/Visualization:  \u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 10-15%\n</code></pre>"},{"location":"concepts/pipeline-flowchart/#bottlenecks","title":"Bottlenecks","text":"<ol> <li>Extraction: Most time-consuming (especially for LLM)</li> <li>Document Processing: Can be slow for large PDFs</li> <li>Consolidation: Adds overhead for Many-to-One with LLM</li> </ol>"},{"location":"concepts/pipeline-flowchart/#optimization-strategies","title":"Optimization Strategies","text":"<ul> <li>Caching: Reuse converted documents</li> <li>Batching: Process multiple chunks in parallel (future)</li> <li>Model Selection: Choose faster models for speed-critical applications</li> </ul>"},{"location":"concepts/pipeline-flowchart/#error-handling-flow","title":"Error Handling Flow","text":"<pre><code>Pipeline Start\n    \u2193\nTry: Load Config\n    \u251c\u2500 Success \u2192 Continue\n    \u2514\u2500 Failure \u2192 Exit with error\n    \u2193\nTry: Load Template\n    \u251c\u2500 Success \u2192 Continue\n    \u2514\u2500 Failure \u2192 Exit with error\n    \u2193\nTry: Extract Data\n    \u251c\u2500 Success \u2192 Continue\n    \u2514\u2500 Failure \u2192 Log warning, return empty\n    \u2193\nTry: Validate Models\n    \u251c\u2500 Success \u2192 Continue\n    \u2514\u2500 Failure \u2192 Log error, skip invalid\n    \u2193\nTry: Build Graph\n    \u251c\u2500 Success \u2192 Continue\n    \u2514\u2500 Failure \u2192 Exit with error\n    \u2193\nTry: Export\n    \u251c\u2500 Success \u2192 Complete\n    \u2514\u2500 Failure \u2192 Log error, partial output\n    \u2193\nFinally: Cleanup Resources\n</code></pre>"},{"location":"concepts/pipeline-flowchart/#next-steps","title":"Next Steps","text":"<ul> <li>Understand Architecture in detail</li> <li>Learn about Extraction Backends</li> <li>Explore Processing Strategies</li> <li>Deep dive into Graph Construction</li> </ul>"},{"location":"concepts/processing-strategies/","title":"Processing Strategies","text":"<p>Docling Graph offers two processing strategies for handling multi-page documents: One-to-One and Many-to-One. Understanding these strategies is crucial for choosing the right approach for your use case.</p>"},{"location":"concepts/processing-strategies/#overview","title":"Overview","text":"Strategy Pages \u2192 Models Use Case Output One-to-One 1 page \u2192 1 model Page-specific analysis N models (one per page) Many-to-One N pages \u2192 1 model Document-level extraction 1 merged model"},{"location":"concepts/processing-strategies/#one-to-one-strategy","title":"One-to-One Strategy","text":""},{"location":"concepts/processing-strategies/#concept","title":"Concept","text":"<p>The One-to-One strategy processes each page independently, producing a separate Pydantic model instance for each page.</p> <p>Location: <code>docling_graph/core/extractors/strategies/one_to_one.py</code></p>"},{"location":"concepts/processing-strategies/#how-it-works","title":"How It Works","text":"<pre><code>Page 1  \u2192  Extract  \u2192  Model 1\nPage 2  \u2192  Extract  \u2192  Model 2\nPage 3  \u2192  Extract  \u2192  Model 3\n...\nPage N  \u2192  Extract  \u2192  Model N\n\nResult: [Model 1, Model 2, Model 3, ..., Model N]\n</code></pre>"},{"location":"concepts/processing-strategies/#configuration","title":"Configuration","text":"<pre><code>from docling_graph import PipelineConfig\n\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    processing_mode=\"one-to-one\",  # Process each page separately\n    backend=\"llm\",\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#characteristics","title":"Characteristics","text":"<p>\u2705 Independent Processing: Each page is processed in isolation \u2705 Page-Level Granularity: Preserves page-specific information \u2705 Parallel Potential: Pages can be processed in parallel (future optimization) \u2705 Simple Logic: No merging or consolidation required  </p> <p>\u26a0\ufe0f Multiple Models: Returns N models instead of 1 \u26a0\ufe0f No Cross-Page Context: Cannot capture relationships across pages \u26a0\ufe0f Redundant Data: May duplicate information present on multiple pages  </p>"},{"location":"concepts/processing-strategies/#ideal-use-cases","title":"Ideal Use Cases","text":""},{"location":"concepts/processing-strategies/#1-page-specific-analysis","title":"1. Page-Specific Analysis","text":"<p>When each page contains distinct, independent information:</p> <pre><code># Example: Multi-page invoice batch\n# Each page is a separate invoice\nconfig = PipelineConfig(\n    source=\"invoice_batch.pdf\",\n    template=Invoice,\n    processing_mode=\"one-to-one\",\n    output_dir=\"outputs/invoices\"\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#2-document-collections","title":"2. Document Collections","text":"<p>When a single PDF contains multiple independent documents:</p> <pre><code># Example: Scanned ID cards\n# Each page is a different person's ID\nconfig = PipelineConfig(\n    source=\"id_cards_batch.pdf\",\n    template=IDCard,\n    processing_mode=\"one-to-one\",\n    output_dir=\"outputs/id_cards\"\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#3-page-level-metadata","title":"3. Page-Level Metadata","text":"<p>When you need to track which page information came from:</p> <pre><code># Each model represents one page\n# Useful for citation or reference tracking\nmodels = extractor.extract(source, template)\nfor i, model in enumerate(models, 1):\n    print(f\"Page {i}: {model}\")\n</code></pre>"},{"location":"concepts/processing-strategies/#example-output","title":"Example Output","text":"<pre><code># Input: 3-page document\n# Output: List of 3 models\n\n[\n    Invoice(invoice_number=\"INV-001\", total=100.00, ...),\n    Invoice(invoice_number=\"INV-002\", total=250.00, ...),\n    Invoice(invoice_number=\"INV-003\", total=175.00, ...)\n]\n</code></pre>"},{"location":"concepts/processing-strategies/#graph-construction","title":"Graph Construction","text":"<p>Each model becomes a separate subgraph:</p> <pre><code>Graph:\n  Invoice_INV001 (from page 1)\n    \u251c\u2500 ISSUED_BY \u2192 Organization_A\n    \u2514\u2500 SENT_TO \u2192 Customer_X\n\n  Invoice_INV002 (from page 2)\n    \u251c\u2500 ISSUED_BY \u2192 Organization_A\n    \u2514\u2500 SENT_TO \u2192 Customer_Y\n\n  Invoice_INV003 (from page 3)\n    \u251c\u2500 ISSUED_BY \u2192 Organization_B\n    \u2514\u2500 SENT_TO \u2192 Customer_Z\n</code></pre>"},{"location":"concepts/processing-strategies/#many-to-one-strategy","title":"Many-to-One Strategy","text":""},{"location":"concepts/processing-strategies/#concept_1","title":"Concept","text":"<p>The Many-to-One strategy processes all pages together, producing a single merged Pydantic model instance for the entire document.</p> <p>Location: <code>docling_graph/core/extractors/strategies/many_to_one.py</code></p>"},{"location":"concepts/processing-strategies/#how-it-works_1","title":"How It Works","text":"<pre><code>Page 1 \u2510\nPage 2 \u251c\u2500\u2192  Extract  \u2192  Merge  \u2192  Single Model\nPage 3 \u2518\n\nResult: [Merged Model]\n</code></pre>"},{"location":"concepts/processing-strategies/#processing-modes","title":"Processing Modes","text":""},{"location":"concepts/processing-strategies/#without-chunking","title":"Without Chunking","text":"<pre><code># Process entire document as one markdown\nFull Markdown  \u2192  Extract  \u2192  Single Model\n</code></pre>"},{"location":"concepts/processing-strategies/#with-chunking","title":"With Chunking","text":"<pre><code># Split into chunks, extract from each, then merge\nChunk 1  \u2192  Extract  \u2192  Partial Model 1 \u2510\nChunk 2  \u2192  Extract  \u2192  Partial Model 2 \u251c\u2500\u2192  Merge  \u2192  Final Model\nChunk 3  \u2192  Extract  \u2192  Partial Model 3 \u2518\n</code></pre>"},{"location":"concepts/processing-strategies/#configuration_1","title":"Configuration","text":""},{"location":"concepts/processing-strategies/#basic-no-chunking","title":"Basic (No Chunking)","text":"<pre><code>config = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    processing_mode=\"many-to-one\",\n    use_chunking=False,  # Process full document\n    backend=\"llm\",\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#with-chunking-recommended-for-large-documents","title":"With Chunking (Recommended for Large Documents)","text":"<pre><code>config = PipelineConfig(\n    source=\"large_document.pdf\",\n    template=YourTemplate,\n    processing_mode=\"many-to-one\",\n    use_chunking=True,           # Enable chunking\n    llm_consolidation=False,     # Programmatic merge (faster)\n    backend=\"llm\",\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#with-llm-consolidation","title":"With LLM Consolidation","text":"<pre><code>config = PipelineConfig(\n    source=\"complex_document.pdf\",\n    template=YourTemplate,\n    processing_mode=\"many-to-one\",\n    use_chunking=True,\n    llm_consolidation=True,  # Use LLM to intelligently merge\n    backend=\"llm\",\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#characteristics_1","title":"Characteristics","text":"<p>\u2705 Single Model: Returns one consolidated model \u2705 Cross-Page Context: Captures relationships across pages \u2705 Deduplication: Automatically merges duplicate information \u2705 Document-Level View: Represents the entire document holistically  </p> <p>\u26a0\ufe0f More Complex: Requires merging logic \u26a0\ufe0f Context Limits: May need chunking for large documents \u26a0\ufe0f Slower: Additional consolidation step  </p>"},{"location":"concepts/processing-strategies/#merging-strategies","title":"Merging Strategies","text":""},{"location":"concepts/processing-strategies/#1-programmatic-merge-default","title":"1. Programmatic Merge (Default)","text":"<p>Fast, rule-based merging:</p> <ul> <li>Lists: Concatenate and deduplicate</li> <li>Scalars: Keep first non-null value</li> <li>Nested Objects: Recursively merge</li> </ul> <pre><code># Automatic merging of extracted data\nconfig = PipelineConfig(\n    processing_mode=\"many-to-one\",\n    llm_consolidation=False  # Use programmatic merge\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#2-llm-consolidation","title":"2. LLM Consolidation","text":"<p>Intelligent, context-aware merging:</p> <ul> <li>Uses LLM to resolve conflicts</li> <li>Better handles semantic duplicates</li> <li>Slower but more accurate</li> </ul> <pre><code># LLM-based intelligent merging\nconfig = PipelineConfig(\n    processing_mode=\"many-to-one\",\n    llm_consolidation=True  # Use LLM to merge\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#ideal-use-cases_1","title":"Ideal Use Cases","text":""},{"location":"concepts/processing-strategies/#1-multi-page-documents","title":"1. Multi-Page Documents","text":"<p>When a document spans multiple pages with related content:</p> <pre><code># Example: Research paper\n# Title on page 1, abstract on page 1-2, content on pages 2-10\nconfig = PipelineConfig(\n    source=\"research_paper.pdf\",\n    template=ResearchPaper,\n    processing_mode=\"many-to-one\",\n    use_chunking=True,\n    output_dir=\"outputs/research\"\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#2-narrative-documents","title":"2. Narrative Documents","text":"<p>When information flows across pages:</p> <pre><code># Example: Insurance policy\n# Coverage details span multiple pages\nconfig = PipelineConfig(\n    source=\"insurance_policy.pdf\",\n    template=InsurancePolicy,\n    processing_mode=\"many-to-one\",\n    use_chunking=True,\n    output_dir=\"outputs/policies\"\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#3-documents-with-relationships","title":"3. Documents with Relationships","text":"<p>When entities and relationships span pages:</p> <pre><code># Example: Contract\n# Parties on page 1, terms on pages 2-5, signatures on page 6\nconfig = PipelineConfig(\n    source=\"contract.pdf\",\n    template=Contract,\n    processing_mode=\"many-to-one\",\n    use_chunking=True,\n    output_dir=\"outputs/contracts\"\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#example-output_1","title":"Example Output","text":"<pre><code># Input: 10-page research paper\n# Output: Single merged model\n\nResearchPaper(\n    title=\"Advanced Battery Technology\",\n    authors=[\n        Author(name=\"Dr. Smith\", affiliation=\"MIT\"),\n        Author(name=\"Dr. Jones\", affiliation=\"Stanford\")\n    ],\n    abstract=\"This paper presents...\",\n    sections=[\n        Section(title=\"Introduction\", content=\"...\"),\n        Section(title=\"Methodology\", content=\"...\"),\n        Section(title=\"Results\", content=\"...\")\n    ],\n    references=[...]\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#graph-construction_1","title":"Graph Construction","text":"<p>Single unified graph:</p> <pre><code>Graph:\n  ResearchPaper_AdvancedBattery\n    \u251c\u2500 HAS_AUTHOR \u2192 Author_DrSmith\n    \u251c\u2500 HAS_AUTHOR \u2192 Author_DrJones\n    \u251c\u2500 HAS_SECTION \u2192 Section_Introduction\n    \u251c\u2500 HAS_SECTION \u2192 Section_Methodology\n    \u2514\u2500 HAS_SECTION \u2192 Section_Results\n\n  Author_DrSmith\n    \u2514\u2500 AFFILIATED_WITH \u2192 Organization_MIT\n\n  Author_DrJones\n    \u2514\u2500 AFFILIATED_WITH \u2192 Organization_Stanford\n</code></pre>"},{"location":"concepts/processing-strategies/#chunking-deep-dive","title":"Chunking Deep Dive","text":""},{"location":"concepts/processing-strategies/#why-chunking","title":"Why Chunking?","text":"<p>Large documents may exceed LLM context limits. Chunking splits the document into manageable pieces while preserving semantic coherence.</p>"},{"location":"concepts/processing-strategies/#hybrid-chunking-strategy","title":"Hybrid Chunking Strategy","text":"<p>Docling Graph uses a hybrid approach:</p> <ol> <li>Docling Segmentation: Respects document structure (sections, tables, lists)</li> <li>Semantic Chunking: Groups related content together</li> <li>Token-Aware: Respects LLM context limits</li> </ol> <pre><code># Chunking configuration\nconfig = PipelineConfig(\n    use_chunking=True,\n    # Chunking happens automatically based on:\n    # - Document structure (from Docling)\n    # - LLM context limit\n    # - Semantic boundaries\n)\n</code></pre>"},{"location":"concepts/processing-strategies/#chunk-processing","title":"Chunk Processing","text":"<pre><code>Document (50 pages)\n    \u2193\nDocling Segmentation\n    \u2193\nSemantic Chunks (respecting structure)\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Chunk 1 \u2502 Chunk 2 \u2502 Chunk 3 \u2502 Chunk 4 \u2502\n\u2502 (10 pg) \u2502 (15 pg) \u2502 (12 pg) \u2502 (13 pg) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193         \u2193         \u2193         \u2193\n Extract   Extract   Extract   Extract\n    \u2193         \u2193         \u2193         \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Model 1 \u2502 Model 2 \u2502 Model 3 \u2502 Model 4 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n         Consolidate\n              \u2193\n        Final Model\n</code></pre>"},{"location":"concepts/processing-strategies/#consolidation-methods","title":"Consolidation Methods","text":""},{"location":"concepts/processing-strategies/#programmatic-fast","title":"Programmatic (Fast)","text":"<pre><code>config = PipelineConfig(\n    use_chunking=True,\n    llm_consolidation=False  # Default\n)\n\n# Merging rules:\n# - Lists: Concatenate + deduplicate\n# - Scalars: First non-null wins\n# - Objects: Recursive merge\n</code></pre>"},{"location":"concepts/processing-strategies/#llm-based-intelligent","title":"LLM-Based (Intelligent)","text":"<pre><code>config = PipelineConfig(\n    use_chunking=True,\n    llm_consolidation=True\n)\n\n# LLM receives:\n# - All partial models\n# - Original template schema\n# - Consolidation prompt\n# Returns: Intelligently merged model\n</code></pre>"},{"location":"concepts/processing-strategies/#choosing-the-right-strategy","title":"Choosing the Right Strategy","text":""},{"location":"concepts/processing-strategies/#decision-tree","title":"Decision Tree","text":"<pre><code>Is each page independent?\n\u251c\u2500 Yes \u2192 One-to-One\n\u2502   \u2514\u2500 Examples: Invoice batches, ID card scans\n\u2502\n\u2514\u2500 No \u2192 Many-to-One\n    \u2502\n    \u251c\u2500 Is document &lt; 5 pages?\n    \u2502   \u2514\u2500 Yes \u2192 Many-to-One (no chunking)\n    \u2502\n    \u2514\u2500 Is document \u2265 5 pages?\n        \u2514\u2500 Yes \u2192 Many-to-One (with chunking)\n            \u2502\n            \u251c\u2500 Simple merging needed?\n            \u2502   \u2514\u2500 Yes \u2192 llm_consolidation=False\n            \u2502\n            \u2514\u2500 Complex merging needed?\n                \u2514\u2500 Yes \u2192 llm_consolidation=True\n</code></pre>"},{"location":"concepts/processing-strategies/#quick-reference","title":"Quick Reference","text":"Document Type Strategy Chunking Consolidation Invoice batch One-to-One N/A N/A ID card scan One-to-One N/A N/A Short report (1-5 pages) Many-to-One No N/A Research paper (10+ pages) Many-to-One Yes Programmatic Complex contract (20+ pages) Many-to-One Yes LLM Insurance policy (15+ pages) Many-to-One Yes Programmatic"},{"location":"concepts/processing-strategies/#performance-considerations","title":"Performance Considerations","text":""},{"location":"concepts/processing-strategies/#one-to-one","title":"One-to-One","text":"<ul> <li>Speed: Fast (pages processed independently)</li> <li>Memory: Low (one page at a time)</li> <li>Accuracy: High (no merging errors)</li> </ul>"},{"location":"concepts/processing-strategies/#many-to-one-no-chunking","title":"Many-to-One (No Chunking)","text":"<ul> <li>Speed: Moderate (single extraction)</li> <li>Memory: High (entire document in context)</li> <li>Accuracy: High (full context available)</li> </ul>"},{"location":"concepts/processing-strategies/#many-to-one-with-chunking","title":"Many-to-One (With Chunking)","text":"<ul> <li>Speed: Moderate to Slow (multiple extractions + merge)</li> <li>Memory: Low to Moderate (chunks processed separately)</li> <li>Accuracy: Good (depends on consolidation method)</li> </ul>"},{"location":"concepts/processing-strategies/#best-practices","title":"Best Practices","text":""},{"location":"concepts/processing-strategies/#for-one-to-one","title":"For One-to-One","text":"<ol> <li>Verify Independence: Ensure pages are truly independent</li> <li>Handle Duplicates: Be prepared for duplicate entities across pages</li> <li>Page Tracking: Consider adding page metadata to models</li> </ol>"},{"location":"concepts/processing-strategies/#for-many-to-one","title":"For Many-to-One","text":"<ol> <li>Enable Chunking: For documents &gt; 5 pages</li> <li>Choose Consolidation: Programmatic for speed, LLM for quality</li> <li>Test Merging: Verify merged output matches expectations</li> <li>Monitor Context: Watch for context limit warnings</li> </ol>"},{"location":"concepts/processing-strategies/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/processing-strategies/#one-to-one-issues","title":"One-to-One Issues","text":"<p>Problem: Duplicate entities across pages Solution: Use graph deduplication or post-process models</p> <p>Problem: Missing cross-page relationships Solution: Switch to Many-to-One strategy</p>"},{"location":"concepts/processing-strategies/#many-to-one-issues","title":"Many-to-One Issues","text":"<p>Problem: Context length exceeded Solution: Enable chunking with <code>use_chunking=True</code></p> <p>Problem: Poor merge quality Solution: Switch to <code>llm_consolidation=True</code></p> <p>Problem: Slow processing Solution: Use programmatic consolidation or reduce document size</p>"},{"location":"concepts/processing-strategies/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Graph Construction</li> <li>Understand Pydantic Templates</li> <li>Explore Extraction Backends</li> </ul>"},{"location":"concepts/pydantic-templates/","title":"Pydantic Templates","text":"<p>Pydantic templates are the foundation of Docling Graph, serving three critical purposes: guiding LLM extraction, validating data, and defining graph structure.</p>"},{"location":"concepts/pydantic-templates/#overview","title":"Overview","text":"<p>A Pydantic template is a Python class that inherits from <code>BaseModel</code> and defines:</p> <ol> <li>Extraction Schema: What data to extract from documents</li> <li>Validation Rules: How to validate and normalize extracted data</li> <li>Graph Structure: How entities and relationships map to nodes and edges</li> </ol>"},{"location":"concepts/pydantic-templates/#why-pydantic","title":"Why Pydantic?","text":"<p>Pydantic provides:</p> <ul> <li>Type Safety: Strong typing with Python type hints</li> <li>Validation: Automatic data validation and coercion</li> <li>Documentation: Field descriptions guide LLM extraction</li> <li>Serialization: Easy conversion to/from JSON</li> <li>IDE Support: Autocomplete and type checking</li> </ul>"},{"location":"concepts/pydantic-templates/#basic-template-structure","title":"Basic Template Structure","text":""},{"location":"concepts/pydantic-templates/#minimal-example","title":"Minimal Example","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass Person(BaseModel):\n    \"\"\"A person entity.\"\"\"\n    model_config = {\n        'is_entity': True,\n        'graph_id_fields': ['name']\n    }\n\n    name: str = Field(\n        description=\"Person's full name\",\n        examples=[\"John Doe\", \"Jane Smith\"]\n    )\n\n    age: Optional[int] = Field(\n        None,\n        description=\"Person's age in years\",\n        examples=[25, 30, 45]\n    )\n</code></pre>"},{"location":"concepts/pydantic-templates/#complete-example","title":"Complete Example","text":"<pre><code>from pydantic import BaseModel, ConfigDict, Field, field_validator\nfrom typing import Any, List, Optional\nfrom datetime import date\n\n# Edge helper function (required)\ndef edge(label: str, **kwargs: Any) -&gt; Any:\n    \"\"\"Create a Field with edge metadata for graph relationships.\"\"\"\n    return Field(..., json_schema_extra={\"edge_label\": label}, **kwargs)\n\nclass Address(BaseModel):\n    \"\"\"Physical address component.\"\"\"\n    model_config = ConfigDict(is_entity=False)\n\n    street: Optional[str] = Field(\n        None,\n        description=\"Street address\",\n        examples=[\"123 Main St\", \"456 Oak Ave\"]\n    )\n    city: Optional[str] = Field(\n        None,\n        description=\"City name\",\n        examples=[\"Boston\", \"New York\"]\n    )\n\n    def __str__(self) -&gt; str:\n        parts = [self.street, self.city]\n        return \", \".join(p for p in parts if p)\n\nclass Person(BaseModel):\n    \"\"\"Person entity with validation.\"\"\"\n    model_config = ConfigDict(\n        graph_id_fields=[\"name\", \"date_of_birth\"]\n    )\n\n    name: str = Field(\n        description=\"Person's full name\",\n        examples=[\"John Doe\", \"Jane Smith\"]\n    )\n\n    date_of_birth: Optional[date] = Field(\n        None,\n        description=\"Date of birth in YYYY-MM-DD format\",\n        examples=[\"1990-01-15\", \"1985-06-20\"]\n    )\n\n    email: Optional[str] = Field(\n        None,\n        description=\"Email address\",\n        examples=[\"john@example.com\", \"jane@company.com\"]\n    )\n\n    address: Optional[Address] = Field(\n        None,\n        description=\"Residential address\"\n    )\n\n    @field_validator(\"email\", mode=\"before\")\n    @classmethod\n    def normalize_email(cls, v: Any) -&gt; Any:\n        \"\"\"Convert email to lowercase.\"\"\"\n        if v:\n            return v.lower().strip()\n        return v\n\n    def __str__(self) -&gt; str:\n        return self.name\n</code></pre>"},{"location":"concepts/pydantic-templates/#model-configuration","title":"Model Configuration","text":""},{"location":"concepts/pydantic-templates/#configdict-options","title":"ConfigDict Options","text":"<pre><code>from pydantic import ConfigDict\n\nmodel_config = ConfigDict(\n    # Graph-specific options\n    is_entity=True,                    # Mark as entity (default: True if graph_id_fields present)\n    graph_id_fields=[\"field1\", \"field2\"],  # Fields for stable node IDs\n\n    # Pydantic options\n    validate_assignment=True,          # Validate on attribute assignment\n    arbitrary_types_allowed=True,      # Allow custom types\n    str_strip_whitespace=True,         # Strip whitespace from strings\n    use_enum_values=True,              # Use enum values instead of enum objects\n)\n</code></pre>"},{"location":"concepts/pydantic-templates/#entity-vs-component","title":"Entity vs Component","text":""},{"location":"concepts/pydantic-templates/#entity-unique-identifiable","title":"Entity (Unique, Identifiable)","text":"<pre><code>class Person(BaseModel):\n    \"\"\"Entity: Unique person tracked individually.\"\"\"\n    model_config = ConfigDict(\n        graph_id_fields=[\"name\", \"date_of_birth\"]\n    )\n\n    name: str\n    date_of_birth: date\n</code></pre> <p>Result: Each person gets a unique node ID based on name + DOB</p>"},{"location":"concepts/pydantic-templates/#component-value-object","title":"Component (Value Object)","text":"<pre><code>class Address(BaseModel):\n    \"\"\"Component: Deduplicated by content.\"\"\"\n    model_config = ConfigDict(is_entity=False)\n\n    street: str\n    city: str\n</code></pre> <p>Result: Identical addresses share the same node</p>"},{"location":"concepts/pydantic-templates/#field-definitions","title":"Field Definitions","text":""},{"location":"concepts/pydantic-templates/#field-types","title":"Field Types","text":"<pre><code>from typing import List, Optional, Union\nfrom datetime import date, datetime\nfrom enum import Enum\n\nclass MyModel(BaseModel):\n    # Required fields\n    required_str: str = Field(...)\n    required_int: int = Field(...)\n\n    # Optional fields\n    optional_str: Optional[str] = Field(None)\n    optional_int: Optional[int] = Field(None)\n\n    # Lists\n    string_list: List[str] = Field(default_factory=list)\n    object_list: List[Address] = Field(default_factory=list)\n\n    # Dates\n    birth_date: Optional[date] = Field(None)\n    timestamp: Optional[datetime] = Field(None)\n\n    # Enums\n    status: StatusEnum = Field(...)\n\n    # Union types\n    value: Union[str, int, float] = Field(...)\n</code></pre>"},{"location":"concepts/pydantic-templates/#field-parameters","title":"Field Parameters","text":"<pre><code>field_name: str = Field(\n    default=...,  # ... = required, None = optional, or actual default value\n\n    # LLM guidance\n    description=\"Detailed description for LLM extraction\",\n    examples=[\"Example 1\", \"Example 2\", \"Example 3\"],\n\n    # Validation\n    min_length=1,\n    max_length=100,\n    ge=0,  # Greater than or equal\n    le=100,  # Less than or equal\n    pattern=r\"^\\d{3}-\\d{3}-\\d{4}$\",  # Regex pattern\n\n    # Metadata\n    alias=\"alternativeName\",  # Alternative field name\n    title=\"Field Title\",\n    deprecated=True,\n)\n</code></pre>"},{"location":"concepts/pydantic-templates/#description-best-practices","title":"Description Best Practices","text":"<p>Good descriptions: - Are specific and detailed - Include extraction hints (field names, patterns) - Provide parsing instructions - Guide the LLM on ambiguous cases</p> <pre><code># EXCELLENT\ndate_of_birth: Optional[date] = Field(\n    None,\n    description=(\n        \"Person's date of birth. Look for 'Date of birth', 'DOB', \"\n        \"'Born on', or 'Date de naissance'. Parse formats like \"\n        \"'DD/MM/YYYY' or 'DD-MM-YYYY' and normalize to YYYY-MM-DD.\"\n    ),\n    examples=[\"1990-05-15\", \"1985-12-20\"]\n)\n\n# POOR\ndate_of_birth: Optional[date] = Field(None, description=\"Birth date\")\n</code></pre>"},{"location":"concepts/pydantic-templates/#examples-best-practices","title":"Examples Best Practices","text":"<p>Provide 2-5 diverse, realistic examples:</p> <pre><code># For simple fields\nemail: Optional[str] = Field(\n    None,\n    description=\"Email address\",\n    examples=[\n        \"john.doe@email.com\",\n        \"contact@company.fr\",\n        \"info@organization.org\"\n    ]\n)\n\n# For lists\ntags: List[str] = Field(\n    default_factory=list,\n    description=\"Document tags or categories\",\n    examples=[\n        [\"finance\", \"invoice\", \"2024\"],\n        [\"research\", \"chemistry\", \"battery\"],\n        [\"legal\", \"contract\"]\n    ]\n)\n\n# For nested objects\ncomponents: List[Component] = Field(\n    default_factory=list,\n    description=\"List of components\",\n    examples=[\n        [\n            {\n                \"name\": \"Battery Cell\",\n                \"material\": {\"name\": \"Lithium\", \"grade\": \"99.9%\"},\n                \"quantity\": 100\n            }\n        ]\n    ]\n)\n</code></pre>"},{"location":"concepts/pydantic-templates/#relationships-and-edges","title":"Relationships and Edges","text":""},{"location":"concepts/pydantic-templates/#edge-helper-function","title":"Edge Helper Function","text":"<p>Required in every template:</p> <pre><code>from typing import Any\n\ndef edge(label: str, **kwargs: Any) -&gt; Any:\n    \"\"\"Create a Field with edge metadata for graph relationships.\"\"\"\n    return Field(..., json_schema_extra={\"edge_label\": label}, **kwargs)\n</code></pre>"},{"location":"concepts/pydantic-templates/#single-relationships","title":"Single Relationships","text":"<pre><code>class Document(BaseModel):\n    # Required single edge\n    issued_by: Organization = edge(\n        label=\"ISSUED_BY\",\n        description=\"Organization that issued this document\"\n    )\n\n    # Optional single edge\n    verified_by: Optional[Person] = edge(\n        label=\"VERIFIED_BY\",\n        description=\"Person who verified this document\"\n    )\n</code></pre>"},{"location":"concepts/pydantic-templates/#list-relationships","title":"List Relationships","text":"<pre><code>class Document(BaseModel):\n    # One-to-many relationship\n    authors: List[Person] = edge(\n        label=\"HAS_AUTHOR\",\n        default_factory=list,  # REQUIRED for lists\n        description=\"Document authors\"\n    )\n\n    # Optional list relationship\n    reviewers: List[Person] = edge(\n        label=\"REVIEWED_BY\",\n        default_factory=list,\n        description=\"People who reviewed this document\"\n    )\n</code></pre>"},{"location":"concepts/pydantic-templates/#edge-label-conventions","title":"Edge Label Conventions","text":"<p>Use descriptive, ALL_CAPS labels:</p> <pre><code># Authorship/Ownership\nISSUED_BY, CREATED_BY, OWNED_BY, AUTHORED_BY\n\n# Recipients\nSENT_TO, ADDRESSED_TO, DELIVERED_TO\n\n# Location\nLOCATED_AT, LIVES_AT, BASED_AT\n\n# Composition\nCONTAINS_ITEM, HAS_COMPONENT, INCLUDES_PART, HAS_SECTION\n\n# Membership\nBELONGS_TO, PART_OF, MEMBER_OF\n\n# Processes\nHAS_PROCESS_STEP, HAS_EVALUATION, HAS_MEASUREMENT\n\n# Temporal\nFOLLOWS, PRECEDES, OCCURS_DURING\n</code></pre>"},{"location":"concepts/pydantic-templates/#validation","title":"Validation","text":""},{"location":"concepts/pydantic-templates/#field-validators","title":"Field Validators","text":"<pre><code>from pydantic import field_validator\n\nclass Person(BaseModel):\n    email: Optional[str] = Field(None)\n    age: Optional[int] = Field(None)\n\n    @field_validator(\"email\", mode=\"before\")\n    @classmethod\n    def normalize_email(cls, v: Any) -&gt; Any:\n        \"\"\"Convert email to lowercase and strip whitespace.\"\"\"\n        if v:\n            return v.lower().strip()\n        return v\n\n    @field_validator(\"age\")\n    @classmethod\n    def validate_age(cls, v: Any) -&gt; Any:\n        \"\"\"Ensure age is reasonable.\"\"\"\n        if v is not None and (v &lt; 0 or v &gt; 150):\n            raise ValueError(\"Age must be between 0 and 150\")\n        return v\n</code></pre>"},{"location":"concepts/pydantic-templates/#model-validators","title":"Model Validators","text":"<pre><code>from pydantic import model_validator\nfrom typing_extensions import Self\n\nclass Measurement(BaseModel):\n    numeric_value: Optional[float] = Field(None)\n    numeric_value_min: Optional[float] = Field(None)\n    numeric_value_max: Optional[float] = Field(None)\n\n    @model_validator(mode=\"after\")\n    def validate_value_consistency(self) -&gt; Self:\n        \"\"\"Ensure value fields are used consistently.\"\"\"\n        has_single = self.numeric_value is not None\n        has_range = (self.numeric_value_min is not None or \n                     self.numeric_value_max is not None)\n\n        if has_single and has_range:\n            raise ValueError(\n                \"Cannot specify both numeric_value and range values\"\n            )\n\n        return self\n</code></pre>"},{"location":"concepts/pydantic-templates/#pre-validators","title":"Pre-validators","text":"<p>Use <code>mode=\"before\"</code> for transformations before type coercion:</p> <pre><code>@field_validator(\"given_names\", mode=\"before\")\n@classmethod\ndef ensure_list(cls, v: Any) -&gt; Any:\n    \"\"\"Convert comma-separated string to list.\"\"\"\n    if isinstance(v, str):\n        if \",\" in v:\n            return [name.strip() for name in v.split(\",\")]\n        return [v]\n    return v\n</code></pre>"},{"location":"concepts/pydantic-templates/#string-representations","title":"String Representations","text":"<p>Add <code>__str__</code> methods for human-readable output:</p> <pre><code>class Person(BaseModel):\n    first_name: Optional[str] = Field(None)\n    last_name: Optional[str] = Field(None)\n\n    def __str__(self) -&gt; str:\n        parts = [self.first_name, self.last_name]\n        return \" \".join(p for p in parts if p) or \"Unknown\"\n\nclass Address(BaseModel):\n    street: Optional[str] = Field(None)\n    city: Optional[str] = Field(None)\n    postal_code: Optional[str] = Field(None)\n\n    def __str__(self) -&gt; str:\n        parts = [self.street, self.city, self.postal_code]\n        return \", \".join(p for p in parts if p)\n\nclass MonetaryAmount(BaseModel):\n    value: float = Field(...)\n    currency: Optional[str] = Field(None)\n\n    def __str__(self) -&gt; str:\n        return f\"{self.value} {self.currency or ''}\".strip()\n</code></pre>"},{"location":"concepts/pydantic-templates/#template-organization","title":"Template Organization","text":""},{"location":"concepts/pydantic-templates/#file-structure","title":"File Structure","text":"<pre><code>\"\"\"\nTemplate for extracting invoice data.\nDefines entities, components, and relationships for invoice documents.\n\"\"\"\n\n# --- Required Imports ---\nfrom pydantic import BaseModel, ConfigDict, Field, field_validator\nfrom typing import Any, List, Optional\nfrom datetime import date\n\n# --- Edge Helper Function ---\ndef edge(label: str, **kwargs: Any) -&gt; Any:\n    return Field(..., json_schema_extra={\"edge_label\": label}, **kwargs)\n\n# --- Reusable Components ---\nclass Address(BaseModel):\n    \"\"\"Physical address component.\"\"\"\n    model_config = ConfigDict(is_entity=False)\n    # ... fields ...\n\nclass MonetaryAmount(BaseModel):\n    \"\"\"Monetary value component.\"\"\"\n    model_config = ConfigDict(is_entity=False)\n    # ... fields ...\n\n# --- Reusable Entities ---\nclass Organization(BaseModel):\n    \"\"\"Organization entity.\"\"\"\n    model_config = ConfigDict(graph_id_fields=[\"name\"])\n    # ... fields ...\n\nclass Person(BaseModel):\n    \"\"\"Person entity.\"\"\"\n    model_config = ConfigDict(graph_id_fields=[\"name\", \"date_of_birth\"])\n    # ... fields ...\n\n# --- Domain-Specific Models ---\nclass LineItem(BaseModel):\n    \"\"\"Invoice line item.\"\"\"\n    model_config = ConfigDict(graph_id_fields=[\"description\", \"amount\"])\n    # ... fields ...\n\n# --- Root Document Model ---\nclass Invoice(BaseModel):\n    \"\"\"Complete invoice document.\"\"\"\n    model_config = ConfigDict(graph_id_fields=[\"invoice_number\"])\n    # ... fields ...\n</code></pre>"},{"location":"concepts/pydantic-templates/#common-patterns","title":"Common Patterns","text":""},{"location":"concepts/pydantic-templates/#pattern-1-document-with-entities","title":"Pattern 1: Document with Entities","text":"<pre><code>class Invoice(BaseModel):\n    \"\"\"Invoice document.\"\"\"\n    model_config = ConfigDict(graph_id_fields=[\"invoice_number\"])\n\n    invoice_number: str = Field(...)\n    issue_date: date = Field(...)\n\n    issuer: Organization = edge(label=\"ISSUED_BY\")\n    recipient: Person = edge(label=\"SENT_TO\")\n    items: List[LineItem] = edge(\n        label=\"CONTAINS_ITEM\",\n        default_factory=list\n    )\n</code></pre>"},{"location":"concepts/pydantic-templates/#pattern-2-hierarchical-structure","title":"Pattern 2: Hierarchical Structure","text":"<pre><code>class Section(BaseModel):\n    \"\"\"Document section.\"\"\"\n    model_config = ConfigDict(graph_id_fields=[\"title\"])\n\n    title: str = Field(...)\n    content: str = Field(...)\n    subsections: List[\"Section\"] = edge(\n        label=\"HAS_SUBSECTION\",\n        default_factory=list\n    )\n</code></pre>"},{"location":"concepts/pydantic-templates/#pattern-3-flexible-measurement","title":"Pattern 3: Flexible Measurement","text":"<pre><code>class Measurement(BaseModel):\n    \"\"\"Flexible measurement supporting single values or ranges.\"\"\"\n    model_config = ConfigDict(is_entity=False)\n\n    name: str = Field(...)\n    numeric_value: Optional[float] = Field(None)\n    numeric_value_min: Optional[float] = Field(None)\n    numeric_value_max: Optional[float] = Field(None)\n    unit: Optional[str] = Field(None)\n\n    @model_validator(mode=\"after\")\n    def validate_values(self) -&gt; Self:\n        has_single = self.numeric_value is not None\n        has_range = (self.numeric_value_min is not None or \n                     self.numeric_value_max is not None)\n        if has_single and has_range:\n            raise ValueError(\"Cannot specify both single and range values\")\n        return self\n</code></pre>"},{"location":"concepts/pydantic-templates/#testing-templates","title":"Testing Templates","text":""},{"location":"concepts/pydantic-templates/#basic-test","title":"Basic Test","text":"<pre><code># test_template.py\nfrom my_template import Invoice, Organization, Person\n\ndef test_invoice_creation():\n    invoice = Invoice(\n        invoice_number=\"INV-001\",\n        issue_date=\"2024-01-15\",\n        issuer=Organization(name=\"Acme Corp\"),\n        recipient=Person(name=\"John Doe\", date_of_birth=\"1990-01-15\")\n    )\n\n    assert invoice.invoice_number == \"INV-001\"\n    assert invoice.issuer.name == \"Acme Corp\"\n    print(invoice.model_dump_json(indent=2))\n</code></pre>"},{"location":"concepts/pydantic-templates/#validation-test","title":"Validation Test","text":"<pre><code>def test_validation():\n    # Should raise validation error\n    try:\n        person = Person(\n            name=\"John Doe\",\n            email=\"INVALID EMAIL\"  # Should fail validation\n        )\n    except ValueError as e:\n        print(f\"Validation error: {e}\")\n</code></pre>"},{"location":"concepts/pydantic-templates/#best-practices-checklist","title":"Best Practices Checklist","text":"<p>When creating templates, ensure:</p> <ul> <li>[ ] All necessary imports included</li> <li>[ ] <code>edge()</code> helper function defined</li> <li>[ ] Entities have <code>graph_id_fields</code></li> <li>[ ] Components have <code>is_entity=False</code></li> <li>[ ] Field descriptions are detailed and LLM-friendly</li> <li>[ ] 2-5 realistic examples per field</li> <li>[ ] Validators for data quality</li> <li>[ ] Edge labels are descriptive and ALL_CAPS</li> <li>[ ] List edges use <code>default_factory=list</code></li> <li>[ ] <code>__str__</code> methods for entities</li> <li>[ ] Docstrings for all models</li> <li>[ ] Proper type hints (Optional, List, Union)</li> </ul>"},{"location":"concepts/pydantic-templates/#complete-example-template","title":"Complete Example Template","text":"<p>See the complete guide for a comprehensive template creation tutorial with advanced patterns and best practices.</p>"},{"location":"concepts/pydantic-templates/#next-steps","title":"Next Steps","text":"<ul> <li>Review example templates</li> <li>Learn about Graph Construction</li> <li>Understand Extraction Backends</li> <li>Explore Processing Strategies</li> </ul>"},{"location":"development/github/","title":"Complete GitHub Setup Guide","text":"<p>This guide walks you through setting up Docling Graph with full automation including semantic versioning, automated releases, and GitHub Pages documentation.</p>"},{"location":"development/github/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Initial Setup</li> <li>GitHub Configuration</li> <li>PyPI Configuration</li> <li>Local Development</li> <li>Verification</li> <li>Troubleshooting</li> </ol>"},{"location":"development/github/#initial-setup","title":"Initial Setup","text":""},{"location":"development/github/#1-clone-and-install","title":"1. Clone and Install","text":"<pre><code># Clone repository\ngit clone https://github.com/IBM/docling-graph.git\ncd docling-graph\n\n# Install with all dependencies\nuv sync --all-extras --dev\n\n# Install pre-commit hooks\nuv run pre-commit install\n</code></pre>"},{"location":"development/github/#2-configure-git","title":"2. Configure Git","text":"<p>Set up commit signing (required for DCO):</p> <pre><code># Configure globally\ngit config --global format.signoff true\n\n# Or sign each commit\ngit commit -s -m \"your message\"\n</code></pre>"},{"location":"development/github/#github-configuration","title":"GitHub Configuration","text":""},{"location":"development/github/#1-enable-github-pages","title":"1. Enable GitHub Pages","text":"<ol> <li>Go to Settings \u2192 Pages</li> <li>Source: Deploy from a branch</li> <li>Branch: gh-pages / root</li> <li>Click Save</li> </ol> <p>The documentation will be available at: <code>https://ibm.github.io/docling-graph</code></p>"},{"location":"development/github/#2-configure-branch-protection","title":"2. Configure Branch Protection","text":"<p>Go to Settings \u2192 Branches \u2192 Add rule for <code>main</code>:</p> <p>Required settings: - Require a pull request before merging - Require approvals: 1 - Require status checks to pass before merging - Require branches to be up to date before merging - Require conversation resolution before merging - Include administrators</p> <p>Optional but recommended: - Require signed commits - Require linear history</p>"},{"location":"development/github/#3-create-github-environments","title":"3. Create GitHub Environments","text":""},{"location":"development/github/#pypi-environment","title":"PyPI Environment","text":"<ol> <li>Go to Settings \u2192 Environments</li> <li>Click New environment</li> <li>Name: <code>pypi</code></li> <li>Deployment protection rules:</li> <li>Required reviewers (optional, for extra safety)</li> <li>Deployment branches: Selected branches \u2192 Add <code>main</code></li> <li>Click Save</li> </ol>"},{"location":"development/github/#testpypi-environment","title":"TestPyPI Environment","text":"<ol> <li>Click New environment</li> <li>Name: <code>testpypi</code></li> <li>No special protection needed</li> <li>Click Save</li> </ol>"},{"location":"development/github/#4-enable-security-features","title":"4. Enable Security Features","text":"<p>Go to Settings \u2192 Code security and analysis and enable the following:</p> <ul> <li>Dependency graph</li> <li>Dependabot alerts</li> <li>Dependabot security updates</li> </ul>"},{"location":"development/github/#5-create-repository-labels","title":"5. Create Repository Labels","text":"<p>Go to Issues \u2192 Labels and create:</p> <pre><code>documentation (color: #0075ca)\ntests (color: #d4c5f9)\nci/cd (color: #28a745)\ncore (color: #d73a4a)\ncli (color: #fbca04)\n</code></pre> <p>Or use the GitHub CLI:</p> <pre><code>gh label create documentation --color 0075ca\ngh label create tests --color d4c5f9\ngh label create ci/cd --color 28a745\ngh label create core --color d73a4a\ngh label create cli --color fbca04\n</code></pre>"},{"location":"development/github/#pypi-configuration","title":"PyPI Configuration","text":""},{"location":"development/github/#1-create-pypi-account","title":"1. Create PyPI Account","text":"<p>If you don't have one: 1. Go to https://pypi.org/account/register/ 2. Verify your email 3. Enable 2FA (required for trusted publishing)</p>"},{"location":"development/github/#2-configure-trusted-publishing-pypi","title":"2. Configure Trusted Publishing (PyPI)","text":"<p>No API tokens needed! Use OIDC authentication:</p> <ol> <li>Go to https://pypi.org/manage/account/publishing/</li> <li>Click Add a new publisher</li> <li>Fill in:</li> <li>PyPI Project Name: <code>docling-graph</code></li> <li>Owner: <code>IBM</code> (or your GitHub username)</li> <li>Repository name: <code>docling-graph</code></li> <li>Workflow name: <code>release.yml</code></li> <li>Environment name: <code>pypi</code></li> <li>Click Add</li> </ol>"},{"location":"development/github/#3-configure-trusted-publishing-testpypi","title":"3. Configure Trusted Publishing (TestPyPI)","text":"<p>Same process for TestPyPI:</p> <ol> <li>Go to https://test.pypi.org/manage/account/publishing/</li> <li>Click Add a new publisher</li> <li>Fill in same details as above</li> <li>Environment name: <code>testpypi</code></li> <li>Click Add</li> </ol>"},{"location":"development/github/#4-verify-configuration","title":"4. Verify Configuration","text":"<pre><code># Check if project exists on PyPI\npip index versions docling-graph\n\n# Check TestPyPI\npip index versions -i https://test.pypi.org/simple/ docling-graph\n</code></pre>"},{"location":"development/github/#local-development","title":"Local Development","text":""},{"location":"development/github/#1-development-workflow","title":"1. Development Workflow","text":"<pre><code># Create feature branch\ngit checkout -b feature/my-feature\n\n# Make changes\n# ... edit files ...\n\n# Run pre-commit checks\nuv run pre-commit run --all-files\n\n# Run tests\nuv run pytest\n\n# Commit with conventional format and sign-off\ngit commit -s -m \"feat: add new feature\"\n\n# Push and create PR\ngit push -u origin feature/my-feature\n</code></pre>"},{"location":"development/github/#2-conventional-commits","title":"2. Conventional Commits","text":"<p>Use these commit types:</p> Type Description Version Bump <code>feat:</code> New feature Minor (0.x.0) <code>fix:</code> Bug fix Patch (0.0.x) <code>perf:</code> Performance Patch (0.0.x) <code>docs:</code> Documentation None <code>test:</code> Tests None <code>chore:</code> Maintenance None <p>Example: <pre><code>git commit -s -m \"feat(cli): add batch processing command\"\ngit commit -s -m \"fix(core): resolve memory leak\"\ngit commit -s -m \"docs: update installation guide\"\n</code></pre></p>"},{"location":"development/github/#3-testing-locally","title":"3. Testing Locally","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with coverage\nuv run pytest --cov=docling_graph\n\n# Run specific test file\nuv run pytest tests/unit/test_pipeline.py\n\n# Run type checking\nuv run mypy docling_graph\n\n# Run linting\nuv run ruff check .\nuv run ruff format --check .\n</code></pre>"},{"location":"development/github/#4-build-documentation-locally","title":"4. Build Documentation Locally","text":"<pre><code># Install docs dependencies\npip install mkdocs-material mkdocstrings[python] pymdown-extensions\n\n# Serve locally\nmkdocs serve\n\n# Open http://127.0.0.1:8000\n</code></pre>"},{"location":"development/github/#verification","title":"Verification","text":""},{"location":"development/github/#1-test-ci-workflows","title":"1. Test CI Workflows","text":"<p>Create a test PR:</p> <pre><code>git checkout -b test/ci-verification\necho \"# Test\" &gt;&gt; test.md\ngit add test.md\ngit commit -s -m \"test: verify CI workflows\"\ngit push -u origin test/ci-verification\n</code></pre> <p>Create PR and verify: - Pre-commit checks pass - Tests pass on Python 3.10 and 3.12 - Type checking passes - Linting passes - DCO check passes</p>"},{"location":"development/github/#2-test-semantic-release","title":"2. Test Semantic Release","text":"<p>After merging a PR with conventional commits to <code>main</code>:</p> <ol> <li>Check Actions \u2192 Semantic Release workflow</li> <li>Verify it creates a new version and tag</li> <li>Check that CHANGELOG.md is updated</li> <li>Verify version in <code>pyproject.toml</code> and <code>__init__.py</code></li> </ol>"},{"location":"development/github/#3-test-release-workflow","title":"3. Test Release Workflow","text":"<p>After semantic release creates a tag:</p> <ol> <li>Check Actions \u2192 Release workflow</li> <li>Verify stages:</li> <li>TestPyPI publish succeeds</li> <li>PyPI publish succeeds</li> <li>GitHub Release created</li> <li>Check package on PyPI: https://pypi.org/project/docling-graph/</li> <li>Test installation: <code>pip install docling-graph</code></li> </ol>"},{"location":"development/github/#4-test-documentation-deployment","title":"4. Test Documentation Deployment","text":"<p>After pushing docs changes to <code>main</code>:</p> <ol> <li>Check Actions \u2192 Documentation workflow</li> <li>Verify deployment succeeds</li> <li>Visit: https://ibm.github.io/docling-graph</li> <li>Verify content is updated</li> </ol>"},{"location":"development/github/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/github/#issue-pre-commit-checks-fail","title":"Issue: Pre-commit Checks Fail","text":"<p>Solution: <pre><code># Run locally to see errors\nuv run pre-commit run --all-files\n\n# Fix issues and commit\ngit add .\ngit commit -s -m \"fix: resolve linting issues\"\n</code></pre></p>"},{"location":"development/github/#issue-dco-check-fails","title":"Issue: DCO Check Fails","text":"<p>Solution: <pre><code># Amend last commit to add sign-off\ngit commit --amend -s --no-edit\n\n# Force push (if already pushed)\ngit push --force-with-lease\n\n# For multiple commits\ngit rebase --signoff HEAD~3  # Last 3 commits\ngit push --force-with-lease\n</code></pre></p>"},{"location":"development/github/#issue-semantic-release-doesnt-create-version","title":"Issue: Semantic Release Doesn't Create Version","text":"<p>Causes: - No conventional commits since last release - Only non-release commits (docs, chore, etc.)</p> <p>Solution: <pre><code># Check commit history\ngit log --oneline\n\n# Ensure commits follow conventional format\ngit commit -s -m \"feat: add new feature\"  # Will trigger minor bump\ngit commit -s -m \"fix: resolve bug\"       # Will trigger patch bump\n</code></pre></p>"},{"location":"development/github/#issue-pypi-publishing-fails","title":"Issue: PyPI Publishing Fails","text":"<p>Common causes: 1. OIDC not configured 2. Version already exists 3. Environment not set up</p> <p>Solutions: <pre><code># Verify OIDC configuration\n# Go to PyPI \u2192 Account \u2192 Publishing\n\n# Check if version exists\npip index versions docling-graph\n\n# Verify GitHub environment exists\n# Settings \u2192 Environments \u2192 pypi\n</code></pre></p>"},{"location":"development/github/#issue-documentation-not-deploying","title":"Issue: Documentation Not Deploying","text":"<p>Solutions: <pre><code># Check workflow logs\n# Actions \u2192 Documentation \u2192 View logs\n\n# Verify gh-pages branch exists\ngit branch -r | grep gh-pages\n\n# Manually trigger deployment\n# Actions \u2192 Documentation \u2192 Run workflow\n</code></pre></p>"},{"location":"development/github/#issue-tests-fail-in-ci-but-pass-locally","title":"Issue: Tests Fail in CI but Pass Locally","text":"<p>Common causes: - Different Python version - Missing dependencies - Environment-specific issues</p> <p>Solutions: <pre><code># Test with specific Python version\npyenv install 3.10\npyenv local 3.10\nuv run pytest\n\n# Check dependencies\nuv sync --all-extras --dev\n\n# Run in clean environment\nuv venv --python 3.10\nsource .venv/bin/activate\nuv sync --all-extras --dev\npytest\n</code></pre></p>"},{"location":"development/github/#next-steps","title":"Next Steps","text":"<p>After setup is complete:</p> <ol> <li>Read the documentation:</li> <li>Release Process</li> <li> <p>Contributing Guide</p> </li> <li> <p>Start developing:</p> </li> <li>Create feature branch</li> <li>Make changes</li> <li>Write tests</li> <li> <p>Submit PR</p> </li> <li> <p>Monitor automation:</p> </li> <li>Watch GitHub Actions</li> <li>Review release notes</li> <li>Check documentation updates</li> </ol>"},{"location":"development/github/#quick-reference","title":"Quick Reference","text":""},{"location":"development/github/#common-commands","title":"Common Commands","text":"<pre><code># Development\nuv sync --all-extras --dev\nuv run pre-commit run --all-files\nuv run pytest\nuv run mypy docling_graph\n\n# Documentation\nmkdocs serve\nmkdocs build\n\n# Release (automatic)\ngit commit -s -m \"feat: new feature\"\ngit push\n# Semantic release handles the rest\n\n# Manual release (emergency only)\ngit tag v0.3.0\ngit push origin v0.3.0\n</code></pre>"},{"location":"development/github/#useful-links","title":"Useful Links","text":"<ul> <li>Repository: https://github.com/IBM/docling-graph</li> <li>Documentation: https://ibm.github.io/docling-graph</li> <li>PyPI: https://pypi.org/project/docling-graph/</li> <li>Issues: https://github.com/IBM/docling-graph/issues</li> <li>Actions: https://github.com/IBM/docling-graph/actions</li> </ul>"},{"location":"development/github/#support","title":"Support","text":"<p>If you encounter issues:</p> <ol> <li>Check Troubleshooting section</li> <li>Search GitHub Issues</li> <li>Create new issue with:</li> <li>Clear description</li> <li>Steps to reproduce</li> <li>Error messages</li> <li>Environment details</li> </ol>"},{"location":"development/github/#contributing","title":"Contributing","text":"<p>See CONTRIBUTING.md for detailed contribution guidelines.</p>"},{"location":"development/release/","title":"Release Process","text":"<p>This document describes the automated release process for Docling Graph.</p>"},{"location":"development/release/#overview","title":"Overview","text":"<p>Docling Graph uses a fully automated release process powered by:</p> <ul> <li>Python Semantic Release - Automatic version bumping and changelog generation</li> <li>GitHub Actions - Automated testing, building, and publishing</li> <li>Conventional Commits - Standardized commit messages for version determination</li> </ul>"},{"location":"development/release/#release-workflow","title":"Release Workflow","text":"<pre><code>graph TB\n    A[Commit with conventional format] --&gt; B[Push to main]\n    B --&gt; C[Semantic Release Workflow]\n    C --&gt; D{Analyze commits}\n    D --&gt;|feat:| E[Minor version bump]\n    D --&gt;|fix:| F[Patch version bump]\n    D --&gt;|BREAKING CHANGE:| G[Major version bump]\n    D --&gt;|No release commits| H[Skip release]\n    E --&gt; I[Update version in files]\n    F --&gt; I\n    G --&gt; I\n    I --&gt; J[Generate CHANGELOG]\n    J --&gt; K[Create git tag]\n    K --&gt; L[Push tag]\n    L --&gt; M[Release Workflow]\n    M --&gt; N[Build package]\n    N --&gt; O[Publish to TestPyPI]\n    O --&gt; P{Tests pass?}\n    P --&gt;|Yes| Q[Publish to PyPI]\n    P --&gt;|No| R[Notify &amp; stop]\n    Q --&gt; S[Create GitHub Release]\n    S --&gt; T[Upload artifacts]</code></pre>"},{"location":"development/release/#conventional-commits","title":"Conventional Commits","text":""},{"location":"development/release/#commit-format","title":"Commit Format","text":"<pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n\n&lt;body&gt;\n\n&lt;footer&gt;\n</code></pre>"},{"location":"development/release/#commit-types","title":"Commit Types","text":"Type Description Version Bump Example <code>feat</code> New feature Minor (0.x.0) <code>feat: add PDF batch processing</code> <code>fix</code> Bug fix Patch (0.0.x) <code>fix: resolve memory leak in chunker</code> <code>perf</code> Performance improvement Patch (0.0.x) <code>perf: optimize graph construction</code> <code>refactor</code> Code refactoring Minor (0.x.0) <code>refactor: simplify config loading</code> <code>docs</code> Documentation only None <code>docs: update installation guide</code> <code>style</code> Code style changes None <code>style: format with ruff</code> <code>test</code> Test additions/changes None <code>test: add integration tests</code> <code>chore</code> Maintenance tasks None <code>chore: update dependencies</code> <code>ci</code> CI/CD changes None <code>ci: add caching to workflows</code>"},{"location":"development/release/#breaking-changes","title":"Breaking Changes","text":"<p>To trigger a major version bump (x.0.0), include <code>BREAKING CHANGE:</code> in the commit footer:</p> <pre><code>feat: redesign configuration API\n\nBREAKING CHANGE: Configuration now uses PipelineConfig instead of dict.\nMigration guide available in docs/migration.md\n</code></pre> <p>Or use <code>!</code> after the type:</p> <pre><code>feat!: redesign configuration API\n</code></pre>"},{"location":"development/release/#making-a-release","title":"Making a Release","text":""},{"location":"development/release/#automatic-release-recommended","title":"Automatic Release (Recommended)","text":"<ol> <li>Make changes and commit with conventional format:</li> </ol> <pre><code>git checkout -b feature/my-feature\n# Make changes\ngit add .\ngit commit -s -m \"feat: add new extraction strategy\"\ngit push origin feature/my-feature\n</code></pre> <ol> <li>Create and merge PR:</li> </ol> <pre><code># Create PR on GitHub\n# Wait for CI checks to pass\n# Merge to main\n</code></pre> <ol> <li> <p>Automatic release happens:</p> </li> <li> <p>Semantic Release workflow analyzes commits</p> </li> <li>Determines version bump (if needed)</li> <li>Updates version in <code>pyproject.toml</code> and <code>__init__.py</code></li> <li>Generates CHANGELOG entry</li> <li>Creates and pushes git tag</li> <li>Release workflow triggers automatically</li> <li>Package published to TestPyPI, then PyPI</li> <li>GitHub Release created with artifacts</li> </ol>"},{"location":"development/release/#manual-release-emergency","title":"Manual Release (Emergency)","text":"<p>If you need to create a release manually:</p> <pre><code># 1. Update version manually\n# Edit pyproject.toml and docling_graph/__init__.py\n\n# 2. Update CHANGELOG.md\n# Add entry for new version\n\n# 3. Commit changes\ngit add .\ngit commit -s -m \"chore(release): 0.3.0\"\ngit push\n\n# 4. Create and push tag\ngit tag v0.3.0\ngit push origin v0.3.0\n\n# Release workflow will trigger automatically\n</code></pre>"},{"location":"development/release/#release-stages","title":"Release Stages","text":""},{"location":"development/release/#stage-1-semantic-release","title":"Stage 1: Semantic Release","text":"<p>Workflow: <code>.github/workflows/semantic-release.yml</code></p> <p>Triggers: Push to main branch</p> <p>Actions: 1. Analyzes commits since last release 2. Determines version bump (major/minor/patch) 3. Updates version in files 4. Generates CHANGELOG entry 5. Creates git commit and tag 6. Pushes changes</p> <p>Skip conditions: - No conventional commits found - Only non-release commits (docs, chore, etc.)</p>"},{"location":"development/release/#stage-2-testpypi-publishing","title":"Stage 2: TestPyPI Publishing","text":"<p>Workflow: <code>.github/workflows/release.yml</code> (job: test-build)</p> <p>Triggers: Git tag push (v..*)</p> <p>Actions: 1. Builds distribution packages 2. Publishes to TestPyPI 3. Tests installation from TestPyPI 4. Uploads artifacts for next stage</p> <p>Purpose: Catch issues before production release</p>"},{"location":"development/release/#stage-3-pypi-publishing","title":"Stage 3: PyPI Publishing","text":"<p>Workflow: <code>.github/workflows/release.yml</code> (job: publish-pypi)</p> <p>Triggers: Successful TestPyPI stage</p> <p>Actions: 1. Downloads build artifacts 2. Publishes to production PyPI</p> <p>Requirements: - TestPyPI tests passed - OIDC trusted publishing configured</p>"},{"location":"development/release/#stage-4-github-release","title":"Stage 4: GitHub Release","text":"<p>Workflow: <code>.github/workflows/release.yml</code> (job: create-release)</p> <p>Triggers: Successful PyPI publishing</p> <p>Actions: 1. Extracts release notes from CHANGELOG 2. Creates GitHub Release 3. Uploads distribution artifacts 4. Generates summary</p>"},{"location":"development/release/#configuration","title":"Configuration","text":""},{"location":"development/release/#pypi-trusted-publishing","title":"PyPI Trusted Publishing","text":"<p>Configure OIDC trusted publishing (no API tokens needed):</p> <ol> <li>For PyPI:</li> <li>Go to https://pypi.org/manage/account/publishing/</li> <li>Add GitHub as trusted publisher</li> <li>Repository: <code>IBM/docling-graph</code></li> <li>Workflow: <code>release.yml</code></li> <li> <p>Environment: <code>pypi</code></p> </li> <li> <p>For TestPyPI:</p> </li> <li>Go to https://test.pypi.org/manage/account/publishing/</li> <li>Add GitHub as trusted publisher</li> <li>Repository: <code>IBM/docling-graph</code></li> <li>Workflow: <code>release.yml</code></li> <li>Environment: <code>testpypi</code></li> </ol>"},{"location":"development/release/#github-environments","title":"GitHub Environments","text":"<p>Create environments in repository settings:</p> <ol> <li>pypi environment:</li> <li>Protection rules: Require approval (optional)</li> <li> <p>Deployment branches: Only <code>main</code></p> </li> <li> <p>testpypi environment:</p> </li> <li>No special protection needed</li> </ol>"},{"location":"development/release/#branch-protection","title":"Branch Protection","text":"<p>Configure branch protection for <code>main</code>:</p> <ul> <li>Require pull request reviews</li> <li>Require status checks to pass</li> <li>Require branches to be up to date</li> <li>Include administrators</li> </ul>"},{"location":"development/release/#versioning-strategy","title":"Versioning Strategy","text":"<p>Docling Graph follows Semantic Versioning 2.0.0:</p> <ul> <li>Major (x.0.0): Breaking changes</li> <li>Minor (0.x.0): New features (backward compatible)</li> <li>Patch (0.0.x): Bug fixes (backward compatible)</li> </ul>"},{"location":"development/release/#version-examples","title":"Version Examples","text":"<pre><code>0.1.0 \u2192 0.2.0  (feat: new feature)\n0.2.0 \u2192 0.2.1  (fix: bug fix)\n0.2.1 \u2192 1.0.0  (BREAKING CHANGE: API redesign)\n</code></pre>"},{"location":"development/release/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/release/#no-release-created","title":"No Release Created","text":"<p>Symptom: Semantic Release workflow runs but no release is created</p> <p>Causes: - No conventional commits since last release - Only non-release commits (docs, chore, etc.)</p> <p>Solution: Check commit messages follow conventional format</p>"},{"location":"development/release/#testpypi-publish-fails","title":"TestPyPI Publish Fails","text":"<p>Symptom: TestPyPI publishing fails</p> <p>Common causes: 1. Version already exists on TestPyPI 2. OIDC not configured 3. Package name conflict</p> <p>Solutions: <pre><code># Check if version exists\npip index versions -i https://test.pypi.org/simple/ docling-graph\n\n# Verify OIDC configuration in TestPyPI settings\n</code></pre></p>"},{"location":"development/release/#pypi-publish-fails","title":"PyPI Publish Fails","text":"<p>Symptom: PyPI publishing fails after TestPyPI success</p> <p>Common causes: 1. OIDC not configured for PyPI 2. Version already exists 3. Package validation errors</p> <p>Solutions: - Verify OIDC configuration - Check PyPI project settings - Review package metadata</p>"},{"location":"development/release/#github-release-fails","title":"GitHub Release Fails","text":"<p>Symptom: Release created but GitHub Release fails</p> <p>Common causes: 1. Insufficient permissions 2. Tag already has a release 3. CHANGELOG format issues</p> <p>Solutions: <pre><code># Check permissions\n# Verify GITHUB_TOKEN has write access\n\n# Check existing releases\ngh release list\n\n# Validate CHANGELOG format\n</code></pre></p>"},{"location":"development/release/#best-practices","title":"Best Practices","text":""},{"location":"development/release/#1-write-good-commit-messages","title":"1. Write Good Commit Messages","text":"<pre><code># Good\nfeat(cli): add batch processing command\nfix(extractor): resolve memory leak in chunker\ndocs: update installation guide\n\n# Bad\nupdate code\nfix bug\nchanges\n</code></pre>"},{"location":"development/release/#2-use-scopes","title":"2. Use Scopes","text":"<p>Scopes help organize changes:</p> <pre><code>feat(cli): ...\nfix(core): ...\ndocs(api): ...\ntest(integration): ...\n</code></pre>"},{"location":"development/release/#3-test-before-merging","title":"3. Test Before Merging","text":"<pre><code># Run tests locally\nuv run pytest\n\n# Run pre-commit checks\nuv run pre-commit run --all-files\n\n# Test build\nuv build\n</code></pre>"},{"location":"development/release/#4-review-changelog","title":"4. Review CHANGELOG","text":"<p>After release, verify CHANGELOG.md:</p> <ul> <li>Version number correct</li> <li>Release date accurate</li> <li>Changes properly categorized</li> <li>Breaking changes highlighted</li> </ul>"},{"location":"development/release/#5-monitor-releases","title":"5. Monitor Releases","text":"<ul> <li>Check GitHub Actions for workflow status</li> <li>Verify package on PyPI</li> <li>Test installation: <code>pip install docling-graph==x.y.z</code></li> <li>Review GitHub Release notes</li> </ul>"},{"location":"development/release/#release-checklist","title":"Release Checklist","text":"<p>Before merging to main:</p> <ul> <li>[ ] All tests pass</li> <li>[ ] Pre-commit checks pass</li> <li>[ ] Commit messages follow conventional format</li> <li>[ ] Breaking changes documented</li> <li>[ ] Migration guide created (if needed)</li> <li>[ ] Examples updated</li> <li>[ ] Documentation updated</li> </ul> <p>After release:</p> <ul> <li>[ ] Verify package on PyPI</li> <li>[ ] Test installation</li> <li>[ ] Check GitHub Release</li> <li>[ ] Review CHANGELOG</li> <li>[ ] Announce release (if major/minor)</li> </ul>"},{"location":"development/release/#emergency-procedures","title":"Emergency Procedures","text":""},{"location":"development/release/#rollback-a-release","title":"Rollback a Release","text":"<p>If a critical issue is found:</p> <ol> <li>Yank the release from PyPI:</li> </ol> <pre><code># Requires PyPI credentials\npip install twine\ntwine upload --repository pypi --skip-existing dist/*\n</code></pre> <ol> <li>Create hotfix:</li> </ol> <pre><code>git checkout -b hotfix/critical-fix\n# Fix the issue\ngit commit -s -m \"fix: critical issue description\"\n# Create PR and merge\n</code></pre> <ol> <li>New release will be created automatically</li> </ol>"},{"location":"development/release/#skip-ci-for-commits","title":"Skip CI for Commits","text":"<p>To skip CI workflows:</p> <pre><code>git commit -m \"docs: update README [skip ci]\"\n</code></pre>"},{"location":"development/release/#resources","title":"Resources","text":"<ul> <li>Semantic Versioning</li> <li>Conventional Commits</li> <li>Python Semantic Release</li> <li>PyPI Trusted Publishing</li> <li>GitHub Actions</li> </ul>"},{"location":"examples/","title":"Docling Graph Examples","text":"<p>Welcome to the <code>docling-graph</code> examples. This directory contains all the resources you need to get started.</p>"},{"location":"examples/#project-structure","title":"Project Structure","text":"<ul> <li><code>/examples/scripts/</code>: Python script examples and the CLI recipes.</li> <li><code>/examples/data/</code>: Sample PDF and image files used by the scripts.</li> <li><code>/examples/templates/</code>: The Pydantic schemas (e.g., <code>invoice.py</code>) that define what data to extract.</li> </ul>"},{"location":"examples/#10-core-recipes","title":"10 Core Recipes","text":"<p>All Python scripts are located in the <code>/examples/scripts/</code> folder and are designed to be run from the project's root directory.</p> <ol> <li><code>01_vlm_from_image.py</code>: The \"Hello World.\" Extracts from a single image using the VLM.</li> <li><code>02_vlm_from_pdf_page.py</code>: Shows the VLM backend on a single-page PDF.</li> <li><code>03_llm_remote_api.py</code>: Standard LLM workflow using a remote API (Mistral) on a multi-page PDF.</li> <li><code>04_llm_local_ollama.py</code>: Uses a local LLM (Ollama) on the same PDF.</li> <li><code>05_llm_with_consolidation.py</code>: Advanced merging strategy using an LLM to consolidate results.</li> <li><code>06_llm_one_to_one.py</code>: Processes each page individually (<code>one-to-one</code>) and combines them into one graph.</li> <li><code>07_llm_no_chunking.py</code>: Processes a short document in a single pass (disables chunking).</li> <li><code>08_llm_with_vision_config.py</code>: Hybrid mode: uses the <code>vision</code> config for layout-aware chunks for the LLM.</li> <li><code>09_export_to_cypher.py</code>: Shows how to change the output format to a Cypher script for Neo4j.</li> <li><code>10_cli_recipes.md</code>: A markdown file showing the CLI (command-line) equivalents for all the examples above.</li> </ol>"},{"location":"examples/scripts/10_cli_recipes/","title":"Example 10: CLI Recipes","text":"<p>These recipes show how to run the <code>docling-graph convert</code> command from your project's root directory.</p> <p>All paths (<code>--template</code>, source file, <code>--output-dir</code>) are relative to the root.</p>"},{"location":"examples/scripts/10_cli_recipes/#recipe-1-vlm-from-image","title":"Recipe 1: VLM from Image","text":"<p>(Python: <code>01_vlm_from_image.py</code>)</p> <pre><code>uv run docling-graph convert \"docs/examples/data/invoice/sample_invoice.jpg\" \\\n    --template \"docs.examples.templates.invoice.Invoice\" \\\n    --output-dir \"outputs/cli_01\" \\\n    --backend \"vlm\" \\\n    --processing-mode \"one-to-one\" \\\n    --docling-pipeline \"vision\"\n````\n\n-----\n\n### Recipe 3: Remote LLM (Mistral)\n\n(Python: `03_llm_remote_api.py`)\n\n*(Requires `MISTRAL_API_KEY` env var)*\n\n```bash\nuv run docling-graph convert \"docs/examples/data/research_paper/rheology.pdf\" \\\n    --template \"docs.examples.templates.rheology_research.Research\" \\\n    --output-dir \"outputs/cli_03\" \\\n    --backend \"llm\" \\\n    --inference \"remote\" \\\n    --provider \"mistral\" \\\n    --model \"mistral-large-latest\" \\\n    --processing-mode \"many-to-one\" \\\n    --use-chunking \\\n    --no-llm-consolidation\n</code></pre>"},{"location":"examples/scripts/10_cli_recipes/#recipe-4-local-llm-ollama","title":"Recipe 4: Local LLM (Ollama)","text":"<p>(Python: <code>04_llm_local_ollama.py</code>)</p> <p>(Requires Ollama server to be running)</p> <pre><code>uv run docling-graph convert \"docs/examples/data/research_paper/rheology.pdf\" \\\n    --template \"docs.examples.templates.rheology_research.Research\" \\\n    --output-dir \"outputs/cli_04\" \\\n    --backend \"llm\" \\\n    --inference \"local\" \\\n    --provider \"ollama\" \\\n    --model \"llama3:8b\" \\\n    --processing-mode \"many-to-one\" \\\n    --use-chunking \\\n    --no-llm-consolidation\n</code></pre>"},{"location":"examples/scripts/10_cli_recipes/#recipe-5-llm-with-consolidation","title":"Recipe 5: LLM with Consolidation","text":"<p>(Python: <code>05_llm_with_consolidation.py</code>)</p> <pre><code>uv run docling-graph convert \"docs/examples/data/research_paper/rheology.pdf\" \\\n    --template \"docs.examples.templates.rheology_research.Research\" \\\n    --output-dir \"outputs/cli_05\" \\\n    --backend \"llm\" \\\n    --inference \"remote\" \\\n    --provider \"mistral\" \\\n    --processing-mode \"many-to-one\" \\\n    --use-chunking \\\n    --llm-consolidation\n</code></pre>"},{"location":"examples/scripts/10_cli_recipes/#recipe-8-llm-with-vision-config-hybrid","title":"Recipe 8: LLM with 'vision' Config (Hybrid)","text":"<p>(Python: <code>08_llm_with_vision_config.py</code>)</p> <pre><code>uv run docling-graph convert \"docs/examples/data/research_paper/rheology.pdf\" \\\n    --template \"docs.examples.templates.rheology_research.Research\" \\\n    --output-dir \"outputs/cli_08\" \\\n    --backend \"llm\" \\\n    --inference \"local\" \\\n    --provider \"ollama\" \\\n    --model \"llama3:8b\" \\\n    --docling-pipeline \"vision\" \\\n    --processing-mode \"many-to-one\" \\\n    --use-chunking\n</code></pre>"},{"location":"examples/scripts/10_cli_recipes/#recipe-9-export-to-cypher","title":"Recipe 9: Export to Cypher","text":"<p>(Python: <code>09_export_to_cypher.py</code>)</p> <pre><code>uv run docling-graph convert \"docs/examples/data/invoice/sample_invoice.jpg\" \\\n    --template \"docs.examples.templates.invoice.Invoice\" \\\n    --output-dir \"outputs/cli_09\" \\\n    --backend \"vlm\" \\\n    --docling-pipeline \"vision\" \\\n    --export-format \"cypher\"\n</code></pre>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Learn how to configure Docling Graph for your specific use case.</p>"},{"location":"getting-started/configuration/#configuration-methods","title":"Configuration Methods","text":""},{"location":"getting-started/configuration/#1-python-configuration","title":"1. Python Configuration","text":"<p>Use <code>PipelineConfig</code> for programmatic configuration:</p> <pre><code>from docling_graph import PipelineConfig\n\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"openai\",\n    model_override=\"gpt-4\",\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"getting-started/configuration/#2-cli-configuration","title":"2. CLI Configuration","text":"<p>Use command-line arguments:</p> <pre><code>docling-graph convert document.pdf \\\n    --template \"module.Template\" \\\n    --backend llm \\\n    --inference remote \\\n    --provider openai \\\n    --model gpt-4 \\\n    --output-dir outputs\n</code></pre>"},{"location":"getting-started/configuration/#3-configuration-file","title":"3. Configuration File","text":"<p>Create a YAML configuration file:</p> <pre><code># config.yaml\nsource: document.pdf\ntemplate: module.Template\nbackend: llm\ninference: remote\nprovider_override: openai\nmodel_override: gpt-4\noutput_dir: outputs\n</code></pre> <p>Load it in Python:</p> <pre><code>from docling_graph import run_pipeline\nfrom docling_graph.config import load_config\n\nconfig = load_config(\"config.yaml\")\nrun_pipeline(config)\n</code></pre>"},{"location":"getting-started/configuration/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"getting-started/configuration/#core-parameters","title":"Core Parameters","text":""},{"location":"getting-started/configuration/#source","title":"source","text":"<ul> <li>Type: <code>str</code> or <code>Path</code></li> <li>Required: Yes</li> <li>Description: Path to the input document (PDF, image, etc.)</li> </ul> <pre><code>source=\"document.pdf\"\nsource=\"/path/to/document.pdf\"\n</code></pre>"},{"location":"getting-started/configuration/#template","title":"template","text":"<ul> <li>Type: <code>Type[BaseModel]</code> or <code>str</code></li> <li>Required: Yes</li> <li>Description: Pydantic model defining the extraction schema</li> </ul> <pre><code># As class\ntemplate=YourTemplate\n\n# As dotted path (CLI)\ntemplate=\"module.submodule.YourTemplate\"\n</code></pre>"},{"location":"getting-started/configuration/#output_dir","title":"output_dir","text":"<ul> <li>Type: <code>str</code> or <code>Path</code></li> <li>Required: Yes</li> <li>Description: Directory for output files</li> </ul> <pre><code>output_dir=\"outputs\"\noutput_dir=\"outputs/experiment_1\"\n</code></pre>"},{"location":"getting-started/configuration/#backend-configuration","title":"Backend Configuration","text":""},{"location":"getting-started/configuration/#backend","title":"backend","text":"<ul> <li>Type: <code>str</code></li> <li>Options: <code>\"vlm\"</code>, <code>\"llm\"</code></li> <li>Default: <code>\"vlm\"</code></li> <li>Description: Extraction backend to use</li> </ul> <pre><code># Local VLM (Docling)\nbackend=\"vlm\"\n\n# LLM-based extraction\nbackend=\"llm\"\n</code></pre>"},{"location":"getting-started/configuration/#inference","title":"inference","text":"<ul> <li>Type: <code>str</code></li> <li>Options: <code>\"local\"</code>, <code>\"remote\"</code></li> <li>Default: <code>\"remote\"</code></li> <li>Description: Inference mode (only for LLM backend)</li> </ul> <pre><code># Remote API\ninference=\"remote\"\n\n# Local inference\ninference=\"local\"\n</code></pre>"},{"location":"getting-started/configuration/#provider-configuration","title":"Provider Configuration","text":""},{"location":"getting-started/configuration/#provider_override","title":"provider_override","text":"<ul> <li>Type: <code>str</code> or <code>None</code></li> <li>Options: <code>\"openai\"</code>, <code>\"mistral\"</code>, <code>\"gemini\"</code>, <code>\"watsonx\"</code>, <code>\"ollama\"</code>, <code>\"vllm\"</code></li> <li>Default: <code>None</code></li> <li>Description: Override the LLM provider</li> </ul> <pre><code># Remote providers\nprovider_override=\"openai\"\nprovider_override=\"mistral\"\nprovider_override=\"gemini\"\nprovider_override=\"watsonx\"\n\n# Local providers\nprovider_override=\"ollama\"\nprovider_override=\"vllm\"\n</code></pre>"},{"location":"getting-started/configuration/#model_override","title":"model_override","text":"<ul> <li>Type: <code>str</code> or <code>None</code></li> <li>Default: <code>None</code></li> <li>Description: Override the model name</li> </ul> <pre><code># OpenAI models\nmodel_override=\"gpt-4\"\nmodel_override=\"gpt-3.5-turbo\"\n\n# Mistral models\nmodel_override=\"mistral-medium-latest\"\nmodel_override=\"mistral-large-latest\"\n\n# Gemini models\nmodel_override=\"gemini-pro\"\n\n# Ollama models\nmodel_override=\"llama2\"\nmodel_override=\"mistral\"\n</code></pre>"},{"location":"getting-started/configuration/#processing-configuration","title":"Processing Configuration","text":""},{"location":"getting-started/configuration/#processing_mode","title":"processing_mode","text":"<ul> <li>Type: <code>str</code></li> <li>Options: <code>\"one-to-one\"</code>, <code>\"many-to-one\"</code></li> <li>Default: <code>\"one-to-one\"</code></li> <li>Description: How to process multi-page documents</li> </ul> <pre><code># Process each page separately\nprocessing_mode=\"one-to-one\"\n\n# Combine all pages into one extraction\nprocessing_mode=\"many-to-one\"\n</code></pre>"},{"location":"getting-started/configuration/#use_chunking","title":"use_chunking","text":"<ul> <li>Type: <code>bool</code></li> <li>Default: <code>False</code></li> <li>Description: Enable hybrid chunking for better context</li> </ul> <pre><code># Enable chunking\nuse_chunking=True\n\n# Disable chunking\nuse_chunking=False\n</code></pre>"},{"location":"getting-started/configuration/#llm_consolidation","title":"llm_consolidation","text":"<ul> <li>Type: <code>bool</code></li> <li>Default: <code>False</code></li> <li>Description: Use LLM to consolidate extracted data</li> </ul> <pre><code># LLM-based consolidation\nllm_consolidation=True\n\n# Programmatic consolidation\nllm_consolidation=False\n</code></pre>"},{"location":"getting-started/configuration/#export-configuration","title":"Export Configuration","text":""},{"location":"getting-started/configuration/#export_formats","title":"export_formats","text":"<ul> <li>Type: <code>List[str]</code></li> <li>Options: <code>\"json\"</code>, <code>\"csv\"</code>, <code>\"cypher\"</code>, <code>\"docling\"</code>, <code>\"markdown\"</code></li> <li>Default: <code>[\"json\", \"csv\", \"cypher\"]</code></li> <li>Description: Output formats to generate</li> </ul> <pre><code># All formats\nexport_formats=[\"json\", \"csv\", \"cypher\", \"docling\", \"markdown\"]\n\n# Specific formats\nexport_formats=[\"json\", \"cypher\"]\n</code></pre>"},{"location":"getting-started/configuration/#generate_visualization","title":"generate_visualization","text":"<ul> <li>Type: <code>bool</code></li> <li>Default: <code>True</code></li> <li>Description: Generate interactive HTML visualization</li> </ul> <pre><code>generate_visualization=True\n</code></pre>"},{"location":"getting-started/configuration/#generate_report","title":"generate_report","text":"<ul> <li>Type: <code>bool</code></li> <li>Default: <code>True</code></li> <li>Description: Generate markdown report</li> </ul> <pre><code>generate_report=True\n</code></pre>"},{"location":"getting-started/configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/configuration/#llm-configuration","title":"LLM Configuration","text":"<p>For fine-grained control over LLM behavior:</p> <pre><code>from docling_graph.llm_clients import LLMConfig\n\nllm_config = LLMConfig(\n    provider=\"openai\",\n    model=\"gpt-4\",\n    temperature=0.1,\n    max_tokens=4000,\n    timeout=60,\n    max_retries=3\n)\n\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    llm_config=llm_config,\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"getting-started/configuration/#vlm-configuration","title":"VLM Configuration","text":"<p>For VLM-specific settings:</p> <pre><code>from docling_graph.config import VLMConfig\n\nvlm_config = VLMConfig(\n    use_ocr=True,\n    extract_tables=True,\n    extract_images=True\n)\n\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"vlm\",\n    vlm_config=vlm_config,\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"getting-started/configuration/#chunking-configuration","title":"Chunking Configuration","text":"<p>Control chunking behavior:</p> <pre><code>config = PipelineConfig(\n    source=\"document.pdf\",\n    template=YourTemplate,\n    backend=\"llm\",\n    use_chunking=True,\n    chunk_size=1000,        # Characters per chunk\n    chunk_overlap=200,      # Overlap between chunks\n    output_dir=\"outputs\"\n)\n</code></pre>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"getting-started/configuration/#api-keys","title":"API Keys","text":"<pre><code># OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Mistral\nexport MISTRAL_API_KEY=\"...\"\n\n# Google Gemini\nexport GEMINI_API_KEY=\"...\"\n\n# IBM WatsonX\nexport WATSONX_API_KEY=\"...\"\nexport WATSONX_PROJECT_ID=\"...\"\nexport WATSONX_URL=\"https://us-south.ml.cloud.ibm.com\"\n</code></pre>"},{"location":"getting-started/configuration/#other-settings","title":"Other Settings","text":"<pre><code># Logging level\nexport LOG_LEVEL=\"INFO\"  # DEBUG, INFO, WARNING, ERROR\n\n# Cache directory\nexport DOCLING_CACHE_DIR=\"~/.cache/docling\"\n\n# Temporary directory\nexport TEMP_DIR=\"/tmp/docling\"\n</code></pre>"},{"location":"getting-started/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"getting-started/configuration/#example-1-simple-invoice-processing","title":"Example 1: Simple Invoice Processing","text":"<pre><code>config = PipelineConfig(\n    source=\"invoice.pdf\",\n    template=Invoice,\n    backend=\"vlm\",\n    output_dir=\"outputs/invoices\"\n)\n</code></pre>"},{"location":"getting-started/configuration/#example-2-research-paper-with-llm","title":"Example 2: Research Paper with LLM","text":"<pre><code>config = PipelineConfig(\n    source=\"paper.pdf\",\n    template=ResearchPaper,\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"mistral\",\n    model_override=\"mistral-medium-latest\",\n    processing_mode=\"many-to-one\",\n    use_chunking=True,\n    llm_consolidation=False,\n    output_dir=\"outputs/papers\"\n)\n</code></pre>"},{"location":"getting-started/configuration/#example-3-local-processing-with-ollama","title":"Example 3: Local Processing with Ollama","text":"<pre><code>config = PipelineConfig(\n    source=\"document.pdf\",\n    template=Document,\n    backend=\"llm\",\n    inference=\"local\",\n    provider_override=\"ollama\",\n    model_override=\"llama2\",\n    output_dir=\"outputs/local\"\n)\n</code></pre>"},{"location":"getting-started/configuration/#example-4-batch-processing","title":"Example 4: Batch Processing","text":"<pre><code>from pathlib import Path\n\nfor pdf in Path(\"documents\").glob(\"*.pdf\"):\n    config = PipelineConfig(\n        source=str(pdf),\n        template=YourTemplate,\n        backend=\"llm\",\n        output_dir=f\"outputs/{pdf.stem}\"\n    )\n    run_pipeline(config)\n</code></pre>"},{"location":"getting-started/configuration/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/configuration/#1-choose-the-right-backend","title":"1. Choose the Right Backend","text":"<ul> <li>VLM: Best for structured documents (invoices, forms, ID cards)</li> <li>LLM: Best for unstructured text (research papers, reports, articles)</li> </ul>"},{"location":"getting-started/configuration/#2-optimize-processing-mode","title":"2. Optimize Processing Mode","text":"<ul> <li>one-to-one: Better for documents with distinct page-level information</li> <li>many-to-one: Better for documents with continuous narrative</li> </ul>"},{"location":"getting-started/configuration/#3-use-chunking-wisely","title":"3. Use Chunking Wisely","text":"<ul> <li>Enable for long documents (&gt;10 pages)</li> <li>Disable for short, structured documents</li> <li>Adjust chunk size based on model context window</li> </ul>"},{"location":"getting-started/configuration/#4-select-appropriate-models","title":"4. Select Appropriate Models","text":"<ul> <li>GPT-4: Best quality, higher cost</li> <li>GPT-3.5-turbo: Good balance</li> <li>Mistral Medium: Cost-effective alternative</li> <li>Local models: Privacy-focused, no API costs</li> </ul>"},{"location":"getting-started/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/configuration/#configuration-validation","title":"Configuration Validation","text":"<pre><code>from docling_graph import PipelineConfig\n\ntry:\n    config = PipelineConfig(\n        source=\"document.pdf\",\n        template=YourTemplate,\n        output_dir=\"outputs\"\n    )\n    print(\"Configuration valid!\")\nexcept ValueError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"getting-started/configuration/#debug-mode","title":"Debug Mode","text":"<pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\nconfig = PipelineConfig(...)\nrun_pipeline(config)\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Get started with examples</li> <li>Pydantic Templates - Create templates</li> <li>API Reference - Complete configuration API</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>pip or uv package manager</li> </ul>"},{"location":"getting-started/installation/#installation-options","title":"Installation Options","text":"<p>Choose the installation option that matches your use case:</p>"},{"location":"getting-started/installation/#minimal-installation","title":"Minimal Installation","text":"<p>Includes core VLM features (Docling), no LLM inference:</p> <pre><code>pip install docling-graph\n</code></pre> <p>Or with uv:</p> <pre><code>uv pip install docling-graph\n</code></pre>"},{"location":"getting-started/installation/#full-installation","title":"Full Installation","text":"<p>Includes all features, VLM, and all local/remote LLM providers:</p> <pre><code>pip install docling-graph[all]\n</code></pre>"},{"location":"getting-started/installation/#specific-features","title":"Specific Features","text":"<p>Install only the features you need:</p>"},{"location":"getting-started/installation/#local-llm-support","title":"Local LLM Support","text":"<p>Adds support for vLLM and Ollama (requires GPU for vLLM):</p> <pre><code>pip install docling-graph[local]\n</code></pre>"},{"location":"getting-started/installation/#remote-api-support","title":"Remote API Support","text":"<p>Adds support for Mistral, OpenAI, Gemini, and IBM WatsonX APIs:</p> <pre><code>pip install docling-graph[remote]\n</code></pre>"},{"location":"getting-started/installation/#individual-providers","title":"Individual Providers","text":"<p>Install specific LLM providers:</p> <pre><code># OpenAI\npip install docling-graph[openai]\n\n# Mistral\npip install docling-graph[mistral]\n\n# Google Gemini\npip install docling-graph[gemini]\n\n# IBM WatsonX\npip install docling-graph[watsonx]\n\n# Ollama (local)\npip install docling-graph[ollama]\n\n# vLLM (local)\npip install docling-graph[vllm]\n</code></pre>"},{"location":"getting-started/installation/#gpu-support-optional","title":"GPU Support (Optional)","text":"<p>For local inference with GPU acceleration, follow the GPU Setup Guide.</p>"},{"location":"getting-started/installation/#api-key-setup","title":"API Key Setup","text":"<p>If you're using remote/cloud inference, set your API keys:</p>"},{"location":"getting-started/installation/#linuxmacos","title":"Linux/macOS","text":"<pre><code>export OPENAI_API_KEY=\"your-key-here\"\nexport MISTRAL_API_KEY=\"your-key-here\"\nexport GEMINI_API_KEY=\"your-key-here\"\nexport WATSONX_API_KEY=\"your-key-here\"\nexport WATSONX_PROJECT_ID=\"your-project-id\"\nexport WATSONX_URL=\"https://us-south.ml.cloud.ibm.com\"  # Optional\n</code></pre>"},{"location":"getting-started/installation/#windows-command-prompt","title":"Windows (Command Prompt)","text":"<pre><code>set OPENAI_API_KEY=your-key-here\nset MISTRAL_API_KEY=your-key-here\nset GEMINI_API_KEY=your-key-here\nset WATSONX_API_KEY=your-key-here\nset WATSONX_PROJECT_ID=your-project-id\n</code></pre>"},{"location":"getting-started/installation/#windows-powershell","title":"Windows (PowerShell)","text":"<pre><code>$env:OPENAI_API_KEY=\"your-key-here\"\n$env:MISTRAL_API_KEY=\"your-key-here\"\n$env:GEMINI_API_KEY=\"your-key-here\"\n$env:WATSONX_API_KEY=\"your-key-here\"\n$env:WATSONX_PROJECT_ID=\"your-project-id\"\n</code></pre>"},{"location":"getting-started/installation/#using-env-file","title":"Using .env File","text":"<p>Alternatively, create a <code>.env</code> file in your project root:</p> <pre><code>OPENAI_API_KEY=your-key-here\nMISTRAL_API_KEY=your-key-here\nGEMINI_API_KEY=your-key-here\nWATSONX_API_KEY=your-key-here\nWATSONX_PROJECT_ID=your-project-id\nWATSONX_URL=https://us-south.ml.cloud.ibm.com\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Check version\npython -c \"import docling_graph; print(docling_graph.__version__)\"\n\n# Test CLI\ndocling-graph --help\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing to the project:</p> <pre><code># Clone repository\ngit clone https://github.com/IBM/docling-graph.git\ncd docling-graph\n\n# Install with development dependencies\nuv sync --all-extras --dev\n\n# Install pre-commit hooks\nuv run pre-commit install\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If you encounter import errors, ensure you've installed the required extras:</p> <pre><code>pip install docling-graph[all]\n</code></pre>"},{"location":"getting-started/installation/#gpu-issues","title":"GPU Issues","text":"<p>For GPU-related issues, see the GPU Setup Guide.</p>"},{"location":"getting-started/installation/#api-connection-issues","title":"API Connection Issues","text":"<p>Verify your API keys are set correctly:</p> <pre><code>import os\nprint(os.getenv('OPENAI_API_KEY'))  # Should print your key\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started with your first conversion</li> <li>Configuration - Learn about configuration options</li> <li>Examples - Explore example use cases</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with Docling Graph in minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher installed</li> <li>Docling Graph installed (see Installation)</li> <li>API keys configured (if using remote LLM providers)</li> </ul>"},{"location":"getting-started/quickstart/#your-first-conversion","title":"Your First Conversion","text":""},{"location":"getting-started/quickstart/#1-using-the-cli","title":"1. Using the CLI","text":"<p>The easiest way to get started is with the CLI:</p> <pre><code># Initialize configuration (interactive wizard)\ndocling-graph init\n\n# Convert a document\ndocling-graph convert document.pdf \\\n    --template \"your_module.YourTemplate\" \\\n    --output-dir outputs\n</code></pre>"},{"location":"getting-started/quickstart/#2-using-python","title":"2. Using Python","text":"<p>For programmatic control:</p> <pre><code>from docling_graph import run_pipeline, PipelineConfig\nfrom pydantic import BaseModel, Field\n\n# Define your extraction template\nclass Person(BaseModel):\n    \"\"\"Person entity.\"\"\"\n    model_config = {\n        'is_entity': True,\n        'graph_id_fields': ['name']\n    }\n\n    name: str = Field(description=\"Person's full name\")\n    age: int = Field(description=\"Person's age\")\n    occupation: str = Field(description=\"Person's occupation\")\n\n# Configure pipeline\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=Person,\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"openai\",\n    model_override=\"gpt-4\",\n    output_dir=\"outputs\"\n)\n\n# Run conversion\nrun_pipeline(config)\n</code></pre>"},{"location":"getting-started/quickstart/#example-extract-research-paper-information","title":"Example: Extract Research Paper Information","text":"<p>Let's extract information from a research paper:</p> <pre><code>from docling_graph import run_pipeline, PipelineConfig\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclass Author(BaseModel):\n    \"\"\"Research paper author.\"\"\"\n    model_config = {\n        'is_entity': True,\n        'graph_id_fields': ['name']\n    }\n\n    name: str = Field(description=\"Author's full name\")\n    affiliation: str = Field(description=\"Author's institution\")\n\nclass ResearchPaper(BaseModel):\n    \"\"\"Research paper information.\"\"\"\n\n    title: str = Field(description=\"Paper title\")\n    abstract: str = Field(description=\"Paper abstract\")\n    authors: List[Author] = Field(description=\"List of authors\")\n    keywords: List[str] = Field(description=\"Paper keywords\")\n    year: int = Field(description=\"Publication year\")\n\n# Configure and run\nconfig = PipelineConfig(\n    source=\"research_paper.pdf\",\n    template=ResearchPaper,\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"mistral\",\n    model_override=\"mistral-medium-latest\",\n    use_chunking=True,\n    processing_mode=\"many-to-one\",\n    output_dir=\"outputs/research\"\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"getting-started/quickstart/#understanding-the-output","title":"Understanding the Output","text":"<p>After running the pipeline, you'll find several files in your output directory:</p> <pre><code>outputs/\n\u251c\u2500\u2500 graph.json              # Graph data in JSON format\n\u251c\u2500\u2500 nodes.csv              # Nodes for Neo4j import\n\u251c\u2500\u2500 edges.csv              # Edges for Neo4j import\n\u251c\u2500\u2500 cypher_script.cypher   # Cypher script for bulk import\n\u251c\u2500\u2500 visualization.html     # Interactive graph visualization\n\u2514\u2500\u2500 report.md             # Detailed markdown report\n</code></pre>"},{"location":"getting-started/quickstart/#viewing-the-graph","title":"Viewing the Graph","text":"<p>Open <code>visualization.html</code> in your browser to explore the interactive graph:</p> <ul> <li>Click nodes to see details</li> <li>Hover over edges to see relationships</li> <li>Use the search to find specific entities</li> <li>Zoom and pan to navigate</li> </ul>"},{"location":"getting-started/quickstart/#configuration-options","title":"Configuration Options","text":""},{"location":"getting-started/quickstart/#backend-options","title":"Backend Options","text":"<pre><code># Local VLM (Docling)\nbackend=\"vlm\"\n\n# LLM-based extraction\nbackend=\"llm\"\n</code></pre>"},{"location":"getting-started/quickstart/#inference-options","title":"Inference Options","text":"<pre><code># Remote API\ninference=\"remote\"\nprovider_override=\"openai\"  # or \"mistral\", \"gemini\", \"watsonx\"\n\n# Local inference\ninference=\"local\"\nprovider_override=\"ollama\"  # or \"vllm\"\n</code></pre>"},{"location":"getting-started/quickstart/#processing-modes","title":"Processing Modes","text":"<pre><code># Process each page separately\nprocessing_mode=\"one-to-one\"\n\n# Combine all pages into one extraction\nprocessing_mode=\"many-to-one\"\n</code></pre>"},{"location":"getting-started/quickstart/#chunking-options","title":"Chunking Options","text":"<pre><code># Enable hybrid chunking\nuse_chunking=True\n\n# Disable chunking (process full document)\nuse_chunking=False\n\n# LLM-based consolidation\nllm_consolidation=True\n</code></pre>"},{"location":"getting-started/quickstart/#cli-examples","title":"CLI Examples","text":""},{"location":"getting-started/quickstart/#basic-conversion","title":"Basic Conversion","text":"<pre><code>docling-graph convert document.pdf \\\n    --template \"templates.Invoice\" \\\n    --output-dir outputs\n</code></pre>"},{"location":"getting-started/quickstart/#with-custom-configuration","title":"With Custom Configuration","text":"<pre><code>docling-graph convert document.pdf \\\n    --template \"templates.Research\" \\\n    --backend llm \\\n    --inference remote \\\n    --provider openai \\\n    --model gpt-4 \\\n    --processing-mode many-to-one \\\n    --use-chunking \\\n    --output-dir outputs\n</code></pre>"},{"location":"getting-started/quickstart/#inspect-results","title":"Inspect Results","text":"<pre><code>docling-graph inspect outputs\n</code></pre>"},{"location":"getting-started/quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quickstart/#pattern-1-invoice-processing","title":"Pattern 1: Invoice Processing","text":"<pre><code>from docling_graph import run_pipeline, PipelineConfig\nfrom templates.invoice import Invoice\n\nconfig = PipelineConfig(\n    source=\"invoice.pdf\",\n    template=Invoice,\n    backend=\"vlm\",  # VLM works well for structured documents\n    output_dir=\"outputs/invoices\"\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"getting-started/quickstart/#pattern-2-multi-page-document","title":"Pattern 2: Multi-Page Document","text":"<pre><code>config = PipelineConfig(\n    source=\"report.pdf\",\n    template=Report,\n    backend=\"llm\",\n    processing_mode=\"many-to-one\",  # Combine all pages\n    use_chunking=True,              # Use smart chunking\n    llm_consolidation=False,        # Merge programmatically\n    output_dir=\"outputs/reports\"\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"getting-started/quickstart/#pattern-3-batch-processing","title":"Pattern 3: Batch Processing","text":"<pre><code>from pathlib import Path\n\ndocuments = Path(\"documents\").glob(\"*.pdf\")\n\nfor doc in documents:\n    config = PipelineConfig(\n        source=str(doc),\n        template=YourTemplate,\n        backend=\"llm\",\n        output_dir=f\"outputs/{doc.stem}\"\n    )\n    run_pipeline(config)\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Detailed configuration options</li> <li>Pydantic Templates - Create custom templates</li> <li>Examples - More example use cases</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"getting-started/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quickstart/#common-issues","title":"Common Issues","text":"<p>Issue: Import errors <pre><code># Solution: Install required extras\npip install docling-graph[all]\n</code></pre></p> <p>Issue: API authentication errors <pre><code># Solution: Set API keys\nexport OPENAI_API_KEY=\"your-key\"\n</code></pre></p> <p>Issue: Out of memory errors <pre><code># Solution: Enable chunking\nuse_chunking=True\n</code></pre></p> <p>For more help, see our GitHub Issues.</p>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/","title":"Complete Guide to Creating Pydantic Templates for Knowledge Graph Extraction","text":"<p>This comprehensive guide provides domain-agnostic best practices for creating Pydantic templates optimized for LLM-based document extraction and automatic conversion to knowledge graphs. The patterns and practices described here are derived from production templates across multiple domains and ensure consistency in structure, validation, and graph representation.</p>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core Concepts</li> <li>Required Imports and Helper Functions</li> <li>Template Structure and Organization</li> <li>Entity vs Component Classification</li> <li>Field Definition Patterns</li> <li>Validation and Normalization</li> <li>Edge Definitions and Relationships</li> <li>Advanced Patterns</li> <li>String Representations</li> <li>Common Reusable Components</li> </ol>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#1-core-concepts","title":"1. Core Concepts","text":""},{"location":"guides/create_pydantic_templates_for_kg_extraction/#why-pydantic-for-knowledge-graph-extraction","title":"Why Pydantic for Knowledge Graph Extraction?","text":"<p>Pydantic models serve three critical purposes in the extraction pipeline:</p> <ol> <li>LLM Guidance: Field descriptions and examples guide the language model to extract accurate, structured data</li> <li>Data Validation: Field validators ensure data quality and consistency</li> <li>Graph Structure: Models define nodes, edges, and relationships for the knowledge graph</li> </ol>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#key-terminology","title":"Key Terminology","text":"Term Definition Graph Behavior Entity A unique, identifiable object tracked individually Identified by <code>graph_id_fields</code> Component A value object deduplicated by content Set <code>is_entity=False</code> Node Any Pydantic model that becomes a graph node All BaseModel subclasses Edge A relationship between nodes Defined via <code>edge()</code> helper graph_id_fields Fields used to create stable, unique node IDs Required for entities"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#2-required-imports-and-helper-functions","title":"2. Required Imports and Helper Functions","text":""},{"location":"guides/create_pydantic_templates_for_kg_extraction/#standard-import-block","title":"Standard Import Block","text":"<p>Every template must include this import structure:</p> <pre><code>\"\"\"\nBrief description of what this template extracts.\nMention the document type and key domain features.\n\"\"\"\n\nfrom typing import Any, List, Optional, Union, Self, Type\nfrom pydantic import BaseModel, ConfigDict, Field, field_validator, model_validator\nfrom datetime import date, datetime  # Include based on domain needs\nfrom enum import Enum  # Include if using enums\nimport re  # Include if using regex validators\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#edge-helper-function-required","title":"Edge Helper Function (Required)","text":"<p>This function must be defined identically in every template:</p> <pre><code>def edge(label: str, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Helper function to create a Pydantic Field with edge metadata.\n    The 'edge_label' defines the type of relationship in the knowledge graph.\n    \"\"\"\n    return Field(..., json_schema_extra={\"edge_label\": label}, **kwargs)\n</code></pre> <p>Critical Rules: - Function name must be lowercase <code>edge</code> (not <code>Edge</code>) - Returns <code>Field(...)</code> with <code>json_schema_extra={\"edge_label\": label}</code> - Always accepts <code>**kwargs</code> to pass through additional Field parameters</p>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#3-template-structure-and-organization","title":"3. Template Structure and Organization","text":""},{"location":"guides/create_pydantic_templates_for_kg_extraction/#standard-file-organization","title":"Standard File Organization","text":"<p>Organize your template in this exact order:</p> <pre><code>\"\"\"\nTemplate docstring describing purpose and domain\n\"\"\"\n\n# --- Required Imports ---\nfrom pydantic import BaseModel, ConfigDict, Field, field_validator\nfrom typing import Any, List, Optional\n# ... additional imports\n\n# --- Edge Helper Function ---\ndef edge(label: str, **kwargs: Any) -&gt; Any:\n    return Field(..., json_schema_extra={\"edge_label\": label}, **kwargs)\n\n# --- Helper Functions (if needed) ---\n# Normalization, parsing, or utility functions\n\n# --- Reusable Components ---\n# Value objects with is_entity=False\n\n# --- Reusable Entities ---\n# Common entities like Person, Organization, Address\n\n# --- Domain-Specific Models ---\n# Models unique to this document type\n\n# --- Root Document Model ---\n# The main entry point (last in file)\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#docstring-standards","title":"Docstring Standards","text":"<p>Each model should have a clear docstring:</p> <pre><code>class MyModel(BaseModel):\n    \"\"\"\n    Brief description of what this model represents.\n    Include uniqueness criteria if it's an entity.\n    \"\"\"\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#4-entity-vs-component-classification","title":"4. Entity vs Component Classification","text":""},{"location":"guides/create_pydantic_templates_for_kg_extraction/#the-critical-distinction","title":"The Critical Distinction","text":"Aspect Entity Component Purpose Unique, identifiable objects Value objects, content-based deduplication Configuration <code>graph_id_fields=[...]</code> <code>is_entity=False</code> Deduplication By specified ID fields By all field values When to Use Track individually (people, documents, organizations) Shared values (addresses, amounts, measurements)"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#entity-pattern","title":"Entity Pattern","text":"<pre><code>class Person(BaseModel):\n    \"\"\"\n    A person entity.\n    Uniquely identified by first name, last name, and date of birth.\n    \"\"\"\n    model_config = ConfigDict(graph_id_fields=[\"first_name\", \"last_name\", \"date_of_birth\"])\n\n    first_name: Optional[str] = Field(...)\n    last_name: Optional[str] = Field(...)\n    date_of_birth: Optional[date] = Field(...)\n    # Additional fields...\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#component-pattern","title":"Component Pattern","text":"<pre><code>class Address(BaseModel):\n    \"\"\"\n    A physical address component.\n    Deduplicated by content - identical addresses share the same node.\n    \"\"\"\n    model_config = ConfigDict(is_entity=False)\n\n    street_address: Optional[str] = Field(...)\n    city: Optional[str] = Field(...)\n    postal_code: Optional[str] = Field(...)\n    country: Optional[str] = Field(...)\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#choosing-graph_id_fields","title":"Choosing graph_id_fields","text":"<p>Select fields that: - Together form a natural unique identifier - Are stable (don't change frequently) - Are likely to be present in the extracted data</p> <p>Examples: - <code>[\"document_number\"]</code> - for documents with unique IDs - <code>[\"name\"]</code> - for organizations (if names are unique) - <code>[\"first_name\", \"last_name\", \"date_of_birth\"]</code> - for people - <code>[\"name\", \"text_value\", \"numeric_value\", \"unit\"]</code> - for measurements</p>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#5-field-definition-patterns","title":"5. Field Definition Patterns","text":""},{"location":"guides/create_pydantic_templates_for_kg_extraction/#field-anatomy","title":"Field Anatomy","text":"<pre><code>field_name: FieldType = Field(\n    default_value,  # ... for required, None for optional, or a default\n    description=\"Detailed, LLM-friendly description with extraction hints\",\n    examples=[\"Example 1\", \"Example 2\", \"Example 3\"]  # 2-5 realistic examples\n)\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#required-vs-optional-fields","title":"Required vs Optional Fields","text":"<pre><code># Required field - LLM must extract this\ndocument_id: str = Field(\n    ...,  # Ellipsis = required\n    description=\"Unique document identifier\",\n    examples=[\"DOC-2024-001\", \"INV-123456\"]\n)\n\n# Optional field with None default\nphone: Optional[str] = Field(\n    None,\n    description=\"Contact phone number\",\n    examples=[\"+33 1 23 45 67 89\", \"06 12 34 56 78\"]\n)\n\n# Optional field with custom default\nstatus: str = Field(\n    \"pending\",\n    description=\"Current processing status\",\n    examples=[\"pending\", \"approved\", \"rejected\"]\n)\n\n# Optional list (always use default_factory)\nitems: List[Item] = Field(\n    default_factory=list,\n    description=\"List of items\",\n    examples=[[{\"name\": \"Item1\"}, {\"name\": \"Item2\"}]]\n)\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#description-best-practices","title":"Description Best Practices","text":"<p>Good descriptions are: - Clear and specific about what to extract - Include extraction hints (field names, patterns, synonyms) - Provide parsing or normalization instructions - Guide the LLM on ambiguous cases</p> <pre><code># EXCELLENT - Comprehensive guidance\ndate_of_birth: Optional[date] = Field(\n    None,\n    description=(\n        \"The person's date of birth. \"\n        \"Look for text like 'Date of birth', 'Date de naiss.', or 'Born on'. \"\n        \"Parse formats like 'DD MM YYYY' or 'DDMMYYYY' and normalize to YYYY-MM-DD.\"\n    ),\n    examples=[\"1990-05-15\", \"1985-12-20\", \"1978-03-30\"]\n)\n\n# POOR - Too vague\ndate_of_birth: Optional[date] = Field(None, description=\"Birth date\")\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#examples-best-practices","title":"Examples Best Practices","text":"<p>Provide 2-5 diverse, realistic examples per field:</p> <pre><code># For simple fields\nemail: Optional[str] = Field(\n    None,\n    description=\"Contact email address\",\n    examples=[\n        \"jean.dupont@email.com\",\n        \"contact@company.fr\",\n        \"info@organization.org\"\n    ]\n)\n\n# For lists - show the list structure\nguarantees: List[str] = Field(\n    default_factory=list,\n    description=\"List of coverage items\",\n    examples=[\n        [\"Fire protection\", \"Water damage\", \"Theft\"],\n        [\"Basic coverage\", \"Extended warranty\"],\n        [\"Liability\", \"Property damage\"]\n    ]\n)\n\n# For complex nested objects\ncomponents: List[Component] = Field(\n    default_factory=list,\n    description=\"List of components with roles and amounts\",\n    examples=[\n        [\n            {\n                \"material\": {\"name\": \"Steel\", \"grade\": \"304\"},\n                \"role\": \"Primary\",\n                \"amount\": {\"value\": 12.0, \"unit\": \"kg\"}\n            }\n        ]\n    ]\n)\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#6-validation-and-normalization","title":"6. Validation and Normalization","text":""},{"location":"guides/create_pydantic_templates_for_kg_extraction/#field-validators","title":"Field Validators","text":"<p>Use <code>@field_validator</code> for data quality checks and transformations:</p> <pre><code>class MonetaryAmount(BaseModel):\n    \"\"\"Monetary value with validation.\"\"\"\n    model_config = ConfigDict(is_entity=False)\n\n    value: float = Field(...)\n    currency: Optional[str] = Field(None)\n\n    @field_validator(\"value\")\n    @classmethod\n    def validate_positive(cls, v: Any) -&gt; Any:\n        \"\"\"Ensure value is non-negative.\"\"\"\n        if v &lt; 0:\n            raise ValueError(\"Monetary amount must be non-negative\")\n        return v\n\n    @field_validator(\"currency\")\n    @classmethod\n    def validate_currency_format(cls, v: Any) -&gt; Any:\n        \"\"\"Ensure currency is 3 uppercase letters (ISO 4217).\"\"\"\n        if v and not (len(v) == 3 and v.isupper()):\n            raise ValueError(\"Currency must be 3 uppercase letters (ISO 4217)\")\n        return v\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#pre-validators-modebefore","title":"Pre-validators (mode='before')","text":"<p>Use <code>mode='before'</code> to transform input before type coercion:</p> <pre><code>@field_validator(\"email\", mode=\"before\")\n@classmethod\ndef normalize_email(cls, v: Any) -&gt; Any:\n    \"\"\"Convert email to lowercase and strip whitespace.\"\"\"\n    if v:\n        return v.lower().strip()\n    return v\n\n@field_validator(\"given_names\", mode=\"before\")\n@classmethod\ndef ensure_list(cls, v: Any) -&gt; Any:\n    \"\"\"Ensure given_names is always a list.\"\"\"\n    if isinstance(v, str):\n        # Handle comma-separated names\n        if \",\" in v:\n            return [name.strip() for name in v.split(\",\")]\n        return [v]\n    return v\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#model-validators","title":"Model Validators","text":"<p>Use <code>@model_validator</code> for cross-field validation:</p> <pre><code>class Measurement(BaseModel):\n    \"\"\"Flexible measurement model with single value or range support.\"\"\"\n    model_config = ConfigDict(is_entity=False)\n\n    name: str = Field(...)\n    numeric_value: Optional[float] = Field(None)\n    numeric_value_min: Optional[float] = Field(None)\n    numeric_value_max: Optional[float] = Field(None)\n    unit: Optional[str] = Field(None)\n\n    @model_validator(mode=\"after\")\n    def validate_value_consistency(self) -&gt; Self:\n        \"\"\"Ensure value fields are used consistently.\"\"\"\n        has_single = self.numeric_value is not None\n        has_min = self.numeric_value_min is not None\n        has_max = self.numeric_value_max is not None\n\n        # Reject ambiguous cases\n        if has_single and has_min and has_max:\n            raise ValueError(\n                \"Cannot specify numeric_value, numeric_value_min, \"\n                \"and numeric_value_max simultaneously\"\n            )\n\n        return self\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#enum-normalization-helper","title":"Enum Normalization Helper","text":"<p>For flexible enum handling, use this helper pattern:</p> <pre><code>from enum import Enum\nfrom typing import Type\n\ndef _normalize_enum(enum_cls: Type[Enum], v: Any) -&gt; Any:\n    \"\"\"\n    Accept enum instances, value strings, or member names.\n    Handles various formats: 'VALUE', 'value', 'Value', 'VALUE_NAME'.\n    Falls back to 'OTHER' member if present.\n    \"\"\"\n    if isinstance(v, enum_cls):\n        return v\n\n    if isinstance(v, str):\n        # Normalize to alphanumeric lowercase\n        key = re.sub(r\"[^A-Za-z0-9]+\", \"\", v).lower()\n\n        # Build mapping of normalized names/values to enum members\n        mapping = {}\n        for member in enum_cls:\n            normalized_name = re.sub(r\"[^A-Za-z0-9]+\", \"\", member.name).lower()\n            normalized_value = re.sub(r\"[^A-Za-z0-9]+\", \"\", member.value).lower()\n            mapping[normalized_name] = member\n            mapping[normalized_value] = member\n\n        if key in mapping:\n            return mapping[key]\n\n        # Last attempt: direct value match\n        try:\n            return enum_cls(v)\n        except Exception:\n            # Safe fallback to OTHER if present\n            if \"OTHER\" in enum_cls.__members__:\n                return enum_cls.OTHER\n            raise\n\n    raise ValueError(f\"Cannot normalize {v} to {enum_cls}\")\n\n# Usage in validator\nclass MyModel(BaseModel):\n    status: MyStatusEnum = Field(...)\n\n    @field_validator(\"status\", mode=\"before\")\n    @classmethod\n    def normalize_status(cls, v: Any) -&gt; Any:\n        return _normalize_enum(MyStatusEnum, v)\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#7-edge-definitions-and-relationships","title":"7. Edge Definitions and Relationships","text":""},{"location":"guides/create_pydantic_templates_for_kg_extraction/#edge-usage-patterns","title":"Edge() Usage Patterns","text":"<pre><code># Required single relationship\nissued_by: Organization = edge(\n    label=\"ISSUED_BY\",\n    description=\"The organization that issued this document\"\n)\n\n# Optional single relationship\nverified_by: Optional[Person] = edge(\n    label=\"VERIFIED_BY\",\n    description=\"Person who verified this document, if applicable\"\n)\n\n# Required list relationship (one-to-many)\ncontains_items: List[LineItem] = edge(\n    label=\"CONTAINS_ITEM\",\n    default_factory=list,  # REQUIRED for lists\n    description=\"Line items contained in this document\"\n)\n\n# Optional list relationship\naddresses: List[Address] = edge(\n    label=\"LOCATED_AT\",\n    default_factory=list,\n    description=\"Physical addresses for this entity\"\n)\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#edge-label-conventions","title":"Edge Label Conventions","text":"<p>Consistent naming standards: - Use ALL_CAPS with underscores - Use verb phrases that describe the relationship - Choose descriptive, domain-appropriate verbs</p> <p>Common edge labels: - <code>ISSUED_BY</code>, <code>CREATED_BY</code>, <code>OWNED_BY</code> - authorship/ownership - <code>SENT_TO</code>, <code>ADDRESSED_TO</code>, <code>DELIVERED_TO</code> - recipients - <code>LOCATED_AT</code>, <code>LIVES_AT</code>, <code>BASED_AT</code> - physical location - <code>CONTAINS_ITEM</code>, <code>HAS_COMPONENT</code>, <code>INCLUDES_PART</code> - composition - <code>BELONGS_TO</code>, <code>PART_OF</code>, <code>MEMBER_OF</code> - membership - <code>HAS_GUARANTEE</code>, <code>OFFERS_PLAN</code>, <code>PROVIDES_COVERAGE</code> - offerings - <code>HAS_PROCESS_STEP</code>, <code>HAS_EVALUATION</code>, <code>HAS_MEASUREMENT</code> - processes</p> <p>Bad examples (avoid): - <code>issuedBy</code>, <code>located-at</code>, <code>has guarantee</code> - inconsistent formatting - <code>relationship</code>, <code>link</code>, <code>connection</code> - too vague</p>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#8-advanced-patterns","title":"8. Advanced Patterns","text":""},{"location":"guides/create_pydantic_templates_for_kg_extraction/#pattern-flexible-measurement-with-range-support","title":"Pattern: Flexible Measurement with Range Support","text":"<pre><code>class Measurement(BaseModel):\n    \"\"\"\n    Flexible measurement supporting single values or ranges.\n    Can represent '25\u00b0C', '1.6 mPa.s', or '80-90\u00b0C'.\n    \"\"\"\n    model_config = ConfigDict(is_entity=False)\n\n    name: str = Field(\n        description=\"Name of the measured property\",\n        examples=[\"Temperature\", \"Viscosity\", \"pH\", \"Concentration\"]\n    )\n\n    text_value: Optional[str] = Field(\n        default=None,\n        description=\"Textual value if not numerical\",\n        examples=[\"High\", \"Low\", \"Stable\", \"Increasing\"]\n    )\n\n    numeric_value: Optional[Union[float, int]] = Field(\n        default=None,\n        description=\"Single numerical value\",\n        examples=[25.0, 1.6, 8.2]\n    )\n\n    numeric_value_min: Optional[Union[float, int]] = Field(\n        default=None,\n        description=\"Minimum value for range measurements\",\n        examples=[80.0, 1.5]\n    )\n\n    numeric_value_max: Optional[Union[float, int]] = Field(\n        default=None,\n        description=\"Maximum value for range measurements\",\n        examples=[90.0, 2.0]\n    )\n\n    unit: Optional[str] = Field(\n        default=None,\n        description=\"Unit of measurement\",\n        examples=[\"\u00b0C\", \"mPa.s\", \"wt%\", \"kg\"]\n    )\n\n    condition: Optional[str] = Field(\n        default=None,\n        description=\"Measurement conditions or context\",\n        examples=[\"at 25\u00b0C\", \"after 24h\", \"under normal pressure\"]\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_value_consistency(self) -&gt; Self:\n        \"\"\"Ensure value fields don't conflict.\"\"\"\n        has_single = self.numeric_value is not None\n        has_min = self.numeric_value_min is not None\n        has_max = self.numeric_value_max is not None\n\n        if has_single and has_min and has_max:\n            raise ValueError(\n                \"Cannot specify all three: numeric_value, \"\n                \"numeric_value_min, and numeric_value_max\"\n            )\n\n        return self\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#pattern-nested-list-with-edges","title":"Pattern: Nested List with Edges","text":"<pre><code>class Component(BaseModel):\n    \"\"\"A component with material, role, and amount.\"\"\"\n    model_config = ConfigDict(graph_id_fields=[\"material\", \"role\"])\n\n    material: Material = edge(\n        label=\"USES_MATERIAL\",\n        description=\"The material used in this component\"\n    )\n\n    role: RoleEnum = Field(\n        description=\"Function of this component\",\n        examples=[\"Primary\", \"Secondary\", \"Additive\"]\n    )\n\n    amount: Optional[Measurement] = Field(\n        None,\n        description=\"Amount specification\"\n    )\n\nclass Assembly(BaseModel):\n    \"\"\"Root assembly containing components.\"\"\"\n    model_config = ConfigDict(graph_id_fields=[\"assembly_id\"])\n\n    assembly_id: str = Field(...)\n\n    components: List[Component] = edge(\n        label=\"HAS_COMPONENT\",\n        default_factory=list,\n        description=\"List of components in this assembly\"\n    )\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#pattern-multiple-address-support","title":"Pattern: Multiple Address Support","text":"<pre><code>class Entity(BaseModel):\n    \"\"\"Entity that may have multiple addresses.\"\"\"\n    model_config = ConfigDict(graph_id_fields=[\"name\"])\n\n    name: str = Field(...)\n\n    # Support multiple addresses\n    addresses: List[Address] = edge(\n        label=\"LOCATED_AT\",\n        default_factory=list,\n        description=\"Physical addresses (headquarters, branches, etc.)\"\n    )\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#pattern-optional-edges","title":"Pattern: Optional Edges","text":"<pre><code>class Document(BaseModel):\n    \"\"\"Document that may or may not have a verifier.\"\"\"\n    model_config = ConfigDict(graph_id_fields=[\"document_id\"])\n\n    document_id: str = Field(...)\n\n    # Optional single edge\n    verified_by: Optional[Person] = edge(\n        label=\"VERIFIED_BY\",\n        description=\"Person who verified this document, if verified\"\n    )\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#pattern-conditional-fields-with-validators","title":"Pattern: Conditional Fields with Validators","text":"<pre><code>class Document(BaseModel):\n    \"\"\"Document with type-specific fields.\"\"\"\n\n    document_type: str = Field(\n        description=\"Type of document\",\n        examples=[\"Invoice\", \"Receipt\", \"Credit Note\"]\n    )\n\n    # Field only relevant for invoices\n    payment_terms: Optional[str] = Field(\n        None,\n        description=\"Payment terms (primarily for invoices)\",\n        examples=[\"Net 30\", \"Due on receipt\", \"Net 60\"]\n    )\n\n    # Field only relevant for credit notes\n    original_document_ref: Optional[str] = Field(\n        None,\n        description=\"Reference to original document (for credit notes)\",\n        examples=[\"INV-2024-001\", \"DOC-123456\"]\n    )\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#9-string-representations","title":"9. String Representations","text":""},{"location":"guides/create_pydantic_templates_for_kg_extraction/#purpose-and-placement","title":"Purpose and Placement","text":"<p>Add <code>__str__</code> methods to all entities and key components for: - Debugging and logging - Human-readable representation in error messages - Graph visualization labels</p>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#implementation-patterns","title":"Implementation Patterns","text":"<pre><code># Simple concatenation\nclass Person(BaseModel):\n    first_name: Optional[str] = Field(...)\n    last_name: Optional[str] = Field(...)\n\n    def __str__(self) -&gt; str:\n        parts = [self.first_name, self.last_name]\n        return \" \".join(p for p in parts if p) or \"Unknown\"\n\n# With list handling\nclass Person(BaseModel):\n    given_names: Optional[List[str]] = Field(...)\n    last_name: Optional[str] = Field(...)\n\n    def __str__(self) -&gt; str:\n        first_names = \" \".join(self.given_names) if self.given_names else \"\"\n        parts = [first_names, self.last_name]\n        return \" \".join(p for p in parts if p) or \"Unknown\"\n\n# Address formatting\nclass Address(BaseModel):\n    street_address: Optional[str] = Field(...)\n    city: Optional[str] = Field(...)\n    postal_code: Optional[str] = Field(...)\n    country: Optional[str] = Field(...)\n\n    def __str__(self) -&gt; str:\n        parts = [\n            self.street_address,\n            self.city,\n            self.postal_code,\n            self.country\n        ]\n        return \", \".join(p for p in parts if p)\n\n# Value with unit\nclass MonetaryAmount(BaseModel):\n    value: float = Field(...)\n    currency: Optional[str] = Field(None)\n\n    def __str__(self) -&gt; str:\n        return f\"{self.value} {self.currency or ''}\".strip()\n\n# With identifier\nclass Document(BaseModel):\n    document_type: str = Field(...)\n    document_number: str = Field(...)\n\n    def __str__(self) -&gt; str:\n        return f\"{self.document_type} {self.document_number}\"\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#10-common-reusable-components","title":"10. Common Reusable Components","text":""},{"location":"guides/create_pydantic_templates_for_kg_extraction/#address-component","title":"Address Component","text":"<pre><code>class Address(BaseModel):\n    \"\"\"Physical address component (deduplicated by content).\"\"\"\n    model_config = ConfigDict(is_entity=False)\n\n    street_address: Optional[str] = Field(\n        None,\n        description=\"Street name and number\",\n        examples=[\"123 Main Street\", \"45 Avenue des Champs-\u00c9lys\u00e9es\"]\n    )\n\n    city: Optional[str] = Field(\n        None,\n        description=\"City name\",\n        examples=[\"Paris\", \"London\", \"New York\"]\n    )\n\n    state_or_province: Optional[str] = Field(\n        None,\n        description=\"State, province, or region\",\n        examples=[\"\u00cele-de-France\", \"California\", \"Ontario\"]\n    )\n\n    postal_code: Optional[str] = Field(\n        None,\n        description=\"Postal or ZIP code\",\n        examples=[\"75001\", \"SW1A 1AA\", \"10001\"]\n    )\n\n    country: Optional[str] = Field(\n        None,\n        description=\"Country name or code\",\n        examples=[\"France\", \"FR\", \"United Kingdom\"]\n    )\n\n    def __str__(self) -&gt; str:\n        parts = [\n            self.street_address,\n            self.city,\n            self.state_or_province,\n            self.postal_code,\n            self.country\n        ]\n        return \", \".join(p for p in parts if p)\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#monetary-amount-component","title":"Monetary Amount Component","text":"<pre><code>class MonetaryAmount(BaseModel):\n    \"\"\"Monetary value with currency (deduplicated by content).\"\"\"\n    model_config = ConfigDict(is_entity=False)\n\n    value: float = Field(\n        ...,\n        description=\"Numeric amount\",\n        examples=[500.00, 1250.50, 89.99]\n    )\n\n    currency: Optional[str] = Field(\n        None,\n        description=\"ISO 4217 currency code\",\n        examples=[\"EUR\", \"USD\", \"GBP\", \"CHF\"]\n    )\n\n    @field_validator(\"value\")\n    @classmethod\n    def validate_positive(cls, v: Any) -&gt; Any:\n        \"\"\"Ensure amount is non-negative.\"\"\"\n        if v &lt; 0:\n            raise ValueError(\"Monetary amount must be non-negative\")\n        return v\n\n    @field_validator(\"currency\")\n    @classmethod\n    def validate_currency_format(cls, v: Any) -&gt; Any:\n        \"\"\"Ensure currency is 3 uppercase letters.\"\"\"\n        if v and not (len(v) == 3 and v.isupper()):\n            raise ValueError(\"Currency must be 3 uppercase letters (ISO 4217)\")\n        return v\n\n    def __str__(self) -&gt; str:\n        return f\"{self.value} {self.currency or ''}\".strip()\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#person-entity","title":"Person Entity","text":"<pre><code>class Person(BaseModel):\n    \"\"\"Person entity (unique by name and date of birth).\"\"\"\n    model_config = ConfigDict(\n        graph_id_fields=[\"first_name\", \"last_name\", \"date_of_birth\"]\n    )\n\n    first_name: Optional[str] = Field(\n        None,\n        description=\"Person's given name(s)\",\n        examples=[\"Jean\", \"Maria\", \"John\"]\n    )\n\n    last_name: Optional[str] = Field(\n        None,\n        description=\"Person's family name (surname)\",\n        examples=[\"Dupont\", \"Garcia\", \"Smith\"]\n    )\n\n    date_of_birth: Optional[date] = Field(\n        None,\n        description=\"Date of birth in YYYY-MM-DD format\",\n        examples=[\"1985-03-12\", \"1990-06-20\"]\n    )\n\n    email: Optional[str] = Field(\n        None,\n        description=\"Contact email address\",\n        examples=[\"jean.dupont@email.com\", \"maria.garcia@company.com\"]\n    )\n\n    phone: Optional[str] = Field(\n        None,\n        description=\"Contact phone number\",\n        examples=[\"+33 1 23 45 67 89\", \"+1 555-123-4567\"]\n    )\n\n    # Edge to Address\n    addresses: List[Address] = edge(\n        label=\"LIVES_AT\",\n        default_factory=list,\n        description=\"Residential addresses\"\n    )\n\n    @field_validator(\"email\", mode=\"before\")\n    @classmethod\n    def normalize_email(cls, v: Any) -&gt; Any:\n        \"\"\"Convert email to lowercase and strip whitespace.\"\"\"\n        if v:\n            return v.lower().strip()\n        return v\n\n    def __str__(self) -&gt; str:\n        parts = [self.first_name, self.last_name]\n        return \" \".join(p for p in parts if p) or \"Unknown\"\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#organization-entity","title":"Organization Entity","text":"<pre><code>class Organization(BaseModel):\n    \"\"\"Organization entity (unique by name).\"\"\"\n    model_config = ConfigDict(graph_id_fields=[\"name\"])\n\n    name: str = Field(\n        ...,\n        description=\"Legal name of the organization\",\n        examples=[\"Acme Corporation\", \"Tech Solutions Ltd\", \"Global Industries\"]\n    )\n\n    tax_id: Optional[str] = Field(\n        None,\n        description=\"Tax ID, VAT number, or registration number\",\n        examples=[\"123456789\", \"FR12345678901\", \"GB123456789\"]\n    )\n\n    email: Optional[str] = Field(\n        None,\n        description=\"Contact email address\",\n        examples=[\"contact@acme.com\", \"info@techsolutions.com\"]\n    )\n\n    phone: Optional[str] = Field(\n        None,\n        description=\"Contact phone number\",\n        examples=[\"+33 1 23 45 67 89\", \"+1 555-987-6543\"]\n    )\n\n    website: Optional[str] = Field(\n        None,\n        description=\"Official website URL\",\n        examples=[\"www.acme.com\", \"techsolutions.com\"]\n    )\n\n    # Edge to Address\n    addresses: List[Address] = edge(\n        label=\"LOCATED_AT\",\n        default_factory=list,\n        description=\"Physical addresses (headquarters, branches, etc.)\"\n    )\n\n    @field_validator(\"email\", mode=\"before\")\n    @classmethod\n    def normalize_email(cls, v: Any) -&gt; Any:\n        \"\"\"Convert email to lowercase and strip whitespace.\"\"\"\n        if v:\n            return v.lower().strip()\n        return v\n\n    def __str__(self) -&gt; str:\n        return self.name\n</code></pre>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#checklist-for-creating-new-templates","title":"Checklist for Creating New Templates","text":"<p>When creating a new template, verify:</p> <ul> <li>[ ] Imports: All necessary imports included</li> <li>[ ] Edge Helper: <code>edge()</code> function defined correctly</li> <li>[ ] File Organization: Components \u2192 Entities \u2192 Domain Models \u2192 Root</li> <li>[ ] Entity Configuration: All entities have <code>graph_id_fields</code></li> <li>[ ] Component Configuration: All components have <code>is_entity=False</code></li> <li>[ ] Field Descriptions: Clear, detailed, LLM-friendly</li> <li>[ ] Examples: 2-5 realistic examples per field</li> <li>[ ] Validators: Data quality checks where needed</li> <li>[ ] Edge Labels: Descriptive, ALL_CAPS_WITH_UNDERSCORES</li> <li>[ ] List Edges: All use <code>default_factory=list</code></li> <li>[ ] String Methods: <code>__str__</code> defined for entities</li> <li>[ ] Docstrings: All models have clear docstrings</li> <li>[ ] Type Hints: Proper use of Optional, List, Union</li> <li>[ ] Consistency: Patterns match across similar fields</li> </ul>"},{"location":"guides/create_pydantic_templates_for_kg_extraction/#testing-your-template","title":"Testing Your Template","text":"<p>Create a simple test to verify your template works:</p> <pre><code># test_my_template.py\nfrom my_template import RootDocument, Entity, Component\nfrom datetime import date\n\n# Create test instance\ntest_doc = RootDocument(\n    document_id=\"TEST-001\",\n    issued_by=Entity(\n        name=\"Test Organization\",\n        addresses=[\n            Component(\n                street_address=\"123 Test St\",\n                city=\"Paris\",\n                postal_code=\"75001\",\n                country=\"France\"\n            )\n        ]\n    )\n)\n\n# Verify it works\nprint(test_doc)\nprint(test_doc.model_dump_json(indent=2))\n</code></pre> <p>This guide ensures consistency across all Pydantic templates regardless of domain, enabling reliable LLM extraction and seamless knowledge graph conversion.</p>"},{"location":"guides/setup_with_gpu_support/","title":"Installing torch with GPU Support","text":""},{"location":"guides/setup_with_gpu_support/#important-notice-package-conflict-with-uv","title":"Important Notice: Package Conflict with uv","text":"<p>Currently, there is a package conflict preventing the use of PyTorch with GPU support via <code>uv</code> together with <code>docling[vlm]</code>. Installing GPU-enabled torch through <code>uv</code> sync alongside <code>docling[vlm]</code> causes dependency resolution failures.</p>"},{"location":"guides/setup_with_gpu_support/#manual-workaround-for-gpu-support","title":"Manual Workaround for GPU Support","text":"<p>If you need GPU support, follow these manual steps:</p> <ol> <li>Create and activate a virtual environment:</li> </ol> <pre><code>python -m venv venv\n\n# On Windows PowerShell\n.\\venv\\Scripts\\Activate\n\n# On Linux/macOS\nsource venv/bin/activate\n</code></pre> <ol> <li>Install your project in editable mode:</li> </ol> <p>Navigate to the root directory of your project (where <code>pyproject.toml</code> is), then run the command that matches your use case:</p> Option Command Description Minimal <code>pip install -e .</code> Includes core VLM features (Docling), no LLM inference Full <code>pip install -e .[all]</code> Includes all features, VLM, and all local/remote LLM providers Local LLM <code>pip install -e .[local]</code> Adds support for vLLM and Ollama (requires GPU for vLLM) Remote API <code>pip install -e .[remote]</code> Adds support for Mistral, OpenAI, and Google Gemini APIs <ol> <li>Uninstall CPU-only PyTorch packages:</li> </ol> <pre><code>pip uninstall torch torchvision torchaudio -y\n</code></pre> <ol> <li>Check your CUDA version:</li> </ol> <p>Use NVIDIA\u2019s system management tool:</p> <pre><code>nvidia-smi\n</code></pre> <p>Look for something like:</p> <pre><code>CUDA Version: 13.0\n</code></pre> <ol> <li>Install GPU-enabled PyTorch packages:</li> </ol> <p>Visit the official PyTorch installation page for commands matching your CUDA version.</p> <p>Example for CUDA 13.0:</p> <pre><code>pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu130\n</code></pre>"},{"location":"guides/setup_with_gpu_support/#verify-installation","title":"Verify Installation","text":"<p>Run this command to verify that PyTorch is installed correctly with CUDA support:</p> <pre><code>python -c \"import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')\"\n</code></pre> <p>Example output:</p> <pre><code>PyTorch version: 2.9.0+cu130\nCUDA available: True\n</code></pre>"},{"location":"guides/setup_with_gpu_support/#important-cli-usage-change","title":"Important CLI Usage Change","text":"<p>Moving forward, do not use <code>uv run docling-graph &lt;command&gt;</code>. Instead, directly call the docling-graph CLI with:</p> <pre><code>docling-graph &lt;command&gt; [OPTIONS]\n</code></pre> <p>This ensures commands run correctly without the <code>uv</code> wrapper in GPU-enabled environments.</p>"},{"location":"guides/setup_with_gpu_support/#cli-usage-examples-direct-calls","title":"CLI Usage Examples (Direct Calls)","text":""},{"location":"guides/setup_with_gpu_support/#1-initialize-configuration","title":"1. Initialize Configuration","text":"<pre><code>docling-graph init\n</code></pre>"},{"location":"guides/setup_with_gpu_support/#2-run-conversion","title":"2. Run Conversion","text":"<pre><code>docling-graph convert &lt;SOURCE_FILE_PATH&gt; --template \"&lt;TEMPLATE_PATH&gt;\" [OPTIONS]\n</code></pre>"},{"location":"guides/setup_with_gpu_support/#3-inspect-output","title":"3. Inspect Output","text":"<pre><code>docling-graph inspect &lt;CONVERT_OUTPUT_PATH&gt;\n</code></pre>"},{"location":"guides/watsonx_integration/","title":"IBM WatsonX Integration Guide","text":"<p>This guide explains how to use IBM WatsonX foundation models with docling-graph for document extraction and knowledge graph generation.</p>"},{"location":"guides/watsonx_integration/#overview","title":"Overview","text":"<p>IBM WatsonX provides access to powerful foundation models including: - IBM Granite models: Optimized for enterprise use cases - Meta Llama models: High-performance open models - Mistral models: Efficient multilingual models</p>"},{"location":"guides/watsonx_integration/#prerequisites","title":"Prerequisites","text":""},{"location":"guides/watsonx_integration/#1-ibm-cloud-account","title":"1. IBM Cloud Account","text":"<p>You need an IBM Cloud account with access to WatsonX.ai: 1. Sign up at IBM Cloud 2. Create a WatsonX.ai instance 3. Create a project in WatsonX.ai</p>"},{"location":"guides/watsonx_integration/#2-api-credentials","title":"2. API Credentials","text":"<p>Obtain your credentials: - API Key: From IBM Cloud IAM (Identity and Access Management) - Project ID: From your WatsonX.ai project settings - URL: Your WatsonX.ai endpoint (optional, defaults to US South region)</p>"},{"location":"guides/watsonx_integration/#3-installation","title":"3. Installation","text":"<p>Install docling-graph with WatsonX support:</p> <pre><code># Install WatsonX support only\nuv sync --extra watsonx\n\n# Or install all remote providers\nuv sync --extra remote\n\n# Or install everything\nuv sync --extra all\n</code></pre>"},{"location":"guides/watsonx_integration/#configuration","title":"Configuration","text":""},{"location":"guides/watsonx_integration/#environment-variables","title":"Environment Variables","text":"<p>Set your WatsonX credentials as environment variables:</p> <pre><code># Required\nexport WATSONX_API_KEY=\"your-ibm-cloud-api-key\"\nexport WATSONX_PROJECT_ID=\"your-watsonx-project-id\"\n\n# Optional (defaults to US South region)\nexport WATSONX_URL=\"https://us-south.ml.cloud.ibm.com\"\n</code></pre> <p>Regional Endpoints: - US South: <code>https://us-south.ml.cloud.ibm.com</code> - EU Germany: <code>https://eu-de.ml.cloud.ibm.com</code> - Japan Tokyo: <code>https://jp-tok.ml.cloud.ibm.com</code></p>"},{"location":"guides/watsonx_integration/#env-file","title":".env File","text":"<p>Alternatively, create a <code>.env</code> file in your project root:</p> <pre><code>WATSONX_API_KEY=your-ibm-cloud-api-key\nWATSONX_PROJECT_ID=your-watsonx-project-id\nWATSONX_URL=https://us-south.ml.cloud.ibm.com\n</code></pre>"},{"location":"guides/watsonx_integration/#usage","title":"Usage","text":""},{"location":"guides/watsonx_integration/#python-api","title":"Python API","text":"<pre><code>from docling_graph import run_pipeline, PipelineConfig\nfrom examples.templates.rheology_research import Research\n\n# Configure pipeline with WatsonX\nconfig = PipelineConfig(\n    source=\"path/to/document.pdf\",\n    template=Research,\n    backend=\"llm\",\n    inference=\"remote\",\n    processing_mode=\"many-to-one\",\n    provider_override=\"watsonx\",\n    model_override=\"ibm-granite/granite-4.0-h-small\",\n    output_dir=\"outputs/watsonx_extraction\"\n)\n\n# Run extraction\nrun_pipeline(config)\n</code></pre>"},{"location":"guides/watsonx_integration/#cli","title":"CLI","text":"<pre><code>uv run docling-graph convert \"path/to/document.pdf\" \\\n    --template \"examples.templates.battery_research.Research\" \\\n    --backend llm \\\n    --inference remote \\\n    --provider watsonx \\\n    --model \"ibm-granite/granite-4.0-h-small\" \\\n    --output-dir \"outputs/watsonx_extraction\"\n</code></pre>"},{"location":"guides/watsonx_integration/#available-models","title":"Available Models","text":""},{"location":"guides/watsonx_integration/#ibm-granite-models","title":"IBM Granite Models","text":"<p>Granite 4.0 H Small (Recommended) - Model ID: <code>ibm-granite/granite-4.0-h-small</code> - Context: 8K tokens - Best for: General-purpose extraction, fast inference</p> <p>Granite 3.0 8B Instruct - Model ID: <code>ibm-granite/granite-3.0-8b-instruct</code> - Context: 8K tokens - Best for: Instruction following, structured output</p> <p>Granite 3.0 2B Instruct - Model ID: <code>ibm-granite/granite-3.0-2b-instruct</code> - Context: 8K tokens - Best for: Fast inference, cost-effective</p>"},{"location":"guides/watsonx_integration/#meta-llama-models","title":"Meta Llama Models","text":"<p>Llama 3 70B Instruct - Model ID: <code>meta-llama/llama-3-70b-instruct</code> - Context: 8K tokens - Best for: Complex reasoning, high accuracy</p> <p>Llama 3 8B Instruct - Model ID: <code>meta-llama/llama-3-8b-instruct</code> - Context: 8K tokens - Best for: Balanced performance and speed</p>"},{"location":"guides/watsonx_integration/#mistral-models","title":"Mistral Models","text":"<p>Mixtral 8x7B Instruct - Model ID: <code>mistralai/mixtral-8x7b-instruct-v01</code> - Context: 32K tokens - Best for: Long documents, multilingual content</p>"},{"location":"guides/watsonx_integration/#examples","title":"Examples","text":""},{"location":"guides/watsonx_integration/#example-1-scientific-paper-extraction","title":"Example 1: Scientific Paper Extraction","text":"<pre><code>from docling_graph import run_pipeline, PipelineConfig\nfrom examples.templates.rheology_research import Research\n\nconfig = PipelineConfig(\n    source=\"docs/examples/data/research_paper/rheology.pdf\",\n    template=Research,\n    backend=\"llm\",\n    inference=\"remote\",\n    processing_mode=\"many-to-one\",\n    provider_override=\"watsonx\",\n    model_override=\"ibm-granite/granite-4.0-h-small\",\n    output_dir=\"outputs/battery_research\"\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"guides/watsonx_integration/#example-2-invoice-processing","title":"Example 2: Invoice Processing","text":"<pre><code>from docling_graph import run_pipeline, PipelineConfig\nfrom examples.templates.invoice import Invoice\n\nconfig = PipelineConfig(\n    source=\"examples/data/invoice/sample_invoice.jpg\",\n    template=Invoice,\n    backend=\"llm\",\n    inference=\"remote\",\n    processing_mode=\"one-to-one\",\n    provider_override=\"watsonx\",\n    model_override=\"ibm-granite/granite-3.0-8b-instruct\",\n    output_dir=\"outputs/invoice\"\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"guides/watsonx_integration/#example-3-multi-page-document","title":"Example 3: Multi-page Document","text":"<pre><code>from docling_graph import run_pipeline, PipelineConfig\nfrom examples.templates.insurance import InsurancePolicy\n\nconfig = PipelineConfig(\n    source=\"examples/data/insurance/insurance_terms.pdf\",\n    template=InsurancePolicy,\n    backend=\"llm\",\n    inference=\"remote\",\n    processing_mode=\"many-to-one\",\n    provider_override=\"watsonx\",\n    model_override=\"meta-llama/llama-3-70b-instruct\",\n    output_dir=\"outputs/insurance\"\n)\n\nrun_pipeline(config)\n</code></pre>"},{"location":"guides/watsonx_integration/#best-practices","title":"Best Practices","text":""},{"location":"guides/watsonx_integration/#model-selection","title":"Model Selection","text":"<ol> <li>Start with Granite 4.0 H Small: Good balance of speed and accuracy</li> <li>Use Llama 3 70B for complex documents: Better reasoning for technical content</li> <li>Use Mixtral for long documents: 32K context window handles large documents</li> </ol>"},{"location":"guides/watsonx_integration/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Batch Processing: Process multiple documents in sequence</li> <li>Processing Mode: Use <code>many-to-one</code> for multi-page documents</li> <li>Temperature: Keep at 0.1 for consistent extraction (default)</li> </ol>"},{"location":"guides/watsonx_integration/#error-handling","title":"Error Handling","text":"<pre><code>from docling_graph import run_pipeline, PipelineConfig\n\nconfig = PipelineConfig(\n    source=\"document.pdf\",\n    template=MyTemplate,\n    backend=\"llm\",\n    inference=\"remote\",\n    provider_override=\"watsonx\",\n    model_override=\"ibm-granite/granite-4.0-h-small\",\n    output_dir=\"outputs\"\n)\n\ntry:\n    run_pipeline(config)\n    print(\"Extraction successful!\")\nexcept ValueError as e:\n    print(f\"Configuration error: {e}\")\nexcept Exception as e:\n    print(f\"Extraction failed: {e}\")\n</code></pre>"},{"location":"guides/watsonx_integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/watsonx_integration/#authentication-errors","title":"Authentication Errors","text":"<p>Error: <code>WATSONX_API_KEY not set</code> - Solution: Set the environment variable or add to <code>.env</code> file</p> <p>Error: <code>WATSONX_PROJECT_ID not set</code> - Solution: Get your project ID from WatsonX.ai project settings</p>"},{"location":"guides/watsonx_integration/#api-errors","title":"API Errors","text":"<p>Error: <code>401 Unauthorized</code> - Solution: Verify your API key is valid and has WatsonX.ai access</p> <p>Error: <code>403 Forbidden</code> - Solution: Check your project ID and ensure you have access to the model</p> <p>Error: <code>Model not found</code> - Solution: Verify the model ID is correct and available in your region</p>"},{"location":"guides/watsonx_integration/#connection-errors","title":"Connection Errors","text":"<p>Error: <code>Connection timeout</code> - Solution: Check your network connection and WATSONX_URL setting</p> <p>Error: <code>Invalid endpoint</code> - Solution: Verify WATSONX_URL matches your region</p>"},{"location":"guides/watsonx_integration/#cost-considerations","title":"Cost Considerations","text":"<p>WatsonX.ai pricing is based on: - Token usage: Input and output tokens - Model size: Larger models cost more per token - Region: Prices may vary by region</p> <p>Tips to reduce costs: 1. Use smaller models (Granite 2B/3B) for simple extraction 2. Optimize prompts to reduce token usage 3. Use <code>many-to-one</code> mode to process documents in a single call 4. Monitor usage in IBM Cloud dashboard</p>"},{"location":"guides/watsonx_integration/#additional-resources","title":"Additional Resources","text":"<ul> <li>IBM WatsonX.ai Documentation</li> <li>WatsonX Python SDK</li> <li>IBM Cloud API Keys</li> </ul>"},{"location":"guides/watsonx_integration/#support","title":"Support","text":"<p>For issues specific to: - WatsonX.ai: Contact IBM Cloud Support - docling-graph: Open an issue on GitHub</p>"}]}